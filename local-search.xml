<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>0-Fast R-CNN</title>
    <link href="/2024/03/06/0-Fast-R-CNN/"/>
    <url>/2024/03/06/0-Fast-R-CNN/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>目标检测</category>
      
    </categories>
    
    
    <tags>
      
      <tag>目标检测</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0-Rich feature hierarchies for accurate object detection and semantic segmentation</title>
    <link href="/2024/03/05/0-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/"/>
    <url>/2024/03/05/0-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/</url>
    
    <content type="html"><![CDATA[<p><img src="/../images/0-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/image-20240305204216064.png" alt="R-CNN"></p><p>基于区域的卷积神经网络 (R-CNN)Rich feature hierarchies for accurate object detection and semantic segmentation </p>2013 年 11 月：R-CNN。给定输入图像，R-CNN 首先应用一种称为选择性搜索的机制来提取感兴趣区域(ROI)，其中每个 ROI 是一个可以表示图像中对象边界的矩形。根据场景的不同，ROI 可能多达两千个。之后，每个 ROI 都会通过神经网络产生输出特征。对于每个 ROI 的输出特征，使用支持向量机分类器的集合来确定 ROI 中包含什么类型的对象（如果有）。<blockquote><p>2013 年 11 月：R-CNN。给定输入图像，R-CNN 首先应用一种称为选择性搜索的机制来提取感兴趣区域(ROI)，其中每个 ROI 是一个可以表示图像中对象边界的矩形。根据场景的不同，ROI 可能多达两千个。之后，每个 ROI 都会通过神经网络产生输出特征。对于每个 ROI 的输出特征，使用支持向量机分类器的集合来确定 ROI 中包含什么类型的对象（如果有）。<br>2015 年 4 月：Fast R-CNN。原始 R-CNN 在多达 2000 个感兴趣区域中独立计算神经网络特征，而 Fast R-CNN 在整个图像上运行一次神经网络。网络的末端是一种称为 ROIPooling 的新颖方法，它从网络的输出张量中切出每个 ROI，对其进行整形并进行分类。与原始 R-CNN 一样，Fast R-CNN 使用选择性搜索来生成其区域建议。<br>2015 年 6 月：Faster R-CNN。Fast R-CNN 使用选择性搜索来生成 ROI，而 Faster R-CNN 将 ROI 生成集成到神经网络本身中。<br>2017 年 3 月：Mask R-CNN。之前版本的 R-CNN 专注于对象检测，而 Mask R-CNN 添加了实例分割。Mask R-CNN 还用一种名为 ROIAlign 的新方法取代了 ROIPooling，该方法可以表示像素的分数。<br>2019 年 6 月：Mesh R-CNN增加了从 2D 图像生成 3D 网格的功能。</p></blockquote><p>在本论文研究之前的方法：SIFT和HOG是块方向直方图，但是效果并不好。论文作者通过连接图像分类和目标检测，主要关注了1.使用深度网络定位物体和在小规模的标注数据集上进行大型网络模型的训练。2.与图像分类不同的是检测需要定位一个图像内的许多物体；使用滑动窗口探测器，但是由于网络层次更深，输入图片有非常大的感受野和步长，使得滑动窗口的方法充满挑战，通过操作”recognition using regions”范式，解决了CNN的定位问题。<br>由于结合了Region proposals和CNNs，所以起名<em><strong>R-CNN：Regions with CNN features。</strong></em><br>第二个挑战是标签数据太少，传统方法多是采用无监督与训练，再进行有监督调优，本论文使用了也就是第二个核心贡献是在辅助数据集（ILSVRC）上进行有监督预训练，再在小数据集上针对特定问题进行调优。这是在训练数据稀少的情况下一个非常有效的训练大型卷积神经网络的方法。</p><h3 id="步骤："><a href="#步骤：" class="headerlink" title="步骤："></a>步骤：</h3><h4 id="模型设计"><a href="#模型设计" class="headerlink" title="模型设计"></a>模型设计</h4><p>使用selective search进行<strong>Region proposals</strong>，使用AlexNet对每个region提取一个4096维的特征向量的特征提取，采用各向异性缩放变换。</p><p><img src="/../images/0-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/image-20240306113347937.png" alt="模型设计"></p><p>使用selective search(‘fast mode’模式)得到2000个左右的proposals，进行形状变换后传入CNN得到对应特征，然后将特征向量送入SVM中得到对应的类别。现在，我们得到图像中所有已经打分的region，应用greedy non-maximum suppression，去除重复框。</p><h4 id="训练阶段"><a href="#训练阶段" class="headerlink" title="训练阶段"></a>训练阶段</h4><p>在ImageNet数据集上对CNN进行预训练。</p><p>为了将预训练的CNN迁移到本任务(warped proposal windows分类)上，在warped region proposals上使用SGD进行fine-tune，不改变整体的网络结构，只将最后的1000-way分类层改为(N+1)-way，其中N为物体类别数，1为背景类别。SGD的初始学习率为预训练的1&#x2F;10，这样可以进行fine-tune，并且不破坏初始化。batch size为128，其中32个positive windows(所有类别，将IoU≥0.5的proposal视为该类别的positive，其他的为negative)，96个背景windows。 并且，在采样时倾向于采样positive windows，因为与背景相比它们是罕见的。<br>对于R-CNN的分类器，正例就是每一类ground -truth bounding box，IoU小于0.3的作为负类，其他的全部丢弃，不考虑。再训练SVM过程中，为了加速收敛使用了”Hard Negative Mining”策略(将每次loss很大的样本继续送到下一次训练中)。<br><img src="/../images/0-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/image-20240306113750317.png" alt="实验结果"></p><h4 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h4><p>作者在这里提出了一个可视化的想法，核心思想就是让神经元”speak for itself”：挑选出网络中的某个特定uint(当做检测器)，计算所有proposal在这个uint上的输出，按输出大小进行排序之后使用非极大值抑制(NNS)显示那些top-scoring区域。下图为关于CNN的池化层pool5的一个可视化效果。<br><img src="/../images/0-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/image-20240306113906119.png" alt="可视化"></p><h4 id="消融研究"><a href="#消融研究" class="headerlink" title="消融研究"></a>消融研究</h4><p>证明CNN的表征能力基本来自卷积层</p><h4 id="错误率分析"><a href="#错误率分析" class="headerlink" title="错误率分析"></a>错误率分析</h4><p>引入Bounding Box Regression可以减少定位问题，fine-tuning可以提高模型的鲁棒性等</p><p><img src="/../images/0-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/image-20240306114037739.png" alt="错误率分析"></p><p><img src="/../images/0-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/image-20240306114137183.png" alt="对物体特征的敏感性"></p><h3 id="结论："><a href="#结论：" class="headerlink" title="结论："></a>结论：</h3><p>第一是应用了自底向上的候选框训练的高容量的卷积神经网络进行定位和分割物体。另外一个是使用在标签数据匮乏的情况下训练大规模神经网络的一个方法。论文展示了在有监督的情况下使用丰富的数据集（图片分类）预训练一个网络作为辅助性的工作是很有效的，然后采用稀少数据（检测）去调优定位任务的网络。猜测“有监督的预训练+特定领域的调优”这一范式对于数据稀少的视觉问题是很有效的。<br>最后,论文能得到这些结果，将计算机视觉中经典的工具和深度学习(自底向上的区域候选框和卷积神经网络）组合是非常重要的。而不是违背科学探索的主线，这两个部分是自然而且必然的结合。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>目标检测</category>
      
    </categories>
    
    
    <tags>
      
      <tag>目标检测</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>1-基础部分-3-读论文</title>
    <link href="/2024/03/05/1-%E5%9F%BA%E7%A1%80%E9%83%A8%E5%88%86-3-%E8%AF%BB%E8%AE%BA%E6%96%87/"/>
    <url>/2024/03/05/1-%E5%9F%BA%E7%A1%80%E9%83%A8%E5%88%86-3-%E8%AF%BB%E8%AE%BA%E6%96%87/</url>
    
    <content type="html"><![CDATA[<h1 id="学位论文"><a href="#学位论文" class="headerlink" title="学位论文"></a>学位论文</h1><p>摘要<br>第一章绪论<br>第二章材料与方法<br>第三章结果与讨论(⑴)<br>第四章结果与讨论（⑵)<br>第五章结果与讨论(3)<br>结论<br>参考文献<br>攻读硕士学位期间取得创新性成果<br>学位论文原创性声明及使用授权<br>致谢<br>个人简历</p><p><img src="/../images/1-%E5%9F%BA%E7%A1%80%E9%83%A8%E5%88%86-3-%E8%AF%BB%E8%AE%BA%E6%96%87/1709190804696.png" alt="1709190804696"></p><h2 id="功能："><a href="#功能：" class="headerlink" title="功能："></a>功能：</h2><p>·题目→点睛，文章的极致浓缩;题目信息量≥50%文章的内容·</p><p>·摘要→浓缩的论文（重要程度超过论文主体)</p><p>·关键词→漂流瓶上的GPS(频道要一致)</p><p>·引言→背景（目的) -现状（那个等待修补的重要拼图）-创新性（我了解了拼图的基本信息)-方法（路线图)。</p><p>·材料与方法→我们有什么(有&#x3D;限制，思维、方法、技术)</p><p>·结果与讨论→我发现了什么，我的发现怎么样?</p><p>·结论→有得有失</p><p>·参考文献→一封感谢信（定位)</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>题目的扩写(检查)<br>内容的缩写（方法)<br>四要素全是基本要求<br>摘要是论文主体的浓缩<br>简洁，要有取舍、详略</p><h2 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h2><p>·通过对题目拆解得来<br>·题目:通过对关键字组合而来。<br>·准确，过于泛没有针对性。<br>·冷，过于生僻无人认<br>·精准＋宽泛<br>·技术在进步，关键词可能没那么重要了?</p><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>·引言决定论文的格局</p><p>·你以为是套路，其实是逻辑</p><p>·背景</p><p>·进展</p><p>·存在的问题</p><p>·我的解决方案</p><p>·引言是你思考的逻辑顺序</p><h2 id="结果与讨论"><a href="#结果与讨论" class="headerlink" title="结果与讨论"></a>结果与讨论</h2><p>·科技论文中，撰写结果与讨论的目的可总结为:用论据论证论点。<br>·拆分1段完整的结果与讨论，我们会发现，其中一定包括以下几点:1）指出图表;2)结果描述;3)规律总结;4)对比优劣&#x2F;机理阐明;5）给出结论。<br>·以上5部分除了第5部分外，其他4个部分基本上是一定要有的。这5个部分其实也反应了作者在做研究时，对研究本身一个“由表及里”逐步了解的一个过程。</p><p><img src="/../images/1-%E5%9F%BA%E7%A1%80%E9%83%A8%E5%88%86-3-%E8%AF%BB%E8%AE%BA%E6%96%87/1709193287719.png" alt="1709193287719"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>·完美的结论应是超出论文本身的内容</p><p>·提炼的、升华的</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>·真正对本研究有用的文献(实事求是)<br>·重量级相当的文献<br>·注意格式<br>·40%的内容来自于参考文献</p><h2 id="方法来源"><a href="#方法来源" class="headerlink" title="方法来源"></a>方法来源</h2><p><strong>三遍阅读法方法</strong>是ACM和IEEE Fellow 剑桥大学计算机教授Srinivasan Keshav的论文阅读技巧</p><h3 id="第一遍：该不该读？"><a href="#第一遍：该不该读？" class="headerlink" title="第一遍：该不该读？"></a>第一遍：该不该读？</h3><p>1.阅读标题、摘要和简介<br>2.忽略内容，读一读文章中的每个小标题<br>3.如果数学内容，先大致浏览，确定其理论基础<br>4.读结论<br>5.浏览参考文献，如果有读过的，勾选出来<br>第一遍阅读后应得出结论：</p><p>文章分类<br>文章背景<br>结论的正确性<br>所做出的主要贡献<br>结构清晰度</p><h3 id="第二遍：抓住要点，暂略细节"><a href="#第二遍：抓住要点，暂略细节" class="headerlink" title="第二遍：抓住要点，暂略细节"></a>第二遍：抓住要点，暂略细节</h3><p>时间：About 1 hour<br>1.过程中，仔细查看论文的图表，关注细节<br>2.标记论文中涉及的、并未读过的参考文献，之后做进一步阅读<br>第二遍阅读后应做到：</p><p>掌握内容，总结主旨</p><h3 id="第三遍：重构论文，注重细节"><a href="#第三遍：重构论文，注重细节" class="headerlink" title="第三遍：重构论文，注重细节"></a>第三遍：重构论文，注重细节</h3><p>跟随作者的思路，在脑海里重现论文内容<br>细节！细节！细节！<br>第三遍阅读后应做到：</p><p>看出论文的创新点<br>找到隐含假设<br>捕捉试验和技术分析中的潜在问题<br>引文缺失</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>0-Deep Residual Learning for Image Recognition</title>
    <link href="/2024/03/05/0-Deep-Residual-Learning-for-Image-Recognition/"/>
    <url>/2024/03/05/0-Deep-Residual-Learning-for-Image-Recognition/</url>
    
    <content type="html"><![CDATA[<p><img src="/../images/0-Deep-Residual-Learning-for-Image-Recognition/image-20240305100734044.png" alt="文章发表于2015年"></p><p>​残差神经网络（也称为残差网络或<strong>ResNet</strong>）是一种深度学习模型，其中权重层参考层输入学习残差函数。</p><p>​残差学习框架通过引入残差学习的概念，使得训练比以往更深的网络变得更加容易。这种框架允许网络学习残差映射，即学习残差函数而不是直接学习底层特征映射。通过这种方式，网络可以更轻松地学习残差，从而减轻了训练深度网络时出现的梯度消失或梯度爆炸等问题。</p><h3 id="步骤："><a href="#步骤：" class="headerlink" title="步骤："></a>步骤：</h3><p>​在ImageNet 2015比赛之前，2012 年 ImageNet 开发的AlexNet模型是一个八层卷积神经网络。牛津大学视觉几何小组 (VGGNet) 于 2014 年开发的神经网络通过堆叠 3×3 卷积层达到了 19 层的深度，然而，堆叠更多层会导致训练精度急剧下降，这被称为“退化”问题。</p><p><img src="/../images/0-Deep-Residual-Learning-for-Image-Recognition/image-20240305110540544.png" alt="梯度下降"></p><p>如上图所示，将20层神经网络加深到56层之后，模型的training error和test error反而更高了。</p><p>论文提出了一个解决方案，就是使用深度残差网络：</p><p><img src="/../images/0-Deep-Residual-Learning-for-Image-Recognition/image-20240305161546348.png" alt="深度残差网络"></p><p>​从深层网络出发，深层网路&#x3D;浅层网络+附加层，如果浅层网络已经做的非常好了，附加层只会进行一些微小的改动，得到的结果就是网络随着深度的增加，准确率会上升，而不是degredation描述的下降。</p><p><img src="/../images/0-Deep-Residual-Learning-for-Image-Recognition/image-20240305162027063.png" alt="残差网络Residual Network"></p><p>​从图中我们可以看到ResNet中的快捷连接有实线和虚线，实线表示输入输出维度相同，虚线表示维度不同。对于 ResNet，当输入维度小于输出维度时，有3 种类型的快捷连接方式：</p><ul><li>(A) Shortcut 执行恒等映射，使用额外的零填充来增加维度。因此，没有额外的参数。</li><li>(B) 投影快捷方式仅用于增加维度，其他快捷方式是恒等映射。需要额外的参数。</li><li>(C) 所有捷径都是投影。额外的参数比（B）的要多。</li></ul><p>实验表明方式C的精度最高，但作者建议使用方式B，因为C的计算量和参数量都有所增加。</p><p><img src="/../images/0-Deep-Residual-Learning-for-Image-Recognition/image-20240305163658461.png" alt="Table 3. Error rates"></p><h3 id="实验方法："><a href="#实验方法：" class="headerlink" title="实验方法："></a>实验方法：</h3><ul><li>将图像扩充到[256,480]之间，再resize为224 × 224</li><li>使用颜色增强</li><li>使用BN</li><li>将学习率通过乘0.1减小(这个方法现在已经不太用了，因为不知道具体在什么时候乘这个0.1，有的时候可能乘早了，在晚一点乘效果会更好，图中断崖式下降的地方就是学习率乘了0.1的地方)</li><li>没有使用dropout操作(dropout对卷积层的正则化作用很小：卷积层的参数必FC少很多，本身不需要正则化；同时，特征图编码的是空间的关系，他们之间是高度相关的，这也导致了dropout的失效)</li><li>在测试中使用了10-crop(10-crop是指在test的时候，从原始图片及翻转后的图片中，从四个corner和一个center各crop一个(224,224)的图片，一次是5张，镜像之后再操作一次就是10张。然后对这10张图片进行分类，对10次预测结果做average)</li><li>使用了{224，256，384，480，640}这5种不同的分辨率</li></ul><p><img src="/../images/0-Deep-Residual-Learning-for-Image-Recognition/image-20240305164121060.png" alt="实验结果"></p><p><img src="/../images/0-Deep-Residual-Learning-for-Image-Recognition/image-20240305164201058-17096281334771.png" alt="与普通网络的对比结果"></p><p>当使用普通网络时，由于退化问题，18 层的结果优于 34 层；使用 ResNet 时，34 层优于 18 层，通过快捷连接解决了梯度消失问题。(比较 18 层普通网络和 18 层 ResNet，没有太大区别。这是因为浅层网络不会出现梯度消失问题。)</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>目标检测</category>
      
    </categories>
    
    
    <tags>
      
      <tag>目标检测</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>markdown中的数学公式</title>
    <link href="/2023/09/12/markdown%E4%B8%AD%E7%9A%84%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/"/>
    <url>/2023/09/12/markdown%E4%B8%AD%E7%9A%84%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/</url>
    
    <content type="html"><![CDATA[<p>设置 math: true</p><h3 id="符号大全"><a href="#符号大全" class="headerlink" title="符号大全"></a>符号大全</h3><table><thead><tr><th align="left">写法</th><th align="center">符号</th><th align="left">备注</th></tr></thead><tbody><tr><td align="left">\sin(x)</td><td align="center">$$\sin(x)$$</td><td align="left">正弦函数</td></tr><tr><td align="left">\log(x)</td><td align="center">$$\log(x)$$</td><td align="left">对数函数</td></tr><tr><td align="left">\sum_{i&#x3D;0}^n</td><td align="center">$$\sum_{i&#x3D;0}^n$$</td><td align="left">累加和</td></tr><tr><td align="left">\prod_{i&#x3D;0}^n</td><td align="center">$$\prod_{i&#x3D;0}^n$$</td><td align="left">累积乘</td></tr><tr><td align="left">\displaystyle</td><td align="center">$$\displaystyle$$</td><td align="left">块显示</td></tr><tr><td align="left">\ldots</td><td align="center">$$\ldots$$</td><td align="left">底部省略号</td></tr><tr><td align="left">\cdots</td><td align="center">$$\cdots$$</td><td align="left">中部省略号</td></tr><tr><td align="left">\int_a^b</td><td align="center">$$\int_a^b$$</td><td align="left">积分符号</td></tr><tr><td align="left">\lim</td><td align="center">$$\lim$$</td><td align="left">极限函数</td></tr><tr><td align="left">\to</td><td align="center">$$\to$$</td><td align="left">箭头</td></tr><tr><td align="left">\vec{a}</td><td align="center">$$\vec{a}$$</td><td align="left">矢量a</td></tr><tr><td align="left">90^\circ</td><td align="center">$$90^\circ$$</td><td align="left">度数的圆圈</td></tr><tr><td align="left">\uparrow</td><td align="center">$$\uparrow$$</td><td align="left">上箭头</td></tr><tr><td align="left">\Uparrow</td><td align="center">$$\Uparrow$$</td><td align="left">双上箭头</td></tr><tr><td align="left">\partial y</td><td align="center">$$\partial y$$</td><td align="left">导数&#x2F;偏导</td></tr><tr><td align="left">\infty</td><td align="center">$$\infty$$</td><td align="left">无穷</td></tr><tr><td align="left">\Pi</td><td align="center">$$\Pi$$</td><td align="left">累乘</td></tr><tr><td align="left">\sqrt{x}</td><td align="center">$$\sqrt{x}$$</td><td align="left">求平方根</td></tr><tr><td align="left">\overline{a+b}</td><td align="center">$$\overline{a+b}$$</td><td align="left">上划线</td></tr><tr><td align="left">\underline{a+b}</td><td align="center">$$\underline{a+b}$$</td><td align="left">下划线</td></tr><tr><td align="left">\overbrace{a+b}</td><td align="center">$$\overbrace{a+b}$$</td><td align="left">上括号</td></tr><tr><td align="left">\underbrace{a+b}</td><td align="center">$$\underbrace{a+b}$$</td><td align="left">下括号</td></tr><tr><td align="left">\pm{a}{b}</td><td align="center">$$\pm{a}{b}$$</td><td align="left">正负号</td></tr><tr><td align="left">\mp{a}{b}</td><td align="center">$$\mp{a}{b}$$</td><td align="left">负正号</td></tr><tr><td align="left">\times</td><td align="center">$$\times$$</td><td align="left">乘法</td></tr><tr><td align="left">\cdot</td><td align="center">$$\cdot$$</td><td align="left">点乘</td></tr><tr><td align="left">\ast</td><td align="center">$$\ast$$</td><td align="left">星乘</td></tr><tr><td align="left">\div</td><td align="center">$$\div$$</td><td align="left">除法</td></tr><tr><td align="left">\frac{1}{5}</td><td align="center">$$\frac{1}{5}$$</td><td align="left">分数</td></tr><tr><td align="left">\drac{1}{5}</td><td align="center">$$已废弃$$</td><td align="left">分数，字体更大</td></tr><tr><td align="left">\leq</td><td align="center">$$\leq$$</td><td align="left">小于等于</td></tr><tr><td align="left">\not</td><td align="center">$$\not$$</td><td align="left">非</td></tr><tr><td align="left">\geq</td><td align="center">$$\geq$$</td><td align="left">大于等于</td></tr><tr><td align="left">\neq</td><td align="center">$$\neq$$</td><td align="left">不等于</td></tr><tr><td align="left">\nleq</td><td align="center">$$\nleq$$</td><td align="left">不小于等于</td></tr><tr><td align="left">\ngeq</td><td align="center">$$\ngeq$$</td><td align="left">不大于等于</td></tr><tr><td align="left">\sim</td><td align="center">$$\sim$$</td><td align="left">相关符号</td></tr><tr><td align="left">\approx</td><td align="center">$$\approx$$</td><td align="left">约等于</td></tr><tr><td align="left">\equiv</td><td align="center">$$\equiv$$</td><td align="left">常等于&#x2F;横等于</td></tr><tr><td align="left">\bigodot</td><td align="center">$$\bigodot$$</td><td align="left">加运算符</td></tr><tr><td align="left">\bigotimes</td><td align="center">$$\bigotimes$$</td><td align="left">乘运算符</td></tr></tbody></table><h3 id="集合符号"><a href="#集合符号" class="headerlink" title="集合符号"></a>集合符号</h3><table><thead><tr><th>写法</th><th align="center">符号</th><th>备注</th></tr></thead><tbody><tr><td>\in</td><td align="center">$$\in$$</td><td>属于</td></tr><tr><td>\notin</td><td align="center">$$\notin$$</td><td>不属于</td></tr><tr><td>\subset</td><td align="center">$$\subset$$</td><td>真子集</td></tr><tr><td>\not \subset</td><td align="center">$$\not \subset$$</td><td>非子集</td></tr><tr><td>\subseteq</td><td align="center">$$\subseteq$$</td><td>子集</td></tr><tr><td>\supset</td><td align="center">$$\supset$$</td><td>超集</td></tr><tr><td>\supseteq</td><td align="center">$$\supseteq$$</td><td>超集</td></tr><tr><td>\cup</td><td align="center">$$\cup$$</td><td>并集</td></tr><tr><td>\cap</td><td align="center">$$\cap$$</td><td>交集</td></tr><tr><td>\mathbb{R}</td><td align="center">$$\mathbb{R}$$</td><td>实数集</td></tr><tr><td>\emptyset</td><td align="center">$$\emptyset$$</td><td>空集</td></tr></tbody></table><h3 id="希腊符号"><a href="#希腊符号" class="headerlink" title="希腊符号"></a>希腊符号</h3><table><thead><tr><th>写法</th><th align="center">符号</th></tr></thead><tbody><tr><td>\alpha</td><td align="center">α</td></tr><tr><td>\beta</td><td align="center">β</td></tr><tr><td>\gamma</td><td align="center">γ</td></tr><tr><td>\Gamma</td><td align="center">Γ</td></tr><tr><td>\theta</td><td align="center">θ</td></tr><tr><td>\Theta</td><td align="center">Θ</td></tr><tr><td>\delta</td><td align="center">δ</td></tr><tr><td>\Delta</td><td align="center">Δ</td></tr><tr><td>\triangledown</td><td align="center">▽</td></tr><tr><td>\epsilon</td><td align="center">ϵ</td></tr><tr><td>\zeta</td><td align="center">ζ</td></tr><tr><td>\eta</td><td align="center">η</td></tr><tr><td>\kappa</td><td align="center">κ</td></tr><tr><td>\lambda</td><td align="center">λ</td></tr><tr><td>\mu</td><td align="center">μ</td></tr><tr><td>\nu</td><td align="center">ν</td></tr><tr><td>\xi</td><td align="center">ξ</td></tr><tr><td>\pi</td><td align="center">π</td></tr><tr><td>\sigma</td><td align="center">σ</td></tr><tr><td>\tau</td><td align="center">τ</td></tr><tr><td>\upsilon</td><td align="center">υ</td></tr><tr><td>\phi</td><td align="center">ϕ</td></tr><tr><td>\omega</td><td align="center">ω</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>markdown</category>
      
    </categories>
    
    
    <tags>
      
      <tag>markdown</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2-工具与软件-5-TensorFlow</title>
    <link href="/2023/09/08/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/"/>
    <url>/2023/09/08/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/</url>
    
    <content type="html"><![CDATA[<h2 id="环境的安装"><a href="#环境的安装" class="headerlink" title="环境的安装"></a>环境的安装</h2><p>首先去conda官网下载  <a href="https://repo.anaconda.com/">conda</a></p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230912155352950.png" alt="选择合适的版本"></p><p>linux系统先使用bash安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo bash xxx.sh<br></code></pre></td></tr></table></figure><p>安装后在pycharm配置conda环境，然后新建AI项目，选择conda，然后在所选择的解释器中安装tensorflow</p><p>选择pycharm自动安装（会自动安装其他依赖，十分方便）</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230912155649291.png" alt="tensorflow安装"></p><p>所需安装：</p><ul><li>conda</li><li>tensorflow</li></ul><p>如果你是N卡，可继续在项目终端中输入</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">conda install cudatoolkit<br></code></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">conda install cudnn<br></code></pre></td></tr></table></figure><p>安装GUP加速</p><h2 id="1-1-人工智能三学派"><a href="#1-1-人工智能三学派" class="headerlink" title="1.1 人工智能三学派"></a>1.1 人工智能三学派</h2><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240305095606298.png" alt="三学派"></p><p>行为主义：机器人的摔倒预测</p><p>符号主义：用公式描述的人工智能，让PC具有了理性思维</p><p>连接主义：仿造人的感性思维</p><h2 id="1-2-神经网络的设计过程"><a href="#1-2-神经网络的设计过程" class="headerlink" title="1.2 神经网络的设计过程"></a>1.2 神经网络的设计过程</h2><p>用神经网络实现鸢尾花的分类：<strong>梯度下降</strong></p><p>目的：找到一组参数w和b，使得损失函数最小。</p><p>梯度：函数对各参数<strong>求偏导</strong>后的向量。 <u>梯度下降的方向是函数减小的方向</u></p><p>梯度下降法：沿损失函数梯度下降的方向，寻找损失函数的最小值，得到最优参数的方法</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240305095654806.png" alt="学习率"></p><p>学习率（lr）：设置过小，收敛缓慢；设置过大，无法收敛（找不到最小值）</p><p>反向传播：从后向前，逐层求损失函数对每层神经元参数的偏导数，迭代更新所有参数。</p><p>损失函数:<br>$$<br>loss &#x3D; （w + 1 )^2<br>$$</p><p>$$<br>\frac{\part loss}{\part w} &#x3D; 2w +2<br>$$</p><p>代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><br>w = tf.Variable(tf.constant(<span class="hljs-number">5</span>, dtype=tf.float32))<br>lr = <span class="hljs-number">0.2</span>  <span class="hljs-comment"># 学习率</span><br>epoch = <span class="hljs-number">40</span>   <span class="hljs-comment"># 循环迭代数</span><br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epoch):  <span class="hljs-comment"># for epoch 定义顶层循环，表示对数据集循环epoch次，此例数据集数据仅有1个w,初始化时候constant赋值为5，循环40次迭代。</span><br>    <span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:  <span class="hljs-comment"># with结构到grads框起了梯度的计算过程。</span><br>        loss = tf.square(w + <span class="hljs-number">1</span>)<br>    grads = tape.gradient(loss, w)  <span class="hljs-comment"># .gradient函数告知谁对谁求导</span><br><br>    w.assign_sub(lr * grads)  <span class="hljs-comment"># .assign_sub 对变量做自减 即：w -= lr*grads 即 w = w - lr*grads</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;After %s epoch,w is %f,loss is %f&quot;</span> % (epoch, w.numpy(), loss))<br><br><span class="hljs-comment"># lr初始值：0.2   请自改学习率  0.001  0.999 看收敛过程</span><br><span class="hljs-comment"># 最终目的：找到 loss 最小 即 w = -1 的最优参数w</span><br><br></code></pre></td></tr></table></figure><p>Output:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python">After <span class="hljs-number">0</span> epoch,w <span class="hljs-keyword">is</span> <span class="hljs-number">2.600000</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">36.000000</span><br>After <span class="hljs-number">1</span> epoch,w <span class="hljs-keyword">is</span> <span class="hljs-number">1.160000</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">12.959999</span><br>After <span class="hljs-number">2</span> epoch,w <span class="hljs-keyword">is</span> <span class="hljs-number">0.296000</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">4.665599</span><br>After <span class="hljs-number">3</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.222400</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">1.679616</span><br>After <span class="hljs-number">4</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.533440</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.604662</span><br>After <span class="hljs-number">5</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.720064</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.217678</span><br>After <span class="hljs-number">6</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.832038</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.078364</span><br>After <span class="hljs-number">7</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.899223</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.028211</span><br>After <span class="hljs-number">8</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.939534</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.010156</span><br>After <span class="hljs-number">9</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.963720</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.003656</span><br>After <span class="hljs-number">10</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.978232</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.001316</span><br>After <span class="hljs-number">11</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.986939</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000474</span><br>After <span class="hljs-number">12</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.992164</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000171</span><br>After <span class="hljs-number">13</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.995298</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000061</span><br>After <span class="hljs-number">14</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.997179</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000022</span><br>After <span class="hljs-number">15</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.998307</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000008</span><br>After <span class="hljs-number">16</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.998984</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000003</span><br>After <span class="hljs-number">17</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.999391</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000001</span><br>After <span class="hljs-number">18</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.999634</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">19</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.999781</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">20</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.999868</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">21</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.999921</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">22</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.999953</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">23</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.999972</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">24</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.999983</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">25</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.999990</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">26</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.999994</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">27</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.999996</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">28</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.999998</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">29</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.999999</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">30</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.999999</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">31</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">1.000000</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">32</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">1.000000</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">33</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">1.000000</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">34</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">1.000000</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">35</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">1.000000</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">36</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">1.000000</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">37</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">1.000000</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">38</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">1.000000</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">39</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">1.000000</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br></code></pre></td></tr></table></figure><h2 id="1-3-张量生成"><a href="#1-3-张量生成" class="headerlink" title="1.3 张量生成"></a>1.3 张量生成</h2><p>张量（Tensor：多维数组 &#x2F;列表  ）        阶 ：张量的维数</p><table><thead><tr><th>维数</th><th>阶</th><th>名</th><th>例</th></tr></thead><tbody><tr><td>0-D</td><td>0</td><td>标量 scalar</td><td>s&#x3D;1</td></tr><tr><td>1-D</td><td>1</td><td>向量 vector</td><td>v&#x3D;[1,2,3]</td></tr><tr><td>2-D</td><td>2</td><td>矩阵 matrix</td><td>m&#x3D;[[1,2],[3,4],[5,6]]</td></tr><tr><td>n-D</td><td>n</td><td>张量 tensor</td><td>t&#x3D;[[[[……]]]] (n个)</td></tr></tbody></table><p>数据类型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">·tf.<span class="hljs-built_in">int</span>   tf.<span class="hljs-built_in">float</span> ...<br>tf.<span class="hljs-built_in">int</span> <span class="hljs-number">32</span>  , tf.<span class="hljs-built_in">float</span> <span class="hljs-number">32</span>  , tf.<span class="hljs-built_in">float</span> <span class="hljs-number">64</span><br>·tf.<span class="hljs-built_in">bool</span><br>tf.constant([true, false])<br>·tf.string<br>tf.constant(<span class="hljs-string">&quot;Hello world!&quot;</span>)<br></code></pre></td></tr></table></figure><p>创建Tensor</p><p><code>tf.constant(张量内容，dtype=数据类型(可选))</code></p><p>创建全为0的张量 <code>tf.zeros(维度)</code>  </p><p>​ 纬度:一维直接写个数；二维[行，列]；多维[n,m,j,k,…..]</p><p>创建全为1的张量 <code>tf.ones(纬度)</code></p><p>创建全为指定值的张量 <code>tf.fill(维度，指定值)</code></p><p>正态分部的随机数，默认值为0,标准差为1</p><p><code>tf.random.normal(纬度，mean=均值，stddev=标准差)</code></p><p>生成截断式正态分布的随机数</p><p><code>tf.random.truncated_normal(纬度，mean=均值，stddev=标准差)</code></p><p>在正态分布中如果随机生成的数据的取值在（$\mu\pm2\sigma$)</p><p>生成均匀分布的随机数</p><p><code>tf.random.uniform(纬度，minval=最小值，maxval=最大值)</code></p><h2 id="1-4-TF2常用函数"><a href="#1-4-TF2常用函数" class="headerlink" title="1.4 TF2常用函数"></a>1.4 TF2常用函数</h2><p>强制tensor转换为该数据类型<br><code>tf.cast (张量名，dtype=数据类型)</code><br>计算张量维度上元素的最小值<br><code>tf.reduce_min (张量名)</code><br>计算张量维度上元素的最大值<br><code>tf.reduce_max (张量名)</code></p><p>理解axis<br>在一个二维张量或数组中，可以通过调整 axis 等于0或1 控制执行维度。<br> axis&#x3D;0代表跨行（经度，down)，而axis&#x3D;1代表跨列（纬度，across)<br> 如果不指定axis，则所有元素参与计算。</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913093248414.png" alt="理解axis"></p><p>计算张量沿着指定维度的平均值<br><code>tf.reduce_mean (张量名，axis=操作轴)</code>  (不指定axis，则对所有元素进行操作)<br>计算张量沿着指定维度的和<br><code>tf.reduce_sum (张量名，axis=操作轴)</code></p><p><code>tf.Variable () </code>将变量标记为“可训练”，被标记的变量会在反向传播<br>中记录梯度信息。神经网络训练中，常用该函数标记待训练参数。<br><code>tf.Variable(初始值)</code><br><code>w = tf.Variable(tf.random.normal([2, 2], mean=0, stddev=1))</code></p><p>TensorFlow中的数学运算<br>对应元素的四则运算：<code>tf.add</code>，<code>tf.subtract</code>，<code>tf.multiply</code>，<code>tf.divide</code>    </p><p> 只有纬度相同的张量才能做四则运算。<br>平方、次方与开方：<code> tf.square</code>，<code>tf.pow</code>，<code>tf.sqrt</code><br>矩阵乘：<code>tf.matmul</code></p><p>切分传入张量的第一维度，生成输入特征&#x2F;标签对，构建数据集<br><code>data = tf.data.Dataset.from_tensor_slices((输入特征, 标签))</code><br>（Numpy和Tensor格式都可用该语句读入数据）</p><p><code>tf.GradientTape</code><br>with结构记录计算过程，gradient求出张量的梯度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> tf.GradientTape( ) <span class="hljs-keyword">as</span> tape:<br>若干个计算过程<br>grad=tape.gradient(函数，对谁求导)<br></code></pre></td></tr></table></figure><p>enumerate是python的内建函数，它可遍历每个元素(如列表、元组<br>或字符串)，组合为：索引 元素，常在for循环中使用。<br><code>enumerate(列表名)</code></p><p>独热编码：在分类问题中，常用独热码做标签，标记类别：1表示是，0表示非。 <code>tf.one_hot (待转换数据, depth=几分类)</code></p><p>当n分类的n个输出 （y0 ，y1, …… yn-1）通过softmax( ) 函数，<br>便符合概率分布了。也就是说，将多个权重占比划分归为1。<br>$$<br>\forall x \ \ P(X &#x3D; x) \in [0,1] 且 \sum_{x}P(X &#x3D; x) &#x3D; 1<br>$$</p><p>assign_sub 赋值操作，更新参数的值并返回。<br>调用assign_sub前，先用 tf.Variable 定义变量 w 为可训练（可自更新）。<br>w.assign_sub (w要自减的内容)</p><p>返回张量沿指定维度最大值的索引<br>tf.argmax (张量名,axis&#x3D;操作轴)     numpy中也有类似函数</p><h2 id="1-5-鸢尾花数据集的读入"><a href="#1-5-鸢尾花数据集的读入" class="headerlink" title="1.5 鸢尾花数据集的读入"></a>1.5 鸢尾花数据集的读入</h2><p>Setosa Iris（狗尾草鸢尾），Versicolour Iris（杂色鸢尾），Virginica Iris（弗吉尼亚鸢尾）</p><p>鸢尾花数据来源：sklearn框架</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913105626307.png" alt="3种鸢尾花"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets <br><span class="hljs-keyword">from</span> pandas <span class="hljs-keyword">import</span> DataFrame<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br>x_data = datasets.load_iris().data  <span class="hljs-comment"># .data返回iris数据集所有输入特征</span><br>y_data = datasets.load_iris().target  <span class="hljs-comment"># .target返回iris数据集所有标签</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;x_data from datasets: \n&quot;</span>, x_data)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;y_data from datasets: \n&quot;</span>, y_data)<br><br>x_data = DataFrame(x_data, columns=[<span class="hljs-string">&#x27;花萼长度&#x27;</span>, <span class="hljs-string">&#x27;花萼宽度&#x27;</span>, <span class="hljs-string">&#x27;花瓣长度&#x27;</span>, <span class="hljs-string">&#x27;花瓣宽度&#x27;</span>])  <span class="hljs-comment"># 为表格增加行索引（左侧）和列标签（上方）</span><br>pd.set_option(<span class="hljs-string">&#x27;display.unicode.east_asian_width&#x27;</span>, <span class="hljs-literal">True</span>)  <span class="hljs-comment"># 设置列名对齐</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;x_data add index: \n&quot;</span>, x_data)<br><br>x_data[<span class="hljs-string">&#x27;类别&#x27;</span>] = y_data  <span class="hljs-comment"># 新加一列，列标签为‘类别’，数据为y_data</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;x_data add a column: \n&quot;</span>, x_data)<br><br><span class="hljs-comment"># 类型维度不确定时，建议用print函数打印出来确认效果</span><br></code></pre></td></tr></table></figure><h2 id="1-8-神经网络实现鸢尾花的分类"><a href="#1-8-神经网络实现鸢尾花的分类" class="headerlink" title="1.8 神经网络实现鸢尾花的分类"></a>1.8 神经网络实现鸢尾花的分类</h2><p>1.准备数据</p><p>​数据集读入</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 从sklearn包datasets 读入数据集：</span><br><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> datasets<br>x_data = datasets.load_iris().data <span class="hljs-comment"># 返回iris数据集所有输入特征</span><br>y_data = datasets.load_iris().target <span class="hljs-comment"># 返回iris数据集所有标签</span><br></code></pre></td></tr></table></figure><p>​数据集乱序</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">np.random.seed(<span class="hljs-number">116</span>) <span class="hljs-comment"># 使用相同的seed，使输入特征/标签一一对应</span><br>np.random.shuffle(x_data)<br>np.random.seed(<span class="hljs-number">116</span>)<br>np.random.shuffle(y_data)<br>tf.random.set_seed(<span class="hljs-number">116</span>)<br></code></pre></td></tr></table></figure><p>​分成用不相见的训练集和测试集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">x_train = x_data[:-<span class="hljs-number">30</span>]<br>y_train = y_data[:-<span class="hljs-number">30</span>]<br>x_test = x_data[-<span class="hljs-number">30</span>:]<br>y_test = y_data[-<span class="hljs-number">30</span>:]<br></code></pre></td></tr></table></figure><p>​配成【输入特征，标签】对，每次喂入一个batch</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(<span class="hljs-number">32</span>)<br>test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(<span class="hljs-number">32</span>)<br></code></pre></td></tr></table></figure><p>2.搭建网络</p><p>​定义神经网络中的所有可训练参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">w1 = tf.Variable(tf.random.truncated_normal([ <span class="hljs-number">4</span>, <span class="hljs-number">3</span> ], stddev=<span class="hljs-number">0.1</span>, seed=<span class="hljs-number">1</span>)) <span class="hljs-comment"># 四种特征，三个结果</span><br>b1 = tf.Variable(tf.random.truncated_normal([ <span class="hljs-number">3</span> ], stddev=<span class="hljs-number">0.1</span>, seed=<span class="hljs-number">1</span>))<br></code></pre></td></tr></table></figure><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913140637316.png" alt="输入层与输出层"></p><p>3.参数优化</p><p>​嵌套循环迭代，with结构更新参数，显示当前loss</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epoch): <span class="hljs-comment">#数据集级别迭代</span><br><span class="hljs-keyword">for</span> step, (x_train, y_train) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_db): <span class="hljs-comment">#batch级别迭代</span><br><span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape: <span class="hljs-comment"># 记录梯度信息</span><br>前向传播过程计算y<br>计算总loss<br>grads = tape.gradient(loss, [ w1, b1 ])<br>w1.assign_sub(lr * grads[<span class="hljs-number">0</span>]) <span class="hljs-comment">#参数自更新</span><br>b1.assign_sub(lr * grads[<span class="hljs-number">1</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Epoch &#123;&#125;, loss: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(epoch, loss_all/<span class="hljs-number">4</span>))<br></code></pre></td></tr></table></figure><p>4.测试效果</p><p>​计算当前参数前向传播后的准确率，显示当前acc</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> x_test, y_test <span class="hljs-keyword">in</span> test_db:<br>y = tf.matmul(h, w) + b <span class="hljs-comment"># y为预测结果</span><br>y = tf.nn.softmax(y)<br><span class="hljs-comment"># y符合概率分布</span><br>pred = tf.argmax(y, axis=<span class="hljs-number">1</span>) <span class="hljs-comment"># 返回y中最大值的索引，即预测的分类</span><br>pred = tf.cast(pred, dtype=y_test.dtype) <span class="hljs-comment">#调整数据类型与标签一致</span><br>correct = tf.cast(tf.equal(pred, y_test), dtype=tf.int32)<br>correct = tf.reduce_sum (correct) <span class="hljs-comment"># 将每个batch的correct数加起来</span><br>total_correct += <span class="hljs-built_in">int</span> (correct) <span class="hljs-comment"># 将所有batch中的correct数加起来</span><br>total_number += x_test.shape [<span class="hljs-number">0</span>]<br>acc = total_correct / total_number<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;test_acc:&quot;</span>, acc)<br></code></pre></td></tr></table></figure><p>5.acc &#x2F; loss 可视化（查看效果）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.title(<span class="hljs-string">&#x27;Acc Curve&#x27;</span>) <span class="hljs-comment"># 图片标题</span><br>plt.xlabel(<span class="hljs-string">&#x27;Epoch&#x27;</span>) <span class="hljs-comment"># x轴名称</span><br>plt.ylabel(<span class="hljs-string">&#x27;Acc&#x27;</span>) <span class="hljs-comment"># y轴名称</span><br>plt.plot(test_acc, label=<span class="hljs-string">&quot;$Accuracy$&quot;</span>) <span class="hljs-comment"># 逐点画出test_acc值并连线</span><br>plt.legend()<br>plt.show()<br></code></pre></td></tr></table></figure><h2 id="2-1-预备知识"><a href="#2-1-预备知识" class="headerlink" title="2.1 预备知识"></a>2.1 预备知识</h2><p>函数：</p><p><code>tf.where()</code>  条件语句真返回A，条件语句假返回B<br><code>tf.where(条件语句，真返回A，假返回B)</code></p><p><code>np.random.RandomState.rand()</code>返回一个[0,1)之间的随机数<br><code>np.random.RandomState.rand(维度) #维度为空，返回标量</code></p><p><code>np.vstack()</code>将两个数组按垂直方向叠加<br><code>np.vstack(数组1，数组2)</code></p><p><code>np.mgrid[ ] </code>返回间隔数值点，可同时返回多组， [起始值 结束值)<br><code>np.mgrid[ 起始值 : 结束值 : 步长 ，起始值 : 结束值 : 步长 , … ]</code></p><p><code> x.ravel( )</code> 将x变为一维数组，“把. 前变量拉直”<br><code>np.c\_[ ] </code>使返回的间隔数值点配对<br><code>np.c\_[ 数组1，数组2， … ]</code></p><h2 id="2-2-复杂度学习率"><a href="#2-2-复杂度学习率" class="headerlink" title="2.2 复杂度学习率"></a>2.2 复杂度学习率</h2><p>NN复杂度：多用NN层数和NN参数的个数表示</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913145516199.png" alt="复杂度"></p><p>空间复杂度：<br>    层数 &#x3D; 隐藏层的层数 + 1个输出层<br>    图为2层NN<br>                总参数 &#x3D; 总w + 总b<br>                图中 3x4+4 + 4x2+2 &#x3D; 26</p><p>时间复杂度：<br>    乘加运算次数<br>    左图 3x4 +  4x2 &#x3D; 20</p><p>学习率：</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913150028635.png" alt="学习率"></p><p>指数衰减学习率：<br>可以先用较大的学习率，快速得到较优解，然后逐步减小学习率，使<br>模型在训练后期稳定。<br><code>指数衰减学习率 = 初始学习率 * 学习率衰减率（ 当前轮数 / 多少轮衰减一次 ）</code></p><h2 id="2-3-激活函数"><a href="#2-3-激活函数" class="headerlink" title="2.3 激活函数"></a>2.3 激活函数</h2><p>优秀的激活函数：<br>• 非线性： 激活函数非线性时，多层神经网络可逼近所有函数<br>• 可微性： 优化器大多用梯度下降更新参数<br>• 单调性： 当激活函数是单调的，能保证单层网络的损失函数是凸函数<br>• 近似恒等性： f(x)≈x当参数初始化为随机小值时，神经网络更稳定</p><p>激活函数输出值的范围：<br>• 激活函数输出为有限值时，基于梯度的优化方法更稳定<br>• 激活函数输出为无限值时，建议调小学习率</p><p>Sigmoid函数：<br>$$<br>f(x) &#x3D; \frac{1}{1 + e ^ {-x}}<br>$$<br><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913151017142.png" alt="Sigmoid函数"></p><p>特点<br>（1）易造成梯度消失<br>（2）输出非0均值，收敛慢<br>（3）幂运算复杂，训练时间长<br>目前Sigmoid函数因计算复杂，已接近弃用。</p><p>Tanh函数：<br>$$<br>f(x) &#x3D; \frac{1-e^{-2x}}{1+e^{-2x}}<br>$$<br><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913151343657.png" alt="Tanh函数"></p><p>特点<br>（1）输出是0均值<br>（2）易造成梯度消失<br>（3）幂运算复杂，训练时间长</p><p>Relu函数：<br>$$<br>f(x) &#x3D; max(x , 0) &#x3D; \begin{cases}0 \quad x&lt;0 \\ x \quad x\geq0 \end{cases}<br>$$<br><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913152152906.png" alt="Relu函数"></p><p>优点：<br>（1） 解决了梯度消失问题 (在正区间)<br>（2） 只需判断输入是否大于0，计算速度快<br>（3） 收敛速度远快于sigmoid和tanh</p><p>缺点：<br>（1） 输出非0均值，收敛慢<br>（2） Dead RelU问题：某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。（神经元死亡）</p><p>Leaky Relu函数：<br>$$<br>f(x) &#x3D; max (ax,x)<br>$$<br><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913152543744.png" alt="Leaky Relu函数"></p><p>理论上来讲，Leaky Relu有Relu的所有优点，外加不会有Dead Relu问题，但是在实际操作当中，并没有完全证明Leaky Relu总是好于Relu。</p><p>对于初学者的建议：<br>首选relu激活函数；<br>学习率设置较小值；<br>输入特征标准化，即让输入特征满足以0为均值，1为标准差的正态分布；<br>初始参数中心化，即让随机生成的参数满足以0为均值,$\sqrt{\frac{2}{当前层输入特征个数}}$为标准差的正态分布。</p><h2 id="2-4-损失函数"><a href="#2-4-损失函数" class="headerlink" title="2.4 损失函数"></a>2.4 损失函数</h2><p>预测值（y）与已知答案（_y）的差距</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913153356947.png" alt="主流的三种计算方法"></p><p>均方误差mse：<br>$$<br>MSE(y_,y)&#x3D;\frac{\sum_{i&#x3D;1}^n (y-y_)^2}{n}<br>$$<br><code>lost_mse = tf.reduce_mean(tf.square(y_-y))</code></p><p>自定义函数：</p><p>可在一定程度上优化实际问题中的预测误差。</p><p>交叉熵CE：</p><p>表明了两个概率分布之间的距离，交叉熵越大，表明两个概率分布越远<br>$$<br>H(y_,y) &#x3D; - \sum y_ \times ln\ y<br>$$<br>交叉熵越小，证明数据距离真实越准确。</p><p>softmax与交叉熵的结合：</p><p>在TensorFlow中提供了函数<code>tf.nn.softmax_cross_entropy_with_logits(y_，y)</code>输出先过softmax函数，再计算y与y_的交叉熵损失函数。</p><h2 id="2-5-缓解过拟合"><a href="#2-5-缓解过拟合" class="headerlink" title="2.5 缓解过拟合"></a>2.5 缓解过拟合</h2><p>欠拟合与过拟合</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913162344502.png" alt="欠拟合与过拟合"></p><table><thead><tr><th>欠拟合的解决方法：<br/>增加输入特征项<br/>增加网络参数<br/>减少正则化参数</th><th>过拟合的解决方法：<br/>数据清洗<br/>增大训练集<br/>采用正则化<br/>增大正则化参数</th></tr></thead></table><p>正则化缓解过拟合：</p><p>正则化在损失函数中引入模型复杂度指标，利用给W加权值，弱化了训练<br>数据的噪声（一般不正则化b）</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913162811491.png" alt="正则化"></p><p>正则化的选择<br>L1正则化大概率会使很多参数变为零，因此该方法可通过稀疏参数，即减少参数的数量，降低复杂度。<br>L2正则化会使参数很接近零但不为零，因此该方法可通过减小参数值的大小降低复杂度。</p><p>使用L2正则化缓解过拟合</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:  <span class="hljs-comment"># 记录梯度信息</span><br><br>           h1 = tf.matmul(x_train, w1) + b1  <span class="hljs-comment"># 记录神经网络乘加运算</span><br>           h1 = tf.nn.relu(h1)<br>           y = tf.matmul(h1, w2) + b2<br><br>           <span class="hljs-comment"># 采用均方误差损失函数mse = mean(sum(y-out)^2)</span><br>           loss_mse = tf.reduce_mean(tf.square(y_train - y))<br>           <span class="hljs-comment"># 添加l2正则化</span><br>           loss_regularization = []<br>           <span class="hljs-comment"># tf.nn.l2_loss(w)=sum(w ** 2) / 2</span><br>           loss_regularization.append(tf.nn.l2_loss(w1))<br>           loss_regularization.append(tf.nn.l2_loss(w2))<br>           <span class="hljs-comment"># 求和</span><br>           <span class="hljs-comment"># 例：x=tf.constant(([1,1,1],[1,1,1]))</span><br>           <span class="hljs-comment">#   tf.reduce_sum(x)</span><br>           <span class="hljs-comment"># &gt;&gt;&gt;6</span><br>           loss_regularization = tf.reduce_sum(loss_regularization)<br>           loss = loss_mse + <span class="hljs-number">0.03</span> * loss_regularization  <span class="hljs-comment"># REGULARIZER = 0.03</span><br><br>       <span class="hljs-comment"># 计算loss对各个参数的梯度</span><br>       variables = [w1, b1, w2, b2]<br>       grads = tape.gradient(loss, variables)<br></code></pre></td></tr></table></figure><p>下表可以看出，L2正则化函数可有效的缓解过饱和现象</p><table><thead><tr><th><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913170454005.png" alt="未填加L2正则化"></th><th><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913170523568.png" alt="加入了L2正则化"></th></tr></thead></table><h2 id="2-6-优化器"><a href="#2-6-优化器" class="headerlink" title="2.6 优化器"></a>2.6 优化器</h2><p>是引导神经网络更新参数的工具。</p><p>神经网络参数优化器：<br>待优化参数𝒘，损失函数loss，学习率lr，每次迭代一个batch，t表示当前batch迭代的总次数：</p><ol><li>计算t时刻损失函数关于当前参数的梯度 $g_t&#x3D;\triangledown loss &#x3D; \frac{\partial loss}{\partial (w_t)} $</li><li>计算t时刻一阶动量 $m_t$ 和二阶动量$V_t$</li><li>计算t时刻下降梯度：$\eta_t &#x3D;\frac{lr·m_t}{\sqrt{V_t}}$</li><li>计算t+1时刻参数：$w_{t+1} &#x3D; w_t - \eta_t &#x3D; w_t - \frac{lr·m_t}{\sqrt{V_t}}$</li></ol><p>一阶动量：与梯度相关的函数<br>二阶动量：与梯度平方相关的函数</p><p>SGD(无momentum)，常用的梯度下降算法：</p><p>$m_t &#x3D; g_t $ $V_t &#x3D; 1$</p><p>$\eta_t &#x3D; \frac{lr·m_t}{\sqrt{V_t}} $</p><p>$w_{t+1} &#x3D; w_t -\eta_t &#x3D; w_t - \frac{lr·m_t}{\sqrt{V_t}} \ &#x3D;w_t - lr·g_t $</p><p>$w_{t+1} &#x3D; w_t -lr \ast \frac{\partial loss}{\partial w_t}  \ 参数更新公式$</p><p>SGDM(含momentum的SGD)，在SGD的基础上增加了一阶动量:</p><p>$m_{t-1}$表示上一时刻的一阶动量。</p><p>$m_t &#x3D; \beta · m_{t-1} + (1-\beta )·g_t $      $V_t&#x3D;1$</p><p>$\eta_t&#x3D;  \frac{lr·m_t}{\sqrt{V_t}} &#x3D; lr· m_t &#x3D;lr·(\beta · m_{t-1}+(1-\beta)·g_t)$</p><p>$w_{t+1}&#x3D;w_t -\eta_t &#x3D; w_t - lr · (\beta · m_{t-1}+(1-\beta)·g_t)$</p><p>Adagrad，在SGD基础上增加二阶动量:</p><p>$m_t&#x3D;g_t$     $V_t&#x3D;\sum_{\tau&#x3D;1}^t g_\tau^2$</p><p>$\eta_t&#x3D;\frac{lr·m_t}{\sqrt{V_t}} &#x3D;\frac{lr·g_t}{\sqrt{\sum_{\tau&#x3D;1}^t g_\tau^2}} $</p><p>$w_{t+1}&#x3D;w_t-\eta_t&#x3D;w_t-\frac{lr·g_t}{\sqrt{\sum_{\tau&#x3D;1}^t g_\tau^2}}$</p><p>RMSProp，SGD基础上增加二阶动量:</p><p>$m_t&#x3D;g_t$      $V_t &#x3D; \beta · V_{t-1} + (1-\beta)·g_t^2 $</p><p>$\eta_t&#x3D;\frac{lr·m_t}{\sqrt{V_t}} &#x3D;\frac{lr·g_t}{\sqrt{ \beta · V_{t-1} + (1-\beta)·g_t^2}} $</p><p>$w_{t+1}&#x3D;w_t-\eta_t&#x3D;w_t-\frac{lr·g_t}{\sqrt{ \beta · V_{t-1} + (1-\beta)·g_t^2}}$</p><p>Adam, 同时结合SGDM一阶动量和RMSProp二阶动量:</p><p>$m_t&#x3D;\beta_1 ·m_{t-1}+(1-\beta_1)·g_t$</p><p>修正一阶动量的偏差：$\widehat{m_t}&#x3D;\frac{m_t}{1-\beta_1^t}$</p><p>$V_t&#x3D;\beta_2 · V_{step-1}+(1-\beta_2)·g_t^2$</p><p>修正二阶动量的偏差：$ \widehat{V_t}&#x3D; \frac{V_t} {1-\beta_2^t} $</p><p>$\eta_t&#x3D;\frac{lr·\widehat{m_t} }{\sqrt{\widehat{V_t} } } &#x3D; lr \cdot \frac{lr{\frac{m_t}{1-\beta_1^t} } }{\sqrt{ {\frac{V_t} {1-\beta_2^t} } } } $</p><p>$w_{t+1}&#x3D;w_t-\eta_t&#x3D;w_t-\frac{lr\cdot{\frac{m_t}{1-\beta_1^t} } }{\sqrt{ {\frac{V_t}{1-\beta_2^t} } } } $</p><h2 id="3-1-搭建网络八股Sequential"><a href="#3-1-搭建网络八股Sequential" class="headerlink" title="3.1 搭建网络八股Sequential"></a>3.1 搭建网络八股Sequential</h2><p>用Tensorflow API：<code>tf.keras</code>搭建网络八股<br>六步法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span>    <span class="hljs-comment">#引入相关模块</span><br>train, test  <span class="hljs-comment">#告知喂入网络的训练集和测试集    特征x_train和标签y_train </span><br>model = tf.keras.models.Sequential <span class="hljs-comment">#搭建网络解构</span><br>model.<span class="hljs-built_in">compile</span> <span class="hljs-comment"># 配置训练方法（选择优化器、损失函数、评测指标）</span><br>model.fit <span class="hljs-comment"># 训练过程，告知train、test，告知batch、迭代次数</span><br>model.summary <span class="hljs-comment"># 打印网络解构、参数统计</span><br></code></pre></td></tr></table></figure><p><code>model = tf.keras.models.Sequential ([ 网络结构 ]) #描述各层网络</code><br>Sequential是容器，给出从输入层到输出层的各层网络解构</p><p>拉直层： tf.keras.layers.Flatten( )</p><p>全连接层： <code>tf.keras.layers.Dense(神经元个数, activation= &quot;激活函数“ ,kernel_regularizer=哪种正则化)</code><br>activation（字符串给出）可选: relu、 softmax、 sigmoid 、 tanh<br>kernel_regularizer可选:tf.keras.regularizers.l1()tf.keras.regularizers.l2()</p><p>卷积层： tf.keras.layers.Conv2D(filters &#x3D; 卷积核个数, kernel_size &#x3D; 卷积核尺寸,<br>strides &#x3D; 卷积步长， padding &#x3D; “ valid” or “same”)</p><p>LSTM层： tf.keras.layers.LSTM()</p><p><code>model.compile(optimizer = 优化器,loss = 损失函数 metrics = [“准确率”] )</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Optimizer可选:</span><br>‘sgd’ <span class="hljs-keyword">or</span> tf.keras.optimizers.SGD (lr=学习率,momentum=动量参数)<br>‘adagrad’ <span class="hljs-keyword">or</span> tf.keras.optimizers.Adagrad (lr=学习率)<br>‘adadelta’ <span class="hljs-keyword">or</span> tf.keras.optimizers.Adadelta (lr=学习率)<br>‘adam’ <span class="hljs-keyword">or</span> tf.keras.optimizers.Adam (lr=学习率, beta_1=<span class="hljs-number">0.9</span>, beta_2=<span class="hljs-number">0.999</span>)<br><span class="hljs-comment">#　loss可选:</span><br>‘mse’ <span class="hljs-keyword">or</span> tf.keras.losses.MeanSquaredError()<br>‘sparse_categorical_crossentropy’ <span class="hljs-keyword">or</span> tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="hljs-literal">False</span>)<br><span class="hljs-comment"># Metrics可选:</span><br>‘accuracy’ ：y_和y都是数值，如y_=[<span class="hljs-number">1</span>] y=[<span class="hljs-number">1</span>]<br>‘categorical_accuracy’ ：y_和y都是独热码(概率分布)，如y_=[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>] y=[<span class="hljs-number">0.256</span>,<span class="hljs-number">0.695</span>,<span class="hljs-number">0.048</span>]<br>‘sparse_categorical_accuracy’ ：y_是数值，y是独热码(概率分布),如y_=[<span class="hljs-number">1</span>] y=[<span class="hljs-number">0.256</span>,<span class="hljs-number">0.695</span>,<span class="hljs-number">0.048</span>]<br></code></pre></td></tr></table></figure><p><code>model.fit (训练集的输入特征, 训练集的标签, batch_size= , epochs= , validation_data=(测试集的输入特征，测试集的标签), validation_split=从训练集划分多少比例给测试集， validation_freq = 多少次epoch测试一次)</code></p><p>使用sequential可以搭建出上层输出就是下层输入的下层网络机构，但无法写出一些带有跳连的非顺序网络结构，这时候可以选择用类Class搭建神经网络解构。</p><h2 id="3-2-搭建网络八股Class"><a href="#3-2-搭建网络八股Class" class="headerlink" title="3.2 搭建网络八股Class"></a>3.2 搭建网络八股Class</h2><p>在六步法中，将第三部的Model改为<code>class MyModel(Model) model=MyModel</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyModel</span>(<span class="hljs-title class_ inherited__">Model</span>): <span class="hljs-comment"># 继承了Tensorflow的model类</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br><span class="hljs-built_in">super</span>(MyModel, self).__init__()<br>定义网络结构块<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">call</span>(<span class="hljs-params">self, x</span>):<br>调用网络结构块，实现前向传播<br><span class="hljs-keyword">return</span> y<br>model = MyModel()<br><br><br><span class="hljs-comment">#__init__( )  定义所需网络结构块</span><br><span class="hljs-comment">#call( )  写出前向传播 实现钱前向传播</span><br><br></code></pre></td></tr></table></figure><h2 id="3-3-MNIST数据集"><a href="#3-3-MNIST数据集" class="headerlink" title="3.3 MNIST数据集"></a>3.3 MNIST数据集</h2><p>手写数字的数据集-上万张</p><p>导入MNIST数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"> mnist = tf.keras.datasets.mnist<br>(x_train, y_train) , (x_test, y_test) = mnist.load_data()<br></code></pre></td></tr></table></figure><p>为输入特征，输入神经网络时，将数据拉伸为一维数组：<br><code>tf.keras.layers.Flatten( )</code></p><h2 id="3-4-FASHION数据集"><a href="#3-4-FASHION数据集" class="headerlink" title="3.4 FASHION数据集"></a>3.4 FASHION数据集</h2><p>提供 6万张 28X28 像素点的衣裤等图片和标签，用于训练。<br>提供 1万张 28X28 像素点的衣裤等图片和标签，用于测试。</p><p>导入FASHION数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">fashion = tf.keras.datasets.fashion_mnist<br>(x_train, y_train),(x_test, y_test) = fashion.load_data()<br></code></pre></td></tr></table></figure><h2 id="4-1-搭建网络八股总览"><a href="#4-1-搭建网络八股总览" class="headerlink" title="4.1 搭建网络八股总览"></a>4.1 搭建网络八股总览</h2><p>① 自制数据集，解决本领域应用<br>② 数据增强，扩充数据集<br>③ 断点续训，存取模型<br>④ 参数提取，把参数存入文本<br>⑤ acc&#x2F;loss可视化，查看训练效果<br>⑥ 应用程序，给图识物</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230916150852673.png" alt="六步法八股总览"></p><h2 id="4-2-自制数据集"><a href="#4-2-自制数据集" class="headerlink" title="4.2 自制数据集"></a>4.2 自制数据集</h2><p>使用Py，目的是将文件夹内的图片读入，返回输入特征、标签。</p><p>标签文件txt</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> os<br><span class="hljs-comment">### import</span><br><br><br>train_path = <span class="hljs-string">&#x27;./mnist_image_label/mnist_train_jpg_60000/&#x27;</span><br>train_txt = <span class="hljs-string">&#x27;./mnist_image_label/mnist_train_jpg_60000.txt&#x27;</span><br>x_train_savepath = <span class="hljs-string">&#x27;./mnist_image_label/mnist_x_train.npy&#x27;</span><br>y_train_savepath = <span class="hljs-string">&#x27;./mnist_image_label/mnist_y_train.npy&#x27;</span><br><span class="hljs-comment">### 训练集</span><br><br><br>test_path = <span class="hljs-string">&#x27;./mnist_image_label/mnist_test_jpg_10000/&#x27;</span><br>test_txt = <span class="hljs-string">&#x27;./mnist_image_label/mnist_test_jpg_10000.txt&#x27;</span><br>x_test_savepath = <span class="hljs-string">&#x27;./mnist_image_label/mnist_x_test.npy&#x27;</span><br>y_test_savepath = <span class="hljs-string">&#x27;./mnist_image_label/mnist_y_test.npy&#x27;</span><br><br><span class="hljs-comment">### 测试集</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">generateds</span>(<span class="hljs-params">path, txt</span>):<br>    f = <span class="hljs-built_in">open</span>(txt, <span class="hljs-string">&#x27;r&#x27;</span>)  <span class="hljs-comment"># 以只读形式打开txt文件</span><br>    contents = f.readlines()  <span class="hljs-comment"># 读取文件中所有行</span><br>    f.close()  <span class="hljs-comment"># 关闭txt文件</span><br>    x, y_ = [], []  <span class="hljs-comment"># 建立空列表</span><br>    <span class="hljs-keyword">for</span> content <span class="hljs-keyword">in</span> contents:  <span class="hljs-comment"># 逐行取出</span><br>        value = content.split()  <span class="hljs-comment"># 以空格分开，图片路径为value[0] , 标签为value[1] , 存入列表</span><br>        img_path = path + value[<span class="hljs-number">0</span>]  <span class="hljs-comment"># 拼出图片路径和文件名</span><br>        img = Image.<span class="hljs-built_in">open</span>(img_path)  <span class="hljs-comment"># 读入图片</span><br>        img = np.array(img.convert(<span class="hljs-string">&#x27;L&#x27;</span>))  <span class="hljs-comment"># 图片变为8位宽灰度值的np.array格式</span><br>        img = img / <span class="hljs-number">255.</span>  <span class="hljs-comment"># 数据归一化 （实现预处理）</span><br>        x.append(img)  <span class="hljs-comment"># 归一化后的数据，贴到列表x</span><br>        y_.append(value[<span class="hljs-number">1</span>])  <span class="hljs-comment"># 标签贴到列表y_</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;loading : &#x27;</span> + content)  <span class="hljs-comment"># 打印状态提示</span><br><br>    x = np.array(x)  <span class="hljs-comment"># 变为np.array格式</span><br>    y_ = np.array(y_)  <span class="hljs-comment"># 变为np.array格式</span><br>    y_ = y_.astype(np.int64)  <span class="hljs-comment"># 变为64位整型</span><br>    <span class="hljs-keyword">return</span> x, y_  <span class="hljs-comment"># 返回输入特征x，返回标签y_</span><br><br><span class="hljs-comment">### generateds函数</span><br><br><br><span class="hljs-keyword">if</span> os.path.exists(x_train_savepath) <span class="hljs-keyword">and</span> os.path.exists(y_train_savepath) <span class="hljs-keyword">and</span> os.path.exists(<br>        x_test_savepath) <span class="hljs-keyword">and</span> os.path.exists(y_test_savepath):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;-------------Load Datasets-----------------&#x27;</span>)<br>    x_train_save = np.load(x_train_savepath)<br>    y_train = np.load(y_train_savepath)<br>    x_test_save = np.load(x_test_savepath)<br>    y_test = np.load(y_test_savepath)<br>    x_train = np.reshape(x_train_save, (<span class="hljs-built_in">len</span>(x_train_save), <span class="hljs-number">28</span>, <span class="hljs-number">28</span>))<br>    x_test = np.reshape(x_test_save, (<span class="hljs-built_in">len</span>(x_test_save), <span class="hljs-number">28</span>, <span class="hljs-number">28</span>))<br><span class="hljs-keyword">else</span>:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;-------------Generate Datasets-----------------&#x27;</span>)<br>    x_train, y_train = generateds(train_path, train_txt)<br>    x_test, y_test = generateds(test_path, test_txt)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;-------------Save Datasets-----------------&#x27;</span>)<br>    x_train_save = np.reshape(x_train, (<span class="hljs-built_in">len</span>(x_train), -<span class="hljs-number">1</span>))<br>    x_test_save = np.reshape(x_test, (<span class="hljs-built_in">len</span>(x_test), -<span class="hljs-number">1</span>))<br>    np.save(x_train_savepath, x_train_save)<br>    np.save(y_train_savepath, y_train)<br>    np.save(x_test_savepath, x_test_save)<br>    np.save(y_test_savepath, y_test)<br><br><span class="hljs-comment">### train test</span><br>    <br>    <br>    <br>model = tf.keras.models.Sequential([<br>    tf.keras.layers.Flatten(),<br>    tf.keras.layers.Dense(<span class="hljs-number">128</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>),<br>    tf.keras.layers.Dense(<span class="hljs-number">10</span>, activation=<span class="hljs-string">&#x27;softmax&#x27;</span>)<br>])<br><br><span class="hljs-comment">### models.sequential</span><br><br>model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;adam&#x27;</span>,<br>              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="hljs-literal">False</span>),<br>              metrics=[<span class="hljs-string">&#x27;sparse_categorical_accuracy&#x27;</span>])<br><br><span class="hljs-comment">### model.compile</span><br><br>model.fit(x_train, y_train, batch_size=<span class="hljs-number">32</span>, epochs=<span class="hljs-number">5</span>, validation_data=(x_test, y_test), validation_freq=<span class="hljs-number">1</span>)<br><br><span class="hljs-comment">### model.fit</span><br><br>model.summary()<br><br><span class="hljs-comment">### model.summary</span><br></code></pre></td></tr></table></figure><p>第一次运行生成了npy格式的数据集。</p><p>第二次会加载数据集，执行训练过程。</p><h2 id="4-3-数据增强"><a href="#4-3-数据增强" class="headerlink" title="4.3 数据增强"></a>4.3 数据增强</h2><p>增大数据量,扩充数据集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">image_gen_train =tf.keras.preprocessing.image.ImageDataGenerator(<br>rescale = 所有数据将乘以该数值<br>rotation_range = 随机旋转角度数范围<br>width_shift_range = 随机宽度偏移量<br>height_shift_range = 随机高度偏移量<br>水平翻转：horizontal_flip = 是否随机水平翻转<br>随机缩放：zoom_range = 随机缩放的范围 [<span class="hljs-number">1</span>-n，<span class="hljs-number">1</span>+n] )<br>image_gen_train.fit(x_train)<br></code></pre></td></tr></table></figure><h2 id="4-4-断点续训"><a href="#4-4-断点续训" class="headerlink" title="4.4 断点续训"></a>4.4 断点续训</h2><p>可以存取模型</p><p>读取模型：<br><code>load_weights(路径文件名）</code></p><p>可以先检查是否存在断点，如有则加载模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">cheakpoint_save_path= <span class="hljs-string">&quot;./cheakpoint/mnist.ckpt&quot;</span><br><span class="hljs-keyword">if</span> os.path.exists(checkpoint_save_path + <span class="hljs-string">&#x27;.index&#x27;</span>):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;--------load the model-------&#x27;</span>)<br>    model.load_weights(checkpoint_save_path)<br></code></pre></td></tr></table></figure><p><code>保存模型： tf.keras.callbacks.ModelCheckpoint(filepath=路径文件名,save_weights_only=True/False,save_best_only=True/False) history = model.fit（ callbacks=[cp_callback] ）</code></p><h2 id="4-5-参数提取"><a href="#4-5-参数提取" class="headerlink" title="4.5 参数提取"></a>4.5 参数提取</h2><p>把参数存入文本</p><p>提取可训练参数<br>model.trainable_variables 返回模型中可训练的参数<br>设置print输出格式<br>np.set_printoptions(threshold&#x3D;超过多少省略显示)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">np.set_printoptions(threshold=np.inf)<br><span class="hljs-comment"># np.inf表示无限大</span><br><br><br><span class="hljs-built_in">print</span>(model.trainable_variables)<br>file = <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;./weights.txt&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>)<br><span class="hljs-keyword">for</span> v <span class="hljs-keyword">in</span> model.trainable_variables:<br>file.write(<span class="hljs-built_in">str</span>(v.name) + <span class="hljs-string">&#x27;\n&#x27;</span>)<br>file.write(<span class="hljs-built_in">str</span>(v.shape) + <span class="hljs-string">&#x27;\n&#x27;</span>)<br>file.write(<span class="hljs-built_in">str</span>(v.numpy()) + <span class="hljs-string">&#x27;\n&#x27;</span>)<br>file.close()<br></code></pre></td></tr></table></figure><h2 id="4-6-acc-loss可视化"><a href="#4-6-acc-loss可视化" class="headerlink" title="4.6 acc&#x2F;loss可视化"></a>4.6 acc&#x2F;loss可视化</h2><p>acc曲线与loss曲线</p><p><code>history=model.fit(训练集数据, 训练集标签, batch_size=, epochs=,validation_split=用作测试数据的比例,validation_data=测试集,validation_freq=测试频率)</code></p><p>只是一段画图程序代码。</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230916164058968.png" alt="添加的画图代码"></p><h2 id="4-7-图片识别"><a href="#4-7-图片识别" class="headerlink" title="4.7 图片识别"></a>4.7 图片识别</h2><p>给图识物</p><p><code>predict（输入特征，batch_size=整数）</code>返回向前传播的计算结果</p><p>复现模型（前向传播）:</p><p><code>model = tf.keras.models.Sequential([ tf.keras.layers.Flatten(), tf.keras.layers.Dense(128, activation=&#39;relu&#39;), tf.keras.layers.Dense(10, activation=&#39;softmax’)])</code></p><p>加载参数:</p><p><code>model.load_weights(model_save_path</code></p><p>预测结果:</p><p><code>result = model.predict(x_predict)</code></p><h2 id="5-1-卷积的计算过程-Convolutional"><a href="#5-1-卷积的计算过程-Convolutional" class="headerlink" title="5.1 卷积的计算过程 Convolutional"></a>5.1 卷积的计算过程 Convolutional</h2><p>全连接 NN 特点：每个神经元与前后相邻层的每一个神经元都有连接关系。（可以实<br>现分类和预测）</p><p>全连接网络参数的个数为：$\sum(前层\times 后层 + 后层)$</p><p>卷积的概念：卷积可以认为是一种有效提取图像特征的方法。一般会用一个正方形的<br>卷积核，按指定步长，在输入特征图上滑动，遍历输入特征图中的每个像素点。每一个步长，<br>卷积核会与输入特征图出现重合区域，重合区域对应元素相乘、求和再加上偏置项得到输出<br>特征的一个像素点。</p><p>对于彩色图像（多通道）来说，卷积核通道数与输入特征一致，套接后在对应位置上进行乘加和操作，如果是彩色图片（RGB）利用三通道卷积核对三通道的彩色特征图做卷积计算。</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230916171743300.png" alt="输出特征尺寸计算"></p><h2 id="5-2-感受野"><a href="#5-2-感受野" class="headerlink" title="5.2 感受野"></a>5.2 感受野</h2><p>感受野（Receptive Field）：卷积神经网络各输出特征图中的每个像素点，在原始输入图片上映射区域的大小。</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230916173848124.png" alt="黄-绿为两次3*3 蓝色为一次5*5"></p><p>通常用两层3*3卷积核替换一层5*5卷积核</p><h2 id="5-3-全零填充-Padding"><a href="#5-3-全零填充-Padding" class="headerlink" title="5.3 全零填充 Padding"></a>5.3 全零填充 Padding</h2><p>将图的四周加上一圈零填充。</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230916174816575.png" alt="padding"></p><p>TF描述全零填充<br>用参数padding &#x3D; ‘SAME’ 或 padding &#x3D; ‘VALID’表示</p><p>SAME：5X5X1  –&gt;  5X5X1      VALID：5X5X1–&gt;3X3X1</p><p>可以让输出特征图和输出特征图的尺寸不变。</p><h2 id="5-4-TF描述卷积计算层"><a href="#5-4-TF描述卷积计算层" class="headerlink" title="5.4 TF描述卷积计算层"></a>5.4 TF描述卷积计算层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">tf.keras.layers.Conv2D (<br>filters = 卷积核个数,<br>kernel_size = 卷积核尺寸, <span class="hljs-comment">#正方形写核长整数，或（核高h，核宽w）</span><br>strides = 滑动步长, <span class="hljs-comment">#横纵向相同写步长整数，或(纵向步长h，横向步长w)，默认1</span><br>padding = “same” <span class="hljs-keyword">or</span> “valid”, <span class="hljs-comment">#使用全零填充是“same”，不使用是“valid”（默认）</span><br>activation = “ relu ” <span class="hljs-keyword">or</span> “ sigmoid ” <span class="hljs-keyword">or</span> “ tanh ” <span class="hljs-keyword">or</span> “ softmax”等 , <span class="hljs-comment">#如有BN此处不写</span><br>input_shape = (高, 宽 , 通道数) <span class="hljs-comment">#输入特征图维度，可省略</span><br>)<br></code></pre></td></tr></table></figure><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240228211041340.png" alt="TF描述卷积层"></p><p>例如可以使用关键字传递参数的方法。</p><h2 id="5-5-批标准化-BN"><a href="#5-5-批标准化-BN" class="headerlink" title="5.5 批标准化 BN"></a>5.5 批标准化 BN</h2><p>标准化：使数据符合0均值，1为标准差的分布。</p><p>批标准化：对一小批数据（batch），做标准化处理 。</p><p>批标准化后，第 k个卷积核的输出特征图（feature map）中第 i 个像素点</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240228211544435.png" alt="BN层位于卷积层之后，激活层之前。"></p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240228211600833.png" alt="批标准化"></p><h2 id="5-6-池化-Pooling"><a href="#5-6-池化-Pooling" class="headerlink" title="5.6 池化 Pooling"></a>5.6 池化 Pooling</h2><p>池化用于减少特征数据量。平均池化和最大池化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">tf.keras.layers.MaxPool2D(<br>pool_size=池化核尺寸，<span class="hljs-comment">#正方形写核长整数，或（核高h，核宽w）</span><br>strides=池化步长，<span class="hljs-comment">#步长整数， 或(纵向步长h，横向步长w)，默认为pool_size</span><br>padding=‘valid’<span class="hljs-keyword">or</span>‘same’ <span class="hljs-comment">#使用全零填充是“same”，不使用是“valid”（默认）</span><br>)<br>tf.keras.layers.AveragePooling2D(<br>pool_size=池化核尺寸，<span class="hljs-comment">#正方形写核长整数，或（核高h，核宽w）</span><br>strides=池化步长，<span class="hljs-comment">#步长整数， 或(纵向步长h，横向步长w)，默认为pool_size</span><br>padding=‘valid’<span class="hljs-keyword">or</span>‘same’ <span class="hljs-comment">#使用全零填充是“same”，不使用是“valid”（默认）</span><br>)<br></code></pre></td></tr></table></figure><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240228211847780.png" alt="池化"></p><h2 id="5-7-舍弃-Dropout"><a href="#5-7-舍弃-Dropout" class="headerlink" title="5.7 舍弃 Dropout"></a>5.7 舍弃 Dropout</h2><p>在神经网络训练时，将一部分神经元按照一定概率从神经网络中暂时舍弃。神经网络使用时，被舍弃的神经元恢复链接。</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240228212108454.png" alt="舍弃"></p><h2 id="5-8-卷积神经网络"><a href="#5-8-卷积神经网络" class="headerlink" title="5.8 卷积神经网络"></a>5.8 卷积神经网络</h2><p>卷积是什么？ 卷积就是特征提取器，就是CBAPD</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240228214733104.png" alt="CBAPD"></p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240228214753084.png" alt="CNN"></p><h2 id="5-9-Cifar10数据集-卷积神经网络搭建示例"><a href="#5-9-Cifar10数据集-卷积神经网络搭建示例" class="headerlink" title="5.9 Cifar10数据集 卷积神经网络搭建示例"></a>5.9 Cifar10数据集 卷积神经网络搭建示例</h2><p>提供 5万张 32*32 像素点的十分类彩色图片和标签，用于训练。<br>提供 1万张 32*32 像素点的十分类彩色图片和标签，用于测试。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#导入cifar10数据集：</span><br>cifar10 = tf.keras.datasets.cifar10<br>(x_train, y_train),(x_test, y_test) = cifar10.load_data()<br><br>plt.imshow(x_train[<span class="hljs-number">0</span>])<br><span class="hljs-comment">#绘制图片</span><br>plt.show()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;x_train[0]:\n&quot;</span> , x_train[<span class="hljs-number">0</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;y_train[0]:&quot;</span>, y_train[<span class="hljs-number">0</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;x_test.shape:&quot;</span>, x_test.shape)<br></code></pre></td></tr></table></figure><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240302132316265.png" alt="卷积神经网络搭建示例"></p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240302132135165.png" alt="搭建示例"></p><h2 id="5-10-LeNet-AlexNet-VGGNet-InceptionNet-ResNet"><a href="#5-10-LeNet-AlexNet-VGGNet-InceptionNet-ResNet" class="headerlink" title="5.10 LeNet AlexNet VGGNet InceptionNet ResNet"></a>5.10 LeNet AlexNet VGGNet InceptionNet ResNet</h2><p>LeNet由Yann LeCun于1998年提出，卷积网络开篇之作。</p><p>AlexNet网络诞生于2012年，当年ImageNet竞赛的冠军，Top5错误率为16.4%。</p><p>VGGNet诞生于2014年，当年ImageNet竞赛的亚军，Top5错误率减小到7.3%。</p><p>InceptionNet诞生于2014年，当年ImageNet竞赛冠军，Top5错误率为6.67%</p><p>ResNet诞生于2015年，当年ImageNet竞赛冠军，Top5错误率为3.57%</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240302153943645.png" alt="经典卷积网络"></p><h2 id="6-1-循环核"><a href="#6-1-循环核" class="headerlink" title="6.1 循环核"></a>6.1 循环核</h2><p>循环核：参数时间共享，循环层提取时间信息。</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240302180849506.png" alt="循环核"></p><h2 id="6-2-循环核按时间步展开"><a href="#6-2-循环核按时间步展开" class="headerlink" title="6.2 循环核按时间步展开"></a>6.2 循环核按时间步展开</h2><p>循环神经网络：借助循环核提取时间特征后，送入全连接网络。</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240302181058519.png" alt="循环神经网络"></p><p>循环计算层：向输出方向生长。</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240302181218194.png" alt="循环计算层"></p><h2 id="6-3-TF描述循环计算层"><a href="#6-3-TF描述循环计算层" class="headerlink" title="6.3 TF描述循环计算层"></a>6.3 TF描述循环计算层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">tf.keras.layers.SimpleRNN(记忆体个数，activation=‘激活函数’ ，<br>return_sequences=是否每个时刻输出ht到下一层)<br>activation=‘激活函数’ （不写，默认使用tanh）<br>return_sequences=<span class="hljs-literal">True</span> 各时间步输出ht<br>return_sequences=<span class="hljs-literal">False</span> 仅最后时间步输出ht（默认）<br>例：SimpleRNN(<span class="hljs-number">3</span>, return_sequences=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>入RNN时， x_train维度：<br>[送入样本数， 循环核时间展开步数， 每个时间步输入特征个数]</p><h2 id="6-4-循环计算过程-字母输入预测"><a href="#6-4-循环计算过程-字母输入预测" class="headerlink" title="6.4 循环计算过程-字母输入预测"></a>6.4 循环计算过程-字母输入预测</h2><p>字母预测：输入a预测出b，输入b预测出c，<br>输入c预测出d，输入d预测出e，输入e预测出a</p><p>用RNN实现输入一个字母，预测下一个字母<br>（One hot 编码）独热码</p><p>用RNN实现输入连续四个字母，预测下一个字母<br>（One hot 编码）</p><p>用RNN实现输入一个字母，预测下一个字母<br>（Embedding 编码）</p><p>用RNN实现输入连续四个字母，预测下一个字母<br>（Embedding 编码）</p><h2 id="6-5-股票预测"><a href="#6-5-股票预测" class="headerlink" title="6.5 股票预测"></a>6.5 股票预测</h2><p>用RNN实现股票预测</p><p>用LSTM实现股票预测</p><p>LSTM 由Hochreiter &amp; Schmidhuber 于1997年提出，通过门控单元改善了RNN长期依赖问题。</p><p>用GRU实现股票预测</p><p>GRU由Cho等人于2014年提出，优化LSTM结构。</p><p>更新于：2024 </p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2-工具与软件-4-PyTorch</title>
    <link href="/2023/09/08/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-4-PyTorch/"/>
    <url>/2023/09/08/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-4-PyTorch/</url>
    
    <content type="html"><![CDATA[<p>安装CUDA</p><p>UPDATEｔｉｍｅ；</p><p>９．２２　１８：００</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2-工具与软件-3-数据分析实战</title>
    <link href="/2023/09/08/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-3-%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%98/"/>
    <url>/2023/09/08/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-3-%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%98/</url>
    
    <content type="html"><![CDATA[<blockquote><p>使用Python进行数据分析，对其编程、库，以及⽤于数据分析的⼯具的相关学习与研究。</p></blockquote><h2 id="一-准备工作"><a href="#一-准备工作" class="headerlink" title="一.准备工作"></a>一.准备工作</h2><h2 id="1-重要的Python库"><a href="#1-重要的Python库" class="headerlink" title="1 重要的Python库"></a>1 重要的Python库</h2><h3 id="NumPy"><a href="#NumPy" class="headerlink" title="NumPy"></a>NumPy</h3><p>NumPy（Numerical Python的简称）是Python科学计算的基础包。<a href="https://cnwuyueyu.github.io/2023/09/08/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-1-Numpy/">详见</a>它提供了以下功能（不限于此）：</p><ul><li>快速⾼效的多维数组对象ndarray。</li><li>⽤于对数组执⾏元素级计算以及直接对数组执⾏数学运算的函数。</li><li>⽤于读写硬盘上基于数组的数据集的⼯具。</li><li>线性代数运算、傅⾥叶变换，以及随机数⽣成。-成熟的C API， ⽤于Python插件和原⽣CC++、Fortran代码访问NumPy的数据结构和计算⼯具。</li></ul><p>除了为Python提供快速的数组处理能⼒，NumPy在数据分析⽅⾯还有另外⼀个主要作⽤，即作为在算法和库之间传递数据的容器。对于数值型数据，NumPy数组在存储和处理数据时要⽐内<br>置的Python数据结构⾼效得多。此外，由低级语⾔（⽐如C和Fortran）编写的库可以直接操作NumPy数组中的数据，⽆需进⾏任何数据复制⼯作。因此，许多Python的数值计算⼯具要么使<br>⽤NumPy数组作为主要的数据结构，要么可以与NumPy进⾏⽆缝交互操作。</p><h3 id="pandas"><a href="#pandas" class="headerlink" title="pandas"></a>pandas</h3><p>pandas提供了快速便捷处理结构化数据的⼤量数据结构和函数。⾃从2010年出现以来，它助使Python成为强⼤⽽⾼效的数据分析环境。⽤得最多的pandas对象是DataFrame，它是⼀个⾯向列（column-oriented）的⼆维表结构，另⼀个是Series，⼀个⼀维的标签化数组对象。<a href="https://cnwuyueyu.github.io/2023/09/08/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-2-Pandas/">详见</a></p><p>pandas兼具NumPy⾼性能的数组计算功能以及电⼦表格和关系型数据库（如SQL）灵活的数据处理功能。它提供了复杂精细的索引功能，以便更为便捷地完成重塑、切⽚和切块、聚合以及选取数据⼦集等操作。因为数据操作、准备、清洗是数据分析最重要的技能。</p><p>pandas这个名字源于panel data（⾯板数据，这是多维结构化数据集在计量经济学中的术语）以及Python dataanalysis（Python数据分析）。</p><h3 id="matplotlib"><a href="#matplotlib" class="headerlink" title="matplotlib"></a>matplotlib</h3><p>matplotlib是最流⾏的⽤于绘制图表和其它⼆维数据可视化的Python库。它⾮常适合创建出版物上⽤的图表。虽然还有其它的Python可视化库，matplotlib却是使⽤最⼴泛的，并且它和其它⽣态⼯具配合也⾮常完美。</p><h3 id="IPython和Jupyter"><a href="#IPython和Jupyter" class="headerlink" title="IPython和Jupyter"></a>IPython和Jupyter</h3><p>IPython项⽬起初是Fernando Pérez在2001年的⼀个⽤以加强和Python交互的⼦项⽬。在随后的16年中，它成为了Python数据栈最重要的⼯具之⼀。虽然IPython本身没有提供计算和数据分析的⼯具，它却可以⼤⼤提⾼交互式计算和软件开发的⽣产率。IPython⿎励“执⾏-探索”的⼯作流，区别于其它编程软件的“编辑-编译-运⾏”的⼯作流。它还可以⽅便地访问系统的shell和⽂件系统。因为⼤部分的数据分析代码包括探索、试错和重复，IPython可以使⼯作更快。IPython shell 和Jupyter notebooks特别适合进⾏数据探索和可视化。</p><p>⼤部分Python都要⽤到IPython，包括运⾏、调试和测试代码。</p><h3 id="SciPy"><a href="#SciPy" class="headerlink" title="SciPy"></a>SciPy</h3><p>SciPy是⼀组专⻔解决科学计算中各种标准问题域的包的集合，主要包括下⾯这些包：</p><ul><li>scipy.integrate：数值积分例程和微分⽅程求解器。</li><li>scipy.linalg：扩展了由numpy.linalg提供的线性代数例程和矩阵分解功能。</li><li>scipy.optimize：函数优化器（最⼩化器）以及根查找算法。</li><li>scipy.signal：信号处理⼯具。</li><li>scipy.sparse：稀疏矩阵和稀疏线性系统求解器。</li><li>scipy.special：SPECFUN（这是⼀个实现了许多常⽤数学函数（如伽玛函数）的Fortran库）的包装器。</li><li>scipy.stats：标准连续和离散概率分布（如密度函数、采样器、连续分布函数等）、各种统计检验⽅法，以及更好的描述统计法。</li></ul><p>NumPy和SciPy结合使⽤，便形成了⼀个相当完备和成熟的计算平台，可以处理多种传统的科学计算问题。</p><h3 id="scikit-learn"><a href="#scikit-learn" class="headerlink" title="scikit-learn"></a>scikit-learn</h3><p>2010年诞⽣以来，scikit-learn成为了Python的通⽤机器学习⼯具包。它的⼦模块包括：</p><ul><li>分类：SVM、近邻、随机森林、逻辑回归等等。</li><li>回归：Lasso、岭回归等等。</li><li>聚类：k-均值、谱聚类等等。</li><li>降维：PCA、特征选择、矩阵分解等等。</li><li>选型：⽹格搜索、交叉验证、度量。</li><li>预处理：特征提取、标准化。</li></ul><p>与pandas、statsmodels和IPython⼀起，scikit-learn对于Python成为⾼效数据科学编程语⾔起到了关键作⽤。</p><h3 id="statsmodels"><a href="#statsmodels" class="headerlink" title="statsmodels"></a>statsmodels</h3><p>statsmodels是⼀个统计分析包，与scikit-learn⽐较，statsmodels包含经典统计学和经济计量学的算法。包括如下⼦模块：</p><ul><li>回归模型：线性回归，⼴义线性模型，健壮线性模型，线性混合效应模型等等。</li><li>⽅差分析（ANOVA）。</li><li>时间序列分析：AR，ARMA，ARIMA，VAR和其它模型。</li><li>⾮参数⽅法： 核密度估计，核回归。</li><li>统计模型结果可视化。</li></ul><p>statsmodels更关注与统计推断，提供不确定估计和参数p-值。相反的，scikit-learn注重预测。</p><h1 id="二-Python语法基础"><a href="#二-Python语法基础" class="headerlink" title="二.Python语法基础"></a>二.Python语法基础</h1><h2 id="1-IPython"><a href="#1-IPython" class="headerlink" title="1.IPython"></a>1.IPython</h2><h3 id="快捷键"><a href="#快捷键" class="headerlink" title="快捷键"></a>快捷键</h3><p>有许多键盘快捷键进⾏导航提示（类似Emacs⽂本编辑器或UNIX bash Shell）和交互shell的历史命令。</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-3-%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%98/image.ZUBJB2-16953667317141.png" alt="IPython sheel的快捷键"></p><p>魔术命令</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-3-%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%98/image.LEO7A2.png" alt="IPython魔术命令"></p><p>IPython同时集成了Matplotlib</p><p>三.Python的数据结构、函数和⽂件</p><p>P85</p><p>update time:</p><p>2023-09-22 16:57:38.877910</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2-工具与软件-2-Pandas</title>
    <link href="/2023/09/08/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-2-Pandas/"/>
    <url>/2023/09/08/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-2-Pandas/</url>
    
    <content type="html"><![CDATA[<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>在pycharm中对应的python解释器内安装pandas。</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-2-Pandas/image-20230919175623027.png" alt="pandas的安装"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas<br><br><span class="hljs-built_in">print</span>(pandas.__version__)<br></code></pre></td></tr></table></figure><p>Output:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-number">2.1</span><span class="hljs-number">.0</span><br><br>进程已结束,退出代码<span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><h2 id="Pandas-数据结构-Series"><a href="#Pandas-数据结构-Series" class="headerlink" title="Pandas 数据结构 - Series"></a>Pandas 数据结构 - Series</h2><p>Series 相当于表格中的一个列，函数如下：</p><p><code>pandas.Series( data, index, dtype, name, copy)</code></p><ul><li><strong>data</strong>：一组数据(ndarray 类型)。</li><li><strong>index</strong>：数据索引标签，如果不指定，默认从 0 开始。</li><li><strong>dtype</strong>：数据类型，默认会自己判断。</li><li><strong>name</strong>：设置名称。</li><li><strong>copy</strong>：拷贝数据，默认为 False。</li></ul><p>也可以使用Map来创建Series：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br>sites = &#123;<span class="hljs-number">1</span>: <span class="hljs-string">&quot;Google&quot;</span>, <span class="hljs-number">2</span>: <span class="hljs-string">&quot;Runoob&quot;</span>, <span class="hljs-number">3</span>: <span class="hljs-string">&quot;Wiki&quot;</span>&#125;<br><br>myvar = pd.Series(sites)<br><br><span class="hljs-built_in">print</span>(myvar)<br></code></pre></td></tr></table></figure><p>这样Key就变为了索引值。</p><h2 id="Pandas-数据结构-DataFrame"><a href="#Pandas-数据结构-DataFrame" class="headerlink" title="Pandas 数据结构 - DataFrame"></a>Pandas 数据结构 - DataFrame</h2><p>DataFrame 是一个表格型的数据结构，由一个index组成的第0列和DataFrame组成的n列构成，相当于Series组成的字典（共用一个index）。</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-2-Pandas/image-20230919182139779.png" alt="Series与DataFrame"></p><p>函数如下：</p><p><code>pandas.DataFrame( data, index, columns, dtype, copy)</code></p><ul><li><strong>data</strong>：一组数据(ndarray、series, map, lists, dict 等类型)。</li><li><strong>index</strong>：索引值，或者可以称为行标签。</li><li><strong>columns</strong>：列标签，默认为 RangeIndex (0, 1, 2, …, n) 。</li><li><strong>dtype</strong>：数据类型。</li><li><strong>copy</strong>：拷贝数据，默认为 False。</li></ul><p>使用ndarrays创建DataFrame对象:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br>data = &#123;<span class="hljs-string">&#x27;Site&#x27;</span>:[<span class="hljs-string">&#x27;Google&#x27;</span>, <span class="hljs-string">&#x27;Runoob&#x27;</span>, <span class="hljs-string">&#x27;Wiki&#x27;</span>], <span class="hljs-string">&#x27;Age&#x27;</span>:[<span class="hljs-number">10</span>, <span class="hljs-number">12</span>, <span class="hljs-number">13</span>]&#125;<br><br>df = pd.DataFrame(data)<br><br><span class="hljs-built_in">print</span> (df)<br></code></pre></td></tr></table></figure><p>Site Age为列名，0、1、2为行标。</p><p>使用Map创建与Series同理。</p><h2 id="Pandas-CSV-文件"><a href="#Pandas-CSV-文件" class="headerlink" title="Pandas CSV 文件"></a>Pandas CSV 文件</h2><p>Pandas 可以很方便的处理 CSV 文件</p><p><code>df = pd.read_csv(&#39;nba.csv&#39;)</code>读取CSV文件。</p><p><code>df.to_csv(&#39;site.csv&#39;)</code>将DataFrame储存为csv文件。</p><p>数据处理</p><p><strong>head( n )</strong> 方法用于读取前面的 n 行，如果不填参数 n ，默认返回 5 行。</p><p><strong>tail( n )</strong> 方法用于读取尾部的 n 行，如果不填参数 n ，默认返回 5 行，空行各个字段的值返回 <strong>NaN</strong>。</p><p><strong>info()</strong> 方法返回表格的一些基本信息：</p><h2 id="Pandas-JSON-文件"><a href="#Pandas-JSON-文件" class="headerlink" title="Pandas JSON 文件"></a>Pandas JSON 文件</h2><p>Pandas 可以很方便的处理 JSON 数据</p><p><strong>to_string()</strong> 用于返回 <strong>DataFrame</strong>(表格) 类型的数据，我们也可以直接处理 JSON 字符串。</p><p>如果是字符串格式的 JSON 可以直接将Python字典转为DataFrame数据（json对象与Map有相同的格式）</p><p><strong>Json数据的解析</strong></p><p>直接加载一个print一个json文件打印出的并不直观。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">          school_name   <span class="hljs-keyword">class</span>                                           <span class="hljs-title class_">students</span><br><span class="hljs-number">0</span>  ABC primary school  Year <span class="hljs-number">1</span>  &#123;<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-string">&#x27;A001&#x27;</span>, <span class="hljs-string">&#x27;name&#x27;</span>: <span class="hljs-string">&#x27;Tom&#x27;</span>, <span class="hljs-string">&#x27;math&#x27;</span>: <span class="hljs-number">60</span>, <span class="hljs-string">&#x27;phy...</span><br><span class="hljs-string">1  ABC primary school  Year 1  &#123;&#x27;</span><span class="hljs-built_in">id</span><span class="hljs-string">&#x27;: &#x27;</span>A002<span class="hljs-string">&#x27;, &#x27;</span>name<span class="hljs-string">&#x27;: &#x27;</span>James<span class="hljs-string">&#x27;, &#x27;</span>math<span class="hljs-string">&#x27;: 89, &#x27;</span>p...<br><span class="hljs-number">2</span>  ABC primary school  Year <span class="hljs-number">1</span>  &#123;<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-string">&#x27;A003&#x27;</span>, <span class="hljs-string">&#x27;name&#x27;</span>: <span class="hljs-string">&#x27;Jenny&#x27;</span>, <span class="hljs-string">&#x27;math&#x27;</span>: <span class="hljs-number">79</span>, <span class="hljs-string">&#x27;p...</span><br><span class="hljs-string"></span><br></code></pre></td></tr></table></figure><p>用到 <strong>json_normalize()</strong> 方法将内嵌的数据完整的解析出来</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> json<br><br><span class="hljs-comment"># 使用 Python JSON 模块载入数据</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;nested_list.json&#x27;</span>,<span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    data = json.loads(f.read()) <br>    <span class="hljs-comment">#data = json.loads(f.read()) 使用 Python JSON 模块载入数据。</span><br><br><span class="hljs-comment"># 展平数据</span><br>df_nested_list = pd.json_normalize(data, record_path =[<span class="hljs-string">&#x27;students&#x27;</span>])<br><span class="hljs-built_in">print</span>(df_nested_list)<br></code></pre></td></tr></table></figure><p>Output:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">     <span class="hljs-built_in">id</span>   name  math  physics  chemistry<br><span class="hljs-number">0</span>  A001    Tom    <span class="hljs-number">60</span>       <span class="hljs-number">66</span>         <span class="hljs-number">61</span><br><span class="hljs-number">1</span>  A002  James    <span class="hljs-number">89</span>       <span class="hljs-number">76</span>         <span class="hljs-number">51</span><br><span class="hljs-number">2</span>  A003  Jenny    <span class="hljs-number">79</span>       <span class="hljs-number">90</span>         <span class="hljs-number">78</span><br></code></pre></td></tr></table></figure><p><strong>读取一组数据glom</strong></p><p><strong>import</strong> pandas <strong>as</strong> pd<br><strong>from</strong> glom <strong>import</strong> glom   glom模块准许使用“.”来获取内嵌对象的属性</p><p><code>data = df[&#39;students&#39;].apply(**lambda** row: glom(row, &#39;grade.math&#39;))</code></p><p>效果类似于查找。</p><h2 id="Pandas-数据清洗"><a href="#Pandas-数据清洗" class="headerlink" title="Pandas 数据清洗"></a>Pandas 数据清洗</h2><p>很多数据集存在数据缺失、数据格式错误、错误数据或重复数据的情况，如果要使数据分析更加准确，就需要对这些没有用的数据进行处理（清洗）。</p><p>例如数据中的“n&#x2F;a  NA  –  na”，或者空值等。</p><p>清洗空值：dropna（）方法</p><p><code>DataFrame.dropna(axis=0, how=&#39;any&#39;, thresh=None, subset=None, inplace=False)</code></p><ul><li>axis：默认为 <strong>0</strong>，表示逢空值剔除整行，如果设置参数 <strong>axis＝1</strong> 表示逢空值去掉整列。</li><li>how：默认为 <strong>‘any’</strong> 如果一行（或一列）里任何一个数据有出现 NA 就去掉整行，如果设置 <strong>how&#x3D;’all’</strong> 一行（或列）都是 NA 才去掉这整行。</li><li>thresh：设置需要多少非空值的数据才可以保留下来的。</li><li>subset：设置想要检查的列。如果是多个列，可以使用列名的 list 作为参数。</li><li>inplace：如果设置 True，将计算得到的值直接覆盖之前的值并返回 None，修改的是源数据。</li></ul><p>可以先用isnull()判断是否为空。</p><p>可以在read_csv（）方法中指定空数据的数据类型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br>missing_values = [<span class="hljs-string">&quot;n/a&quot;</span>, <span class="hljs-string">&quot;na&quot;</span>, <span class="hljs-string">&quot;--&quot;</span>]<br>df = pd.read_csv(<span class="hljs-string">&#x27;property-data.csv&#x27;</span>, na_values = missing_values)<br></code></pre></td></tr></table></figure><p>dropna会返回一个新的Dataframe不会修改源数据。如果需要修改在inplace设置为True。</p><p>移除指定列有空值的行（移除 ST_NUM 列值为空）<code>df.dropna(subset=[&#39;ST_NUM&#39;], inplace = True)</code></p><p>我们也可以 <strong>fillna()</strong> 方法来替换一些空字段<code>df.fillna(12345, inplace = True)</code></p><p>指定某一个列来替换数据<code>df[&#39;PID&#39;].fillna(12345, inplace = True)</code></p><p>替换空单元格的常用方法是计算列的均值、中值或众数。</p><p>Pandas使用 <strong>mean()<strong>、</strong>median()</strong> 和 <strong>mode()</strong> 方法计算列的均值（所有值加起来的平均值）、中位数值（排序后排在中间的数）和众数（出现频率最高的数）。</p><p>比如mode() 方法计算列的众数并替换空单元格：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br>df = pd.read_csv(<span class="hljs-string">&#x27;property-data.csv&#x27;</span>)<br><br>x = df[<span class="hljs-string">&quot;ST_NUM&quot;</span>].mode()<br><br>df[<span class="hljs-string">&quot;ST_NUM&quot;</span>].fillna(x, inplace = <span class="hljs-literal">True</span>)<br><br><span class="hljs-built_in">print</span>(df.to_string())<br></code></pre></td></tr></table></figure><p><strong>清洗格式错误数据:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br><span class="hljs-comment"># 第三个日期格式错误</span><br>data = &#123;<br>  <span class="hljs-string">&quot;Date&quot;</span>: [<span class="hljs-string">&#x27;2020/12/01&#x27;</span>, <span class="hljs-string">&#x27;2020/12/02&#x27;</span> , <span class="hljs-string">&#x27;20201226&#x27;</span>],<br>  <span class="hljs-string">&quot;duration&quot;</span>: [<span class="hljs-number">50</span>, <span class="hljs-number">40</span>, <span class="hljs-number">45</span>]<br>&#125;<br><br>df = pd.DataFrame(data, index = [<span class="hljs-string">&quot;day1&quot;</span>, <span class="hljs-string">&quot;day2&quot;</span>, <span class="hljs-string">&quot;day3&quot;</span>])<br><br>df[<span class="hljs-string">&#x27;Date&#x27;</span>] = pd.to_datetime(df[<span class="hljs-string">&#x27;Date&#x27;</span>])<br><br><span class="hljs-built_in">print</span>(df.to_string())<br><br><br><span class="hljs-comment">#output：</span><br>           Date  duration<br>day1 <span class="hljs-number">2020</span>-<span class="hljs-number">12</span>-01        <span class="hljs-number">50</span><br>day2 <span class="hljs-number">2020</span>-<span class="hljs-number">12</span>-02        <span class="hljs-number">40</span><br>day3 <span class="hljs-number">2020</span>-<span class="hljs-number">12</span>-<span class="hljs-number">26</span>        <span class="hljs-number">45</span><br></code></pre></td></tr></table></figure><p><strong>清洗错误数据：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br>person = &#123;<br>  <span class="hljs-string">&quot;name&quot;</span>: [<span class="hljs-string">&#x27;Google&#x27;</span>, <span class="hljs-string">&#x27;Runoob&#x27;</span> , <span class="hljs-string">&#x27;Taobao&#x27;</span>],<br>  <span class="hljs-string">&quot;age&quot;</span>: [<span class="hljs-number">50</span>, <span class="hljs-number">40</span>, <span class="hljs-number">12345</span>]    <span class="hljs-comment"># 12345 年龄数据是错误的</span><br>&#125;<br><br>df = pd.DataFrame(person)<br><br>df.loc[<span class="hljs-number">2</span>, <span class="hljs-string">&#x27;age&#x27;</span>] = <span class="hljs-number">30</span> <span class="hljs-comment"># 修改数据  对错误的数据进行替换或移除。也可以使用if来判断if df.loc[x, &quot;age&quot;] &gt; ?: df.drop(x, inplaced = True)</span><br><br><span class="hljs-built_in">print</span>(df.to_string())<br><br><span class="hljs-comment">#output:</span><br>     name  age<br><span class="hljs-number">0</span>  Google   <span class="hljs-number">50</span><br><span class="hljs-number">1</span>  Runoob   <span class="hljs-number">40</span><br><span class="hljs-number">2</span>  Taobao   <span class="hljs-number">30</span><br></code></pre></td></tr></table></figure><p><strong>清洗重复数据:</strong></p><p>如果我们要清洗重复数据，可以使用 <strong>duplicated()</strong> 和 <strong>drop_duplicates()</strong> 方法。</p><p>如果对应的数据是重复的，<strong>duplicated()</strong> 会返回 True，否则返回 False。</p><h2 id="Pandas-常用函数"><a href="#Pandas-常用函数" class="headerlink" title="Pandas 常用函数"></a>Pandas 常用函数</h2><h3 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h3><table><thead><tr><th align="left">函数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">pd.read_csv(filename)</td><td align="left">读取 CSV 文件；</td></tr><tr><td align="left">pd.read_excel(filename)</td><td align="left">读取 Excel 文件；</td></tr><tr><td align="left">pd.read_sql(query, connection_object)</td><td align="left">从 SQL 数据库读取数据；</td></tr><tr><td align="left">pd.read_json(json_string)</td><td align="left">从 JSON 字符串中读取数据；</td></tr><tr><td align="left">pd.read_html(url)</td><td align="left">从 HTML 页面中读取数据。</td></tr></tbody></table><h3 id="查看数据"><a href="#查看数据" class="headerlink" title="查看数据"></a>查看数据</h3><table><thead><tr><th align="left">函数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">df.head(n)</td><td align="left">显示前 n 行数据；</td></tr><tr><td align="left">df.tail(n)</td><td align="left">显示后 n 行数据；</td></tr><tr><td align="left">df.info()</td><td align="left">显示数据的信息，包括列名、数据类型、缺失值等；</td></tr><tr><td align="left">df.describe()</td><td align="left">显示数据的基本统计信息，包括均值、方差、最大值、最小值等；</td></tr><tr><td align="left">df.shape</td><td align="left">显示数据的行数和列数。</td></tr></tbody></table><h3 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h3><table><thead><tr><th align="left">函数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">df.dropna()</td><td align="left">删除包含缺失值的行或列；</td></tr><tr><td align="left">df.fillna(value)</td><td align="left">将缺失值替换为指定的值；</td></tr><tr><td align="left">df.replace(old_value, new_value)</td><td align="left">将指定值替换为新值；</td></tr><tr><td align="left">df.duplicated()</td><td align="left">检查是否有重复的数据；</td></tr><tr><td align="left">df.drop_duplicates()</td><td align="left">删除重复的数据。</td></tr></tbody></table><h3 id="数据选择和切片"><a href="#数据选择和切片" class="headerlink" title="数据选择和切片"></a>数据选择和切片</h3><table><thead><tr><th align="left">函数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">df[column_name]</td><td align="left">选择指定的列；</td></tr><tr><td align="left">df.loc[row_index, column_name]</td><td align="left">通过标签选择数据；</td></tr><tr><td align="left">df.iloc[row_index, column_index]</td><td align="left">通过位置选择数据；</td></tr><tr><td align="left">df.ix[row_index, column_name]</td><td align="left">通过标签或位置选择数据；</td></tr><tr><td align="left">df.filter(items&#x3D;[column_name1, column_name2])</td><td align="left">选择指定的列；</td></tr><tr><td align="left">df.filter(regex&#x3D;’regex’)</td><td align="left">选择列名匹配正则表达式的列；</td></tr><tr><td align="left">df.sample(n)</td><td align="left">随机选择 n 行数据。</td></tr></tbody></table><h3 id="数据排序"><a href="#数据排序" class="headerlink" title="数据排序"></a>数据排序</h3><table><thead><tr><th align="left">函数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">df.sort_values(column_name)</td><td align="left">按照指定列的值排序；</td></tr><tr><td align="left">df.sort_values([column_name1, column_name2], ascending&#x3D;[True, False])</td><td align="left">按照多个列的值排序；</td></tr><tr><td align="left">df.sort_index()</td><td align="left">按照索引排序。</td></tr></tbody></table><h3 id="数据分组和聚合"><a href="#数据分组和聚合" class="headerlink" title="数据分组和聚合"></a>数据分组和聚合</h3><table><thead><tr><th align="left">函数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">df.groupby(column_name)</td><td align="left">按照指定列进行分组；</td></tr><tr><td align="left">df.aggregate(function_name)</td><td align="left">对分组后的数据进行聚合操作；</td></tr><tr><td align="left">df.pivot_table(values, index, columns, aggfunc)</td><td align="left">生成透视表。</td></tr></tbody></table><h3 id="数据合并"><a href="#数据合并" class="headerlink" title="数据合并"></a>数据合并</h3><table><thead><tr><th align="left">函数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">pd.concat([df1, df2])</td><td align="left">将多个数据框按照行或列进行合并；</td></tr><tr><td align="left">pd.merge(df1, df2, on&#x3D;column_name)</td><td align="left">按照指定列将两个数据框进行合并。</td></tr></tbody></table><h3 id="数据选择和过滤"><a href="#数据选择和过滤" class="headerlink" title="数据选择和过滤"></a>数据选择和过滤</h3><table><thead><tr><th align="left">函数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">df.loc[row_indexer, column_indexer]</td><td align="left">按标签选择行和列。</td></tr><tr><td align="left">df.iloc[row_indexer, column_indexer]</td><td align="left">按位置选择行和列。</td></tr><tr><td align="left">df[df[‘column_name’] &gt; value]</td><td align="left">选择列中满足条件的行。</td></tr><tr><td align="left">df.query(‘column_name &gt; value’)</td><td align="left">使用字符串表达式选择列中满足条件的行。</td></tr></tbody></table><h3 id="数据统计和描述"><a href="#数据统计和描述" class="headerlink" title="数据统计和描述"></a>数据统计和描述</h3><table><thead><tr><th align="left">函数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">df.describe()</td><td align="left">计算基本统计信息，如均值、标准差、最小值、最大值等。</td></tr><tr><td align="left">df.mean()</td><td align="left">计算每列的平均值。</td></tr><tr><td align="left">df.median()</td><td align="left">计算每列的中位数。</td></tr><tr><td align="left">df.mode()</td><td align="left">计算每列的众数。</td></tr><tr><td align="left">df.count()</td><td align="left">计算每列非缺失值的数量。</td></tr></tbody></table><p>UPDATE TIME: 星期二 2023年9月19日</p>]]></content>
    
    
    
    <tags>
      
      <tag>Pandas</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2-工具与软件-1-Numpy</title>
    <link href="/2023/09/08/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-1-Numpy/"/>
    <url>/2023/09/08/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-1-Numpy/</url>
    
    <content type="html"><![CDATA[<p>NumPy用于数据分析，提供了大量的维度数组与矩阵运算，NumPy 通常与 SciPy（Scientific Python）和 Matplotlib（绘图库）一起使用。</p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p><code>sudo apt-get install python3-numpy python3-scipy python3-matplotlib ipython ipython-notebook python-pandas python-sympy python-nose</code></p><p>或使用pycharm在import numpy后自动导入。</p><h3 id="N-维数组对象-ndarray"><a href="#N-维数组对象-ndarray" class="headerlink" title="N 维数组对象 ndarray"></a>N 维数组对象 ndarray</h3><p>ndarray 中的每个元素在内存中都有相同存储大小的区域</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">numpy.array(<span class="hljs-built_in">object</span>, dtype = <span class="hljs-literal">None</span>, copy = <span class="hljs-literal">True</span>, order = <span class="hljs-literal">None</span>, subok = <span class="hljs-literal">False</span>, ndmin = <span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><table><thead><tr><th align="left">名称</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">object</td><td align="left">数组或嵌套的数列</td></tr><tr><td align="left">dtype</td><td align="left">数组元素的数据类型，可选</td></tr><tr><td align="left">copy</td><td align="left">对象是否需要复制，可选</td></tr><tr><td align="left">order</td><td align="left">创建数组的样式，C为行方向，F为列方向，A为任意方向（默认）</td></tr><tr><td align="left">subok</td><td align="left">默认返回一个与基类类型一致的数组</td></tr><tr><td align="left">ndmin</td><td align="left">指定生成数组的最小维度</td></tr></tbody></table><h3 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h3><p>int8, int16, int32, int64 四种数据类型可以使用字符串 ‘i1’, ‘i2’,’i4’,’i8’ 代替</p><table><thead><tr><th align="left">名称</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">bool_</td><td align="left">布尔型数据类型（True 或者 False）</td></tr><tr><td align="left">int_</td><td align="left">默认的整数类型（类似于 C 语言中的 long，int32 或 int64）</td></tr><tr><td align="left">intc</td><td align="left">与 C 的 int 类型一样，一般是 int32 或 int 64</td></tr><tr><td align="left">intp</td><td align="left">用于索引的整数类型（类似于 C 的 ssize_t，一般情况下仍然是 int32 或 int64）</td></tr><tr><td align="left">int8</td><td align="left">字节（-128 to 127）</td></tr><tr><td align="left">int16</td><td align="left">整数（-32768 to 32767）</td></tr><tr><td align="left">int32</td><td align="left">整数（-2147483648 to 2147483647）</td></tr><tr><td align="left">int64</td><td align="left">整数（-9223372036854775808 to 9223372036854775807）</td></tr><tr><td align="left">uint8</td><td align="left">无符号整数（0 to 255）</td></tr><tr><td align="left">uint16</td><td align="left">无符号整数（0 to 65535）</td></tr><tr><td align="left">uint32</td><td align="left">无符号整数（0 to 4294967295）</td></tr><tr><td align="left">uint64</td><td align="left">无符号整数（0 to 18446744073709551615）</td></tr><tr><td align="left">float_</td><td align="left">float64 类型的简写</td></tr><tr><td align="left">float16</td><td align="left">半精度浮点数，包括：1 个符号位，5 个指数位，10 个尾数位</td></tr><tr><td align="left">float32</td><td align="left">单精度浮点数，包括：1 个符号位，8 个指数位，23 个尾数位</td></tr><tr><td align="left">float64</td><td align="left">双精度浮点数，包括：1 个符号位，11 个指数位，52 个尾数位</td></tr><tr><td align="left">complex_</td><td align="left">complex128 类型的简写，即 128 位复数</td></tr><tr><td align="left">complex64</td><td align="left">复数，表示双 32 位浮点数（实数部分和虚数部分）</td></tr><tr><td align="left">complex128</td><td align="left">复数，表示双 64 位浮点数（实数部分和虚数部分）</td></tr></tbody></table><p>在创建dtype中（数据类型对象），每个内建类型都有一个唯一定义它的字符代码</p><table><thead><tr><th align="left">字符</th><th align="left">对应类型</th></tr></thead><tbody><tr><td align="left">b</td><td align="left">布尔型</td></tr><tr><td align="left">i</td><td align="left">(有符号) 整型</td></tr><tr><td align="left">u</td><td align="left">无符号整型 integer</td></tr><tr><td align="left">f</td><td align="left">浮点型</td></tr><tr><td align="left">c</td><td align="left">复数浮点型</td></tr><tr><td align="left">m</td><td align="left">timedelta（时间间隔）</td></tr><tr><td align="left">M</td><td align="left">datetime（日期时间）</td></tr><tr><td align="left">O</td><td align="left">(Python) 对象</td></tr><tr><td align="left">S, a</td><td align="left">(byte-)字符串</td></tr><tr><td align="left">U</td><td align="left">Unicode</td></tr><tr><td align="left">V</td><td align="left">原始数据 (void)</td></tr></tbody></table><h3 id="Numpy数组"><a href="#Numpy数组" class="headerlink" title="Numpy数组"></a>Numpy数组</h3><p>维数——秩（rank），维度——轴（axis）</p><table><thead><tr><th align="left">属性</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">ndarray.ndim</td><td align="left">秩，即轴的数量或维度的数量</td></tr><tr><td align="left">ndarray.shape</td><td align="left">数组的维度，对于矩阵，n 行 m 列</td></tr><tr><td align="left">ndarray.size</td><td align="left">数组元素的总个数，相当于 .shape 中 n*m 的值</td></tr><tr><td align="left">ndarray.dtype</td><td align="left">ndarray 对象的元素类型</td></tr><tr><td align="left">ndarray.itemsize</td><td align="left">ndarray 对象中每个元素的大小，以字节为单位</td></tr><tr><td align="left">ndarray.flags</td><td align="left">ndarray 对象的内存信息</td></tr><tr><td align="left">ndarray.real</td><td align="left">ndarray元素的实部</td></tr><tr><td align="left">ndarray.imag</td><td align="left">ndarray 元素的虚部</td></tr><tr><td align="left">ndarray.data</td><td align="left">包含实际数组元素的缓冲区，由于一般通过数组的索引获取元素，所以通常不需要使用这个属性。</td></tr></tbody></table><p>数组的创建</p><p><code>numpy.empty</code> 方法用来创建一个指定形状（shape）、数据类型（dtype）且未初始化的数组：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">numpy.empty(shape, dtype = <span class="hljs-built_in">float</span>, order = <span class="hljs-string">&#x27;C&#x27;</span>)<br></code></pre></td></tr></table></figure><table><thead><tr><th align="left">参数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">shape</td><td align="left">数组形状</td></tr><tr><td align="left">dtype</td><td align="left">数据类型，可选</td></tr><tr><td align="left">order</td><td align="left">有”C”和”F”两个选项,分别代表，行优先和列优先，在计算机内存中的存储元素的顺序。</td></tr></tbody></table><p><code>numpy.zeros</code> 创建指定大小的数组，数组元素以 0 来填充：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">numpy.zeros(shape, dtype = <span class="hljs-built_in">float</span>, order = <span class="hljs-string">&#x27;C&#x27;</span>)<br></code></pre></td></tr></table></figure><table><thead><tr><th align="left">参数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">shape</td><td align="left">数组形状</td></tr><tr><td align="left">dtype</td><td align="left">数据类型，可选 默认为浮点数</td></tr><tr><td align="left">order</td><td align="left">‘C’ 用于 C 的行数组，或者 ‘F’ 用于 FORTRAN 的列数组</td></tr></tbody></table><p><code>numpy.ones</code> 创建指定形状的数组，数组元素以 1 来填充：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">numpy.ones(shape, dtype = <span class="hljs-literal">None</span>, order = <span class="hljs-string">&#x27;C&#x27;</span>)<br></code></pre></td></tr></table></figure><table><thead><tr><th align="left">参数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">shape</td><td align="left">数组形状</td></tr><tr><td align="left">dtype</td><td align="left">数据类型，可选 默认为浮点数</td></tr><tr><td align="left">order</td><td align="left">‘C’ 用于 C 的行数组，或者 ‘F’ 用于 FORTRAN 的列数组</td></tr></tbody></table><p><code>numpy.zeros_like</code>  <code>numpy.ones_like</code> 创建一个模仿数组，以1或者0进行填充。</p><p>numpy.asarray 类似 numpy.array，但 numpy.asarray 参数只有三个，比 numpy.array 少两个。</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs crmsh">numpy.asarray(a, dtype = None, <span class="hljs-keyword">order</span> <span class="hljs-title">= None</span>)<br></code></pre></td></tr></table></figure><p>参数说明：</p><table><thead><tr><th align="left">参数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">a</td><td align="left">任意形式的输入参数，可以是，列表, 列表的元组, 元组, 元组的元组, 元组的列表，多维数组</td></tr><tr><td align="left">dtype</td><td align="left">数据类型，可选</td></tr><tr><td align="left">order</td><td align="left">可选，有”C”和”F”两个选项,分别代表，行优先和列优先，在计算机内存中的存储元素的顺序。</td></tr></tbody></table><p>例:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>x = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]<br>a = np.asarray(x)<br><span class="hljs-built_in">print</span>(x)<br><span class="hljs-built_in">print</span>(a)<br></code></pre></td></tr></table></figure><h6 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]<br>[<span class="hljs-number">1</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span>]<br><span class="hljs-comment">#由此可以看出list和array的区别</span><br></code></pre></td></tr></table></figure><p><code>numpy.frombuffer</code> 用于实现动态数组。</p><p><code>numpy.frombuffer</code> 接受 buffer 输入参数，以流的形式读入转化成 ndarray 对象。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">numpy.frombuffer(buffer, dtype = <span class="hljs-built_in">float</span>, count = -<span class="hljs-number">1</span>, offset = <span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><table><thead><tr><th align="left">参数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">buffer</td><td align="left">可以是任意对象，会以流的形式读入。</td></tr><tr><td align="left">dtype</td><td align="left">返回数组的数据类型，可选</td></tr><tr><td align="left">count</td><td align="left">读取的数据数量，默认为-1，读取所有数据。</td></tr><tr><td align="left">offset</td><td align="left">读取的起始位置，默认为0。</td></tr></tbody></table><p><code>numpy.fromiter</code> 方法从可迭代对象中建立 ndarray 对象，返回一维数组。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">numpy.fromiter(iterable, dtype, count=-<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><table><thead><tr><th align="left">参数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">iterable</td><td align="left">可迭代对象</td></tr><tr><td align="left">dtype</td><td align="left">返回数组的数据类型</td></tr><tr><td align="left">count</td><td align="left">读取的数据数量，默认为-1，读取所有数据</td></tr></tbody></table><p>从数值范围创建数组</p><p>numpy 包中的使用 arange 函数创建数值范围并返回 ndarray 对象，函数格式如下：</p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs arduino">numpy.<span class="hljs-built_in">arange</span>(start, stop, step, dtype)<br></code></pre></td></tr></table></figure><p>根据 start 与 stop 指定的范围以及 step 设定的步长，生成一个 ndarray。</p><p>参数说明：</p><table><thead><tr><th align="left">参数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left"><code>start</code></td><td align="left">起始值，默认为<code>0</code></td></tr><tr><td align="left"><code>stop</code></td><td align="left">终止值（不包含）</td></tr><tr><td align="left"><code>step</code></td><td align="left">步长，默认为<code>1</code></td></tr><tr><td align="left"><code>dtype</code></td><td align="left">返回<code>ndarray</code>的数据类型，如果没有提供，则会使用输入数据的类型。</td></tr></tbody></table><p><code>numpy.linspace</code> 函数用于创建一个一维数组，数组是一个等<strong>差数列构</strong>成的，格式如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">np.linspace(start, stop, <span class="hljs-attribute">num</span>=50, <span class="hljs-attribute">endpoint</span>=<span class="hljs-literal">True</span>, <span class="hljs-attribute">retstep</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">dtype</span>=None)<br></code></pre></td></tr></table></figure><p>参数说明：</p><table><thead><tr><th align="left">参数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left"><code>start</code></td><td align="left">序列的起始值</td></tr><tr><td align="left"><code>stop</code></td><td align="left">序列的终止值，如果<code>endpoint</code>为<code>true</code>，该值包含于数列中</td></tr><tr><td align="left"><code>num</code></td><td align="left">要生成的等步长的样本数量，默认为<code>50</code></td></tr><tr><td align="left"><code>endpoint</code></td><td align="left">该值为 <code>true</code> 时，数列中包含<code>stop</code>值，反之不包含，默认是True。</td></tr><tr><td align="left"><code>retstep</code></td><td align="left">如果为 True 时，生成的数组中会显示间距，反之不显示。</td></tr><tr><td align="left"><code>dtype</code></td><td align="left"><code>ndarray</code> 的数据类型</td></tr></tbody></table><p><code>numpy.logspace</code> 函数用于创建一个于<strong>等比数列</strong>。格式如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">np.logspace(start, stop, <span class="hljs-attribute">num</span>=50, <span class="hljs-attribute">endpoint</span>=<span class="hljs-literal">True</span>, <span class="hljs-attribute">base</span>=10.0, <span class="hljs-attribute">dtype</span>=None)<br></code></pre></td></tr></table></figure><p>base 参数意思是取对数的时候 log 的下标。</p><table><thead><tr><th align="left">参数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left"><code>start</code></td><td align="left">序列的起始值为：base ** start</td></tr><tr><td align="left"><code>stop</code></td><td align="left">序列的终止值为：base ** stop。如果<code>endpoint</code>为<code>true</code>，该值包含于数列中</td></tr><tr><td align="left"><code>num</code></td><td align="left">要生成的等步长的样本数量，默认为<code>50</code></td></tr><tr><td align="left"><code>endpoint</code></td><td align="left">该值为 <code>true</code> 时，数列中中包含<code>stop</code>值，反之不包含，默认是True。</td></tr><tr><td align="left"><code>base</code></td><td align="left">对数 log 的底数。</td></tr><tr><td align="left"><code>dtype</code></td><td align="left"><code>ndarray</code> 的数据类型</td></tr></tbody></table><h3 id="切片和索引"><a href="#切片和索引" class="headerlink" title="切片和索引"></a>切片和索引</h3><p>与list的切片相差不大，使用slice方法或者[:::] (start:finish:step)即可，另外对于多维数组的切分，可使用省略号<code>...</code> </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>a = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>], [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>]])<br><span class="hljs-built_in">print</span>(a[..., <span class="hljs-number">1</span>])  <span class="hljs-comment"># 第2列元素</span><br><span class="hljs-built_in">print</span>(a[<span class="hljs-number">1</span>, ...])  <span class="hljs-comment"># 第2行元素</span><br><span class="hljs-built_in">print</span>(a[..., <span class="hljs-number">1</span>:])  <span class="hljs-comment"># 第2列及剩下的所有元素</span><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">[[<span class="hljs-number">1</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span>]<br> [<span class="hljs-number">3</span> <span class="hljs-number">4</span> <span class="hljs-number">5</span>]<br> [<span class="hljs-number">4</span> <span class="hljs-number">5</span> <span class="hljs-number">6</span>]]<br>---------------------<br>[<span class="hljs-number">2</span> <span class="hljs-number">4</span> <span class="hljs-number">5</span>]<br>---------------------<br>[<span class="hljs-number">3</span> <span class="hljs-number">4</span> <span class="hljs-number">5</span>]<br>---------------------<br>[[<span class="hljs-number">2</span> <span class="hljs-number">3</span>]<br> [<span class="hljs-number">4</span> <span class="hljs-number">5</span>]<br> [<span class="hljs-number">5</span> <span class="hljs-number">6</span>]]<br></code></pre></td></tr></table></figure><p>整数数组索引是指使用一个数组来访问另一个数组的元素。这个数组中的每个元素都是目标数组中某个维度上的索引值。</p><p>以下实例获取了 4X3 数组中的四个角的元素。 行索引是 [0,0] 和 [3,3]，而列索引是 [0,2] 和 [0,2]。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np <br> <br>x = np.array([[  <span class="hljs-number">0</span>,  <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>],[  <span class="hljs-number">3</span>,  <span class="hljs-number">4</span>,  <span class="hljs-number">5</span>],[  <span class="hljs-number">6</span>,  <span class="hljs-number">7</span>,  <span class="hljs-number">8</span>],[  <span class="hljs-number">9</span>,  <span class="hljs-number">10</span>,  <span class="hljs-number">11</span>]])  <br><span class="hljs-built_in">print</span> (<span class="hljs-string">&#x27;我们的数组是：&#x27;</span> )<br><span class="hljs-built_in">print</span> (x)<br><span class="hljs-built_in">print</span> (<span class="hljs-string">&#x27;\n&#x27;</span>)<br>rows = np.array([[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>],[<span class="hljs-number">3</span>,<span class="hljs-number">3</span>]]) <br>cols = np.array([[<span class="hljs-number">0</span>,<span class="hljs-number">2</span>],[<span class="hljs-number">0</span>,<span class="hljs-number">2</span>]]) <br><span class="hljs-comment">#这里的索引是0 0,0 2,3 0,3 2</span><br>y = x[rows,cols]  <br><span class="hljs-built_in">print</span>  (<span class="hljs-string">&#x27;这个数组的四个角元素是：&#x27;</span>)<br><span class="hljs-built_in">print</span> (y)<br><br></code></pre></td></tr></table></figure><h6 id="Output-1"><a href="#Output-1" class="headerlink" title="Output"></a>Output</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">我们的数组是：<br>[[ <span class="hljs-number">0</span>  <span class="hljs-number">1</span>  <span class="hljs-number">2</span>]<br> [ <span class="hljs-number">3</span>  <span class="hljs-number">4</span>  <span class="hljs-number">5</span>]<br> [ <span class="hljs-number">6</span>  <span class="hljs-number">7</span>  <span class="hljs-number">8</span>]<br> [ <span class="hljs-number">9</span> <span class="hljs-number">10</span> <span class="hljs-number">11</span>]]<br><br><br>这个数组的四个角元素是：<br>[[ <span class="hljs-number">0</span>  <span class="hljs-number">2</span>]<br> [ <span class="hljs-number">9</span> <span class="hljs-number">11</span>]]<br></code></pre></td></tr></table></figure><p>关于 np.ix_ 的具体使用：</p><p><code>x[np.ix_([1,5,7,2],[0,3,1,2])]</code> 这句话会输出一个4*4的矩阵，其中的元素分别是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">x[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>] x[<span class="hljs-number">1</span>,<span class="hljs-number">3</span>] x[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>] x[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>]<br>x[<span class="hljs-number">5</span>,<span class="hljs-number">0</span>] x[<span class="hljs-number">5</span>,<span class="hljs-number">3</span>] x[<span class="hljs-number">5</span>,<span class="hljs-number">1</span>] x[<span class="hljs-number">5</span>,<span class="hljs-number">2</span>]<br>x[<span class="hljs-number">7</span>,<span class="hljs-number">0</span>] x[<span class="hljs-number">7</span>,<span class="hljs-number">3</span>] x[<span class="hljs-number">7</span>,<span class="hljs-number">1</span>] x[<span class="hljs-number">7</span>,<span class="hljs-number">2</span>]<br>x[<span class="hljs-number">2</span>,<span class="hljs-number">0</span>] x[<span class="hljs-number">2</span>,<span class="hljs-number">3</span>] x[<span class="hljs-number">2</span>,<span class="hljs-number">1</span>] x[<span class="hljs-number">2</span>,<span class="hljs-number">2</span>]<br></code></pre></td></tr></table></figure><p>相当于：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">y=np.array([[x[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>], x[<span class="hljs-number">1</span>,<span class="hljs-number">3</span>], x[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>], x[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>]],\<br>            [x[<span class="hljs-number">5</span>,<span class="hljs-number">0</span>], x[<span class="hljs-number">5</span>,<span class="hljs-number">3</span>], x[<span class="hljs-number">5</span>,<span class="hljs-number">1</span>],x[<span class="hljs-number">5</span>,<span class="hljs-number">2</span>]],\<br>            [x[<span class="hljs-number">7</span>,<span class="hljs-number">0</span>] ,x[<span class="hljs-number">7</span>,<span class="hljs-number">3</span>], x[<span class="hljs-number">7</span>,<span class="hljs-number">1</span>], x[<span class="hljs-number">7</span>,<span class="hljs-number">2</span>]],\<br>            [x[<span class="hljs-number">2</span>,<span class="hljs-number">0</span>], x[<span class="hljs-number">2</span>,<span class="hljs-number">3</span>], x[<span class="hljs-number">2</span>,<span class="hljs-number">1</span>], x[<span class="hljs-number">2</span>,<span class="hljs-number">2</span>]]])<br></code></pre></td></tr></table></figure><p>就是说，如果 np.xi_ 中输入两个列表，则第一个列表存的是待提取元素的行标，第二个列表存的是待提取元素的列标，第一个列表中的每个元素都会遍历第二个列表中的每个值，构成新矩阵的一行元素。</p><h3 id="广播"><a href="#广播" class="headerlink" title="广播"></a>广播</h3><p>广播(Broadcast)是 numpy 对不同形状(shape)的数组进行数值计算的方式， 对数组的算术运算通常在相应的元素上进行。</p><p>如果两个数组 a 和 b 形状相同，即满足 <strong>a.shape &#x3D;&#x3D; b.shape</strong>，那么 a*b 的结果就是 a 与 b 数组对应位相乘。这要求维数相同，且各维度的长度相同。</p><p>但是两个数组形状不同时，numpy就触发了广播机制。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>a = np.array([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],<br>              [<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>],<br>              [<span class="hljs-number">20</span>, <span class="hljs-number">20</span>, <span class="hljs-number">20</span>],<br>              [<span class="hljs-number">30</span>, <span class="hljs-number">30</span>, <span class="hljs-number">30</span>]])<br>b = np.array([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>])<br><span class="hljs-built_in">print</span>(a + b)<br></code></pre></td></tr></table></figure><p><img src="https://www.runoob.com/wp-content/uploads/2018/10/image0020619.gif" alt="img"></p><h6 id="Output-2"><a href="#Output-2" class="headerlink" title="Output"></a>Output</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">[[ <span class="hljs-number">0</span>  <span class="hljs-number">1</span>  <span class="hljs-number">2</span>]<br> [<span class="hljs-number">10</span> <span class="hljs-number">11</span> <span class="hljs-number">12</span>]<br> [<span class="hljs-number">20</span> <span class="hljs-number">21</span> <span class="hljs-number">22</span>]<br> [<span class="hljs-number">30</span> <span class="hljs-number">31</span> <span class="hljs-number">32</span>]]<br></code></pre></td></tr></table></figure><h3 id="迭代器"><a href="#迭代器" class="headerlink" title="迭代器"></a>迭代器</h3><p>NumPy 迭代器对象<code>numpy.nditer</code>提供了一种灵活访问一个或者多个数组元素的方式。</p><p><code>for x in np.nditer(a, order=&#39;F&#39;):</code>Fortran order，即是列序优先；</p><p><code>for x in np.nditer(a.T, order=&#39;C&#39;):</code>C order，即是行序优先；</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> np.nditer(a, op_flags=[<span class="hljs-string">&#x27;readwrite&#x27;</span>]): <br>    x[...]=<span class="hljs-number">2</span>*x <br></code></pre></td></tr></table></figure><p><strong>x[…]</strong> 是修改原 numpy 元素，x 只是个拷贝。</p><p>order &#x3D; ‘C’，numpy 实例（也就是一个多维数组）本身的存储顺序不会因为转置或 order &#x3D; ‘C’ 或 ‘F’ 而改变。</p><p>只是 numpy 实例中，存储了一个默认的访问顺序的字段。</p><p>numpy.copy 做了特殊处理，它拷贝的时候不是直接把对方的内存复制，而是按照上面 order 指定的顺序逐一拷贝。</p><p><strong>for x in np.nditer(a, order &#x3D; ‘C’)</strong>: 可以在循环中另外指定顺序，如果未指定，则按照上面数组的order顺序访问。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> np.nditer(a, flags = [<span class="hljs-string">&#x27;external_loop&#x27;</span>], order = <span class="hljs-string">&#x27;F&#x27;</span>): <br>    <span class="hljs-built_in">print</span> (x, end=<span class="hljs-string">&quot;, &quot;</span> )<br></code></pre></td></tr></table></figure><p>**flags &#x3D; [‘external_loop’]**，当数组的 order 与在循环中指定的 order 顺序不同时，打印为多个一维数组，当相同时，是整个一个一维数组。</p><h3 id="数组操作"><a href="#数组操作" class="headerlink" title="数组操作"></a>数组操作</h3><p>这一部分基本上一些方法，这里只对方法的函数名和描述给出。</p><p><strong>修改数组形状</strong></p><table><thead><tr><th align="left">函数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left"><code>reshape</code></td><td align="left">不改变数据的条件下修改形状 numpy.reshape(arr, newshape, order&#x3D;’C’)</td></tr><tr><td align="left"><code>flat</code></td><td align="left">数组元素迭代器</td></tr><tr><td align="left"><code>flatten</code></td><td align="left">返回一份数组拷贝，对拷贝所做的修改不会影响原始数组 ndarray.flatten(order&#x3D;’C’)</td></tr><tr><td align="left"><code>ravel</code></td><td align="left">返回展开数组 numpy.ravel(a, order&#x3D;’C’)</td></tr></tbody></table><p><strong>翻转数组</strong></p><table><thead><tr><th align="left">函数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left"><code>transpose</code></td><td align="left">对换数组的维度 numpy.transpose(arr, axes)</td></tr><tr><td align="left"><code>ndarray.T</code></td><td align="left">和 <code>self.transpose()</code> 相同</td></tr><tr><td align="left"><code>rollaxis</code></td><td align="left">向后滚动指定的轴 numpy.rollaxis(arr, axis, start)</td></tr><tr><td align="left"><code>swapaxes</code></td><td align="left">对换数组的两个轴 numpy.swapaxes(arr, axis1, axis2)</td></tr></tbody></table><p><strong>修改数组维度</strong></p><table><thead><tr><th align="left">维度</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left"><code>broadcast</code></td><td align="left">产生模仿广播的对象</td></tr><tr><td align="left"><code>broadcast_to</code></td><td align="left">将数组广播到新形状 numpy.broadcast_to(array, shape, subok)</td></tr><tr><td align="left"><code>expand_dims</code></td><td align="left">扩展数组的形状  numpy.expand_dims(arr, axis)</td></tr><tr><td align="left"><code>squeeze</code></td><td align="left">从数组的形状中删除一维条目 numpy.squeeze(arr, axis)</td></tr></tbody></table><p><strong>连接数组</strong></p><table><thead><tr><th align="left">函数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left"><code>concatenate</code></td><td align="left">连接沿现有轴的数组序列 numpy.concatenate((a1, a2, …), axis)</td></tr><tr><td align="left"><code>stack</code></td><td align="left">沿着新的轴加入一系列数组。 numpy.stack(arrays, axis)</td></tr><tr><td align="left"><code>hstack</code></td><td align="left">水平堆叠序列中的数组（列方向）</td></tr><tr><td align="left"><code>vstack</code></td><td align="left">竖直堆叠序列中的数组（行方向）</td></tr></tbody></table><p><strong>分割数组</strong></p><table><thead><tr><th align="left">函数</th><th align="left">数组及操作</th></tr></thead><tbody><tr><td align="left"><code>split</code></td><td align="left">将一个数组分割为多个子数组 numpy.split(ary, indices_or_sections, axis)</td></tr><tr><td align="left"><code>hsplit</code></td><td align="left">将一个数组水平分割为多个子数组（按列）</td></tr><tr><td align="left"><code>vsplit</code></td><td align="left">将一个数组垂直分割为多个子数组（按行）</td></tr></tbody></table><p><strong>数组元素的添加与删除</strong></p><table><thead><tr><th align="left">函数</th><th align="left">元素及描述</th></tr></thead><tbody><tr><td align="left"><code>resize</code></td><td align="left">返回指定形状的新数组 numpy.resize(arr, shape)</td></tr><tr><td align="left"><code>append</code></td><td align="left">将值添加到数组末尾 numpy.append(arr, values, axis&#x3D;None)</td></tr><tr><td align="left"><code>insert</code></td><td align="left">沿指定轴将值插入到指定下标之前 numpy.insert(arr, obj, values, axis)</td></tr><tr><td align="left"><code>delete</code></td><td align="left">删掉某个轴的子数组，并返回删除后的新数组 Numpy.delete(arr, obj, axis)</td></tr><tr><td align="left"><code>unique</code></td><td align="left">查找数组内的唯一元素 numpy.unique(arr, return_index, return_inverse, return_counts)</td></tr></tbody></table><h3 id="位运算"><a href="#位运算" class="headerlink" title="位运算"></a>位运算</h3><p>Numpy也是内置位运算函数的，我认为这部分了解即可</p><p><a href="https://www.runoob.com/numpy/numpy-binary-operators.html">菜鸟教程-NumPy 位运算</a></p><p>NumPy <strong>“bitwise_”</strong> 开头的函数是位运算函数。</p><p>NumPy 位运算包括以下几个函数：</p><table><thead><tr><th align="left">函数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left"><code>bitwise_and</code></td><td align="left">对数组元素执行位与操作</td></tr><tr><td align="left"><code>bitwise_or</code></td><td align="left">对数组元素执行位或操作</td></tr><tr><td align="left"><code>invert</code></td><td align="left">按位取反</td></tr><tr><td align="left"><code>left_shift</code></td><td align="left">向左移动二进制表示的位</td></tr><tr><td align="left"><code>right_shift</code></td><td align="left">向右移动二进制表示的位</td></tr></tbody></table><p><strong>注：</strong>也可以使用 “&amp;”、 “~”、 “|” 和 “^” 等操作符进行计算。</p><h3 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h3><p>Numpy的字符串函数是基于Python内置库中的标准字符串函数。</p><table><thead><tr><th align="left">函数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left"><code>add()</code></td><td align="left">对两个数组的逐个字符串元素进行连接</td></tr><tr><td align="left"><code>multiply()</code></td><td align="left">返回按元素多重连接后的字符串</td></tr><tr><td align="left"><code>center()</code></td><td align="left">居中字符串</td></tr><tr><td align="left"><code>capitalize()</code></td><td align="left">将字符串第一个字母转换为大写</td></tr><tr><td align="left"><code>title()</code></td><td align="left">将字符串的每个单词的第一个字母转换为大写</td></tr><tr><td align="left"><code>lower()</code></td><td align="left">数组元素转换为小写</td></tr><tr><td align="left"><code>upper()</code></td><td align="left">数组元素转换为大写</td></tr><tr><td align="left"><code>split()</code></td><td align="left">指定分隔符对字符串进行分割，并返回数组列表</td></tr><tr><td align="left"><code>splitlines()</code></td><td align="left">返回元素中的行列表，以换行符分割</td></tr><tr><td align="left"><code>strip()</code></td><td align="left">移除元素开头或者结尾处的特定字符</td></tr><tr><td align="left"><code>join()</code></td><td align="left">通过指定分隔符来连接数组中的元素</td></tr><tr><td align="left"><code>replace()</code></td><td align="left">使用新字符串替换字符串中的所有子字符串</td></tr><tr><td align="left"><code>decode()</code></td><td align="left">数组元素依次调用<code>str.decode</code></td></tr><tr><td align="left"><code>encode()</code></td><td align="left">数组元素依次调用<code>str.encode</code></td></tr></tbody></table><h3 id="数学"><a href="#数学" class="headerlink" title="数学"></a>数学</h3><p>提供了标准的三角函数：**sin()、cos()、tan()**。</p><p><strong>arcsin，arccos，和 arctan</strong> 函数返回给定角度的 sin，cos 和 tan 的反三角函数。</p><p>这些函数的结果可以通过<code>numpy.degrees()</code>函数将弧度转换为角度。</p><p><code>numpy.around()</code> 函数返回指定数字的四舍五入值。<br><code>numpy.around(a,decimals)</code> decimals: 舍入的小数位数。 默认值为0。 如果为负，整数将四舍五入到小数点左侧的位置</p><p><code>numpy.floor()</code> 返回小于或者等于指定表达式的最大整数，即向下取整。</p><p><code>numpy.ceil()</code> 返回大于或者等于指定表达式的最小整数，即向上取整。</p><p>NumPy 算术函数包含简单的加减乘除: <strong>add()<strong>，</strong>subtract()<strong>，</strong>multiply()</strong> 和 **divide()**。</p><p><code>numpy.reciprocal() </code>函数返回参数逐元素的<strong>倒数</strong>。如 <strong>1&#x2F;4</strong> 倒数为 <strong>4&#x2F;1</strong>。</p><p><code>numpy.power() </code>函数将第一个输入数组中的元素作为底数，计算它与第二个输入数组中相应元素的幂。</p><p><code>numpy.mod() </code>计算输入数组中相应元素的相除后的余数。函数<code>numpy.remainder()</code>也产生相同的结果。</p><h3 id="统计学"><a href="#统计学" class="headerlink" title="统计学"></a>统计学</h3><p>NumPy 提供了很多统计函数，用于从数组中查找最小元素，最大元素，百分位标准差和方差等。</p><p>这些统计学函数通常带有较多的传入参数，详见<a href="https://www.runoob.com/numpy/numpy-statistical-functions.html">统计学函数</a></p><p><code>numpy.amin()</code> 用于计算数组中的元素沿指定轴的最小值。</p><p><code>numpy.amax() </code>用于计算数组中的元素沿指定轴的最大值。</p><p><code>numpy.ptp()</code>函数计算数组中元素最大值与最小值的差（最大值 - 最小值）。</p><p><code>numpy.percentile()</code>百分位数是统计中使用的度量，表示小于这个值的观察值的百分比。 </p><p><code>numpy.median() </code>函数用于计算数组 a 中元素的中位数（中值）</p><p><code>numpy.mean()</code> 函数返回数组中元素的算术平均值，如果提供了轴，则沿其计算。</p><p><code>numpy.average() </code>函数根据在另一个数组中给出的各自的权重计算数组中元素的加权平均值。</p><p>标准差是一组数据平均值分散程度的一种度量。标准差是方差的算术平方根。</p><p>标准差公式如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">std = sqrt(mean((x - x.mean())**<span class="hljs-number">2</span>))<br><span class="hljs-comment">#使用例：</span><br><span class="hljs-built_in">print</span> (np.std([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]))<br>&gt;&gt; <span class="hljs-number">1.1180339887498949</span><br></code></pre></td></tr></table></figure><p>统计中的方差（样本方差）是每个样本值与全体样本值的平均数之差的平方值的平均数，</p><p>即 <code>mean((x - x.mean())** 2)</code></p><p>换句话说，标准差是方差的平方根。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#使用例：</span><br><span class="hljs-built_in">print</span> (np.var([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]))<br>&gt;&gt; <span class="hljs-number">1.25</span><br></code></pre></td></tr></table></figure><h3 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h3><table><thead><tr><th align="left">种类</th><th align="left">速度</th><th align="left">最坏情况</th></tr></thead><tbody><tr><td align="left"><code>quicksort</code>（快速排序）</td><td align="left">1</td><td align="left"><code>O(n^2)</code></td></tr><tr><td align="left"><code>mergesort</code>（归并排序）</td><td align="left">2</td><td align="left"><code>O(n*log(n))</code></td></tr><tr><td align="left"><code>heapsort</code>（堆排序）</td><td align="left">3</td><td align="left"><code>O(n*log(n))</code></td></tr></tbody></table><p><code>numpy.sort() </code>函数返回输入数组的排序副本，numpy中还能以字段关键字排序。</p><p>倒序使用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">x = <span class="hljs-built_in">abs</span>(np.sort(-x)) <br></code></pre></td></tr></table></figure><p><code>numpy.argsort() </code>函数返回的是数组值从小到大的索引值。</p><p><code>numpy.lexsort() </code>用于对多个序列进行排序。把它想象成对电子表格进行排序，每一列代表一个序列，排序时优先照顾靠后的列。</p><table><thead><tr><th align="left">函数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left"><code>msort(a)</code></td><td align="left">数组按第一个轴排序，返回排序后的数组副本。np.msort(a) 相等于 np.sort(a, axis&#x3D;0)。</td></tr><tr><td align="left"><code>sort_complex(a)</code></td><td align="left">对复数按照先实部后虚部的顺序进行排序。</td></tr><tr><td align="left"><code>partition(a, kth[, axis, kind, order])</code></td><td align="left">指定一个数，对数组进行分区</td></tr><tr><td align="left"><code>argpartition(a, kth[, axis, kind, order])</code></td><td align="left">可以通过关键字 kind 指定算法沿着指定轴对数组进行分区</td></tr></tbody></table><p><code>numpy.argmax()</code> 和 <code>numpy.argmin()</code>函数分别沿给定轴返回最大和最小元素的索引。</p><p><code>numpy.nonzero() </code>函数返回输入数组中非零元素的索引。</p><p><code>numpy.where() </code>函数返回输入数组中满足给定条件的元素的索引。</p><p><code>numpy.extract()</code>函数根据某个条件从数组中抽取元素，返回满条件的元素。</p><h3 id="字节交换"><a href="#字节交换" class="headerlink" title="字节交换"></a>字节交换</h3><ul><li><strong>大端模式：</strong>指数据的高字节保存在内存的低地址中，而数据的低字节保存在内存的高地址中，这样的存储模式有点儿类似于把数据当作字符串顺序处理：地址由小向大增加，而数据从高位往低位放；这和我们的阅读习惯一致。</li><li><strong>小端模式：</strong>指数据的高字节保存在内存的高地址中，而数据的低字节保存在内存的低地址中，这种存储模式将地址的高低和数据位权有效地结合起来，高地址部分权值高，低地址部分权值低。</li></ul><p><code>numpy.ndarray.byteswap() </code>函数将 ndarray 中每个元素中的字节进行大小端转换。</p><p>(我并不知道这样做这有什么用)</p><h3 id="副本和视图"><a href="#副本和视图" class="headerlink" title="副本和视图"></a>副本和视图</h3><p>和数据库语言的副本、视图类似。</p><p>视图或浅拷贝：<code>ndarray.view() </code>方会创建一个新的数组对象，该方法创建的新数组的维数变化不会改变原始数据的维数。</p><p>副本或深拷贝：<code>ndarray.copy() </code>函数创建一个副本。 对副本数据进行修改，不会影响到原始数据，它们物理内存不在同一位置。</p><h3 id="矩阵（matrix）与线性代数"><a href="#矩阵（matrix）与线性代数" class="headerlink" title="矩阵（matrix）与线性代数"></a>矩阵（matrix）与线性代数</h3><p>一个 m * n 的矩阵</p><p>转置： numpy.transpose 函数来对换数组的维度，还可以使用 <strong>T</strong> 属性。例如有个 m 行 n 列的矩阵，使用 t() 函数就能转换为 n 行 m 列的矩阵。</p><p><code>matlib.empty() </code>函数返回一个新的矩阵。</p><p><code>numpy.matlib.zeros() </code>函数创建一个以 0 填充的矩阵。</p><p><code>numpy.matlib.ones()</code>函数创建一个以 1 填充的矩阵。</p><p><code>numpy.matlib.eye()</code> 函数返回一个矩阵，对角线元素为 1，其他位置为零。</p><p><code>numpy.matlib.identity() </code>函数返回给定大小的单位矩阵。</p><p><code>numpy.matlib.rand() </code>函数创建一个给定大小的矩阵，数据是随机填充的。</p><p><strong>线性代数</strong>函数库 <strong>linalg</strong>，该库包含了线性代数所需的所有功能</p><table><thead><tr><th align="left">函数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left"><code>dot</code></td><td align="left">两个数组的点积，即元素对应相乘。numpy.dot(a, b, out&#x3D;None)</td></tr><tr><td align="left"><code>vdot</code></td><td align="left">两个向量的点积</td></tr><tr><td align="left"><code>inner</code></td><td align="left">两个数组的内积</td></tr><tr><td align="left"><code>matmul</code></td><td align="left">两个数组的矩阵积</td></tr><tr><td align="left"><code>determinant</code></td><td align="left">数组的行列式</td></tr><tr><td align="left"><code>solve</code></td><td align="left">求解线性矩阵方程</td></tr><tr><td align="left"><code>inv</code></td><td align="left">计算矩阵的乘法逆矩阵</td></tr></tbody></table><p><code>numpy.linalg.det() </code>函数计算输入矩阵的行列式。</p><p><code>numpy.linalg.solve() </code>函数给出了矩阵形式的线性方程的解。</p><p><a href="https://www.runoob.com/numpy/numpy-linear-algebra.html">详见</a></p><h3 id="IO"><a href="#IO" class="headerlink" title="IO"></a>IO</h3><p>NumPy 为 ndarray 对象引入了一个简单的文件格式：<strong>npy</strong>。</p><p>npy 文件用于存储重建 ndarray 所需的数据、图形、dtype 和其他信息。</p><p><code>numpy.save() </code>函数将数组保存到以 .npy 为扩展名的文件中。</p><p><code>numpy.savez() </code>函数将多个数组保存到以 npz 为扩展名的文件中。</p><p><code>savetxt()</code> 函数是以简单的文本文件格式存储数据，对应的使用<code> loadtxt()</code> 函数来获取数据。</p><h3 id="Matplotlib"><a href="#Matplotlib" class="headerlink" title="Matplotlib"></a>Matplotlib</h3><p>Matplotlib 是 Python 的绘图库。 它可与 NumPy 一起使用，提供了一种有效的 MatLab 开源替代方案。 它也可以和图形工具包一起使用，如 PyQt 和 wxPython。</p><p><a href="https://search.bilibili.com/all?keyword=Matplotlib&from_source=webtop_search&spm_id_from=333.1007&search_source=5">详见</a></p><p>UPDATE TIME ： </p><p>2023年9月12日星期二</p>]]></content>
    
    
    
    <tags>
      
      <tag>Numpy</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>1-基础部分-2-数学基础</title>
    <link href="/2023/09/08/1-%E5%9F%BA%E7%A1%80%E9%83%A8%E5%88%86-2-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/"/>
    <url>/2023/09/08/1-%E5%9F%BA%E7%A1%80%E9%83%A8%E5%88%86-2-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/</url>
    
    <content type="html"><![CDATA[<h2 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h2><h3 id="最优化问题"><a href="#最优化问题" class="headerlink" title="最优化问题"></a>最优化问题</h3><p>目标函数Error:<br>$$<br>E(\theta)&#x3D;\frac{1}{2} \sum_{i&#x3D;1}^n(y_i-f_\theta(x_i))^2<br>$$<br>i是指第 i 个训练数据.对每个训练数据的误差取平方之后，全部相加，然后乘以0.5 。这么做是为了找到使 E(θ) 的值最小的 θ。这样的问题称为最优化问题。</p><h3 id="最速下降法："><a href="#最速下降法：" class="headerlink" title="最速下降法："></a>最速下降法：</h3><p>$$<br>\theta_0 :&#x3D;\theta_0 - \eta\sum_{i&#x3D;1}^n(f_\theta(x_i)-y_i)<br>$$</p><p>$$<br>\theta_1 :&#x3D;\theta_1 - \eta\sum_{i&#x3D;1}^n(f_\theta(x_i)-y_i) x_i<br>$$</p><h3 id="多项式回归："><a href="#多项式回归：" class="headerlink" title="多项式回归："></a>多项式回归：</h3>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>1-基础部分-1-Python</title>
    <link href="/2023/09/08/1-%E5%9F%BA%E7%A1%80%E9%83%A8%E5%88%86-1-Python/"/>
    <url>/2023/09/08/1-%E5%9F%BA%E7%A1%80%E9%83%A8%E5%88%86-1-Python/</url>
    
    <content type="html"><![CDATA[<h1>Chapter 1</h1><h2 id="1-1-分解序列"><a href="#1-1-分解序列" class="headerlink" title="1.1 分解序列"></a>1.1 分解序列</h2><p>需要元素数量匹配，除了元组和列表，其余可迭代也可执行。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">data = [<span class="hljs-string">&#x27;ACME&#x27;</span>, <span class="hljs-number">50</span>, <span class="hljs-number">91.9</span>, (<span class="hljs-number">2023</span>, <span class="hljs-number">9</span>, <span class="hljs-number">9</span>)]<br>name, shares, price, data = data<br><br><span class="hljs-built_in">print</span>(name, data)<br><br></code></pre></td></tr></table></figure><h6 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">ACME (<span class="hljs-number">2023</span>, <span class="hljs-number">9</span>, <span class="hljs-number">9</span>)<br></code></pre></td></tr></table></figure><h2 id="1-2-从可迭代对象中分解元素"><a href="#1-2-从可迭代对象中分解元素" class="headerlink" title="1.2 从可迭代对象中分解元素"></a>1.2 从可迭代对象中分解元素</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">avg</span>(<span class="hljs-params">nums</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">sum</span>(nums)/<span class="hljs-built_in">len</span>(nums)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">drop_fst_and_lst</span>(<span class="hljs-params">self</span>):<br>    fst, *mid, lst = self<br>    <span class="hljs-keyword">return</span> avg(mid)<br><br><br>grades = [<span class="hljs-number">100</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">0</span>]<br><span class="hljs-built_in">print</span>(drop_fst_and_lst(grades))<br></code></pre></td></tr></table></figure><h6 id="Output-1"><a href="#Output-1" class="headerlink" title="Output"></a>Output</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-number">10.0</span><br></code></pre></td></tr></table></figure><p>Tips:</p><p>将函数传入值改为self可避免暴露函数内的变量名。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs stylus">line = <span class="hljs-string">&#x27;gting:*:-2:-2:Unprivileged User:/var/empty:/usr/bin/false&#x27;</span><br><br>unmae, *fields, homedir, sh = line<span class="hljs-selector-class">.split</span>(<span class="hljs-string">&#x27;:&#x27;</span>)<br><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(unmae)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(homedir)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(sh)</span></span><br></code></pre></td></tr></table></figure><h6 id="Output-2"><a href="#Output-2" class="headerlink" title="Output"></a>Output</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">gting<br>/var/empty<br>/usr/<span class="hljs-built_in">bin</span>/false<br></code></pre></td></tr></table></figure><h2 id="1-3-双向队列"><a href="#1-3-双向队列" class="headerlink" title="1.3 双向队列"></a>1.3 双向队列</h2><p>初始化：<code>q = deque(maxlen=?)</code></p><p><code>q.append(?)</code> 右侧插入元素</p><p><code>q.appendleft(?)</code> 左侧插入元素</p><p><code>q.pop()</code> 弹出右侧元素</p><p><code>q.popleft()</code> 弹出左侧元素</p><p>如果不指定队列的大小就是一个无限的队列，可在两段进行插入和弹出，并且都是O(1)，而列表是O(n)</p><h2 id="1-4-堆heapq"><a href="#1-4-堆heapq" class="headerlink" title="1.4 堆heapq"></a>1.4 堆<code>heapq</code></h2><p>找到最大或者最小的N个元素。</p><p><code>imort heapq</code></p><p><code>heapq</code>中有两个函数 <code>nlargest()</code> 和 <code>nsmallest()</code> </p><p><code>heapq.nlargest(?, ?list, key)</code> 取出最大的前三项，最小同理。</p><p><code>heapq.heapify(?list)</code>将list排序为小顶堆</p><p><code>heapq.heappop()</code> 获取弹出对顶元素，O(logn)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> heapq<br><br>portfolio = [<br>   &#123;<span class="hljs-string">&#x27;name&#x27;</span>: <span class="hljs-string">&#x27;IBM&#x27;</span>, <span class="hljs-string">&#x27;shares&#x27;</span>: <span class="hljs-number">100</span>, <span class="hljs-string">&#x27;price&#x27;</span>: <span class="hljs-number">91.1</span>&#125;,<br>   &#123;<span class="hljs-string">&#x27;name&#x27;</span>: <span class="hljs-string">&#x27;AAPL&#x27;</span>, <span class="hljs-string">&#x27;shares&#x27;</span>: <span class="hljs-number">50</span>, <span class="hljs-string">&#x27;price&#x27;</span>: <span class="hljs-number">543.22</span>&#125;,<br>   &#123;<span class="hljs-string">&#x27;name&#x27;</span>: <span class="hljs-string">&#x27;FB&#x27;</span>, <span class="hljs-string">&#x27;shares&#x27;</span>: <span class="hljs-number">200</span>, <span class="hljs-string">&#x27;price&#x27;</span>: <span class="hljs-number">21.09</span>&#125;,<br>   &#123;<span class="hljs-string">&#x27;name&#x27;</span>: <span class="hljs-string">&#x27;HPQ&#x27;</span>, <span class="hljs-string">&#x27;shares&#x27;</span>: <span class="hljs-number">35</span>, <span class="hljs-string">&#x27;price&#x27;</span>: <span class="hljs-number">31.75</span>&#125;,<br>   &#123;<span class="hljs-string">&#x27;name&#x27;</span>: <span class="hljs-string">&#x27;YHOO&#x27;</span>, <span class="hljs-string">&#x27;shares&#x27;</span>: <span class="hljs-number">45</span>, <span class="hljs-string">&#x27;price&#x27;</span>: <span class="hljs-number">16.35</span>&#125;,<br>   &#123;<span class="hljs-string">&#x27;name&#x27;</span>: <span class="hljs-string">&#x27;ACME&#x27;</span>, <span class="hljs-string">&#x27;shares&#x27;</span>: <span class="hljs-number">75</span>, <span class="hljs-string">&#x27;price&#x27;</span>: <span class="hljs-number">115.65</span>&#125;<br>]<br><br>cheap = heapq.nsmallest(<span class="hljs-number">3</span>, portfolio, key=<span class="hljs-keyword">lambda</span> s: s[<span class="hljs-string">&#x27;price&#x27;</span>])<br>expensive = heapq.nlargest(<span class="hljs-number">3</span>, portfolio, key=<span class="hljs-keyword">lambda</span> s: s[<span class="hljs-string">&#x27;price&#x27;</span>])<br><br><span class="hljs-built_in">print</span>(cheap)<br><span class="hljs-built_in">print</span>(expensive)<br></code></pre></td></tr></table></figure><h6 id="Output-3"><a href="#Output-3" class="headerlink" title="Output"></a>Output</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">[&#123;<span class="hljs-string">&#x27;name&#x27;</span>: <span class="hljs-string">&#x27;YHOO&#x27;</span>, <span class="hljs-string">&#x27;shares&#x27;</span>: <span class="hljs-number">45</span>, <span class="hljs-string">&#x27;price&#x27;</span>: <span class="hljs-number">16.35</span>&#125;, &#123;<span class="hljs-string">&#x27;name&#x27;</span>: <span class="hljs-string">&#x27;FB&#x27;</span>, <span class="hljs-string">&#x27;shares&#x27;</span>: <span class="hljs-number">200</span>, <span class="hljs-string">&#x27;price&#x27;</span>: <span class="hljs-number">21.09</span>&#125;, &#123;<span class="hljs-string">&#x27;name&#x27;</span>: <span class="hljs-string">&#x27;HPQ&#x27;</span>, <span class="hljs-string">&#x27;shares&#x27;</span>: <span class="hljs-number">35</span>, <span class="hljs-string">&#x27;price&#x27;</span>: <span class="hljs-number">31.75</span>&#125;]<br>[&#123;<span class="hljs-string">&#x27;name&#x27;</span>: <span class="hljs-string">&#x27;AAPL&#x27;</span>, <span class="hljs-string">&#x27;shares&#x27;</span>: <span class="hljs-number">50</span>, <span class="hljs-string">&#x27;price&#x27;</span>: <span class="hljs-number">543.22</span>&#125;, &#123;<span class="hljs-string">&#x27;name&#x27;</span>: <span class="hljs-string">&#x27;ACME&#x27;</span>, <span class="hljs-string">&#x27;shares&#x27;</span>: <span class="hljs-number">75</span>, <span class="hljs-string">&#x27;price&#x27;</span>: <span class="hljs-number">115.65</span>&#125;, &#123;<span class="hljs-string">&#x27;name&#x27;</span>: <span class="hljs-string">&#x27;IBM&#x27;</span>, <span class="hljs-string">&#x27;shares&#x27;</span>: <span class="hljs-number">100</span>, <span class="hljs-string">&#x27;price&#x27;</span>: <span class="hljs-number">91.1</span>&#125;]<br></code></pre></td></tr></table></figure><h2 id="1-5-优先队列"><a href="#1-5-优先队列" class="headerlink" title="1.5 优先队列"></a>1.5 优先队列</h2><p>使用 <code>heap</code>实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># example.py</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># Example of a priority queue</span><br><br><span class="hljs-keyword">import</span> heapq<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">PriorityQueue</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        self._queue = []<br>        self._index = <span class="hljs-number">0</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">push</span>(<span class="hljs-params">self, item, priority</span>):<br>        heapq.heappush(self._queue, (-priority, self._index, item))<br>        self._index += <span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">pop</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> heapq.heappop(self._queue)[-<span class="hljs-number">1</span>]<br><br><br><span class="hljs-comment"># Example use</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Item</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, name</span>):<br>        self.name = name<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__repr__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&#x27;Item(&#123;!r&#125;)&#x27;</span>.<span class="hljs-built_in">format</span>(self.name)<br><br><br>q = PriorityQueue()<br>q.push(Item(<span class="hljs-string">&#x27;foo&#x27;</span>), <span class="hljs-number">1</span>)<br>q.push(Item(<span class="hljs-string">&#x27;bar&#x27;</span>), <span class="hljs-number">5</span>)<br>q.push(Item(<span class="hljs-string">&#x27;spam&#x27;</span>), <span class="hljs-number">4</span>)<br>q.push(Item(<span class="hljs-string">&#x27;grok&#x27;</span>), <span class="hljs-number">1</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Should be bar:&quot;</span>, q.pop())<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Should be spam:&quot;</span>, q.pop())<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Should be foo:&quot;</span>, q.pop())<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Should be grok:&quot;</span>, q.pop())<br></code></pre></td></tr></table></figure><h6 id="Output-4"><a href="#Output-4" class="headerlink" title="Output"></a>Output</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">Should be bar: Item(<span class="hljs-string">&#x27;bar&#x27;</span>)<br>Should be spam: Item(<span class="hljs-string">&#x27;spam&#x27;</span>)<br>Should be foo: Item(<span class="hljs-string">&#x27;foo&#x27;</span>)<br>Should be grok: Item(<span class="hljs-string">&#x27;grok&#x27;</span>)<br></code></pre></td></tr></table></figure><h2 id="1-6-一键多值字典multdct"><a href="#1-6-一键多值字典multdct" class="headerlink" title="1.6 一键多值字典multdct"></a>1.6 一键多值字典<code>multdct</code></h2><p>使用<code>from collections import defaultdict</code></p><p>使用例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">d = defaultdict(<span class="hljs-built_in">list</span>)<br>d[<span class="hljs-string">&#x27;a&#x27;</span>].append(<span class="hljs-number">1</span>)<br>d[<span class="hljs-string">&#x27;a&#x27;</span>].append(<span class="hljs-number">2</span>)<br><br>d2 = defaultdict(<span class="hljs-built_in">set</span>)<br>d2[<span class="hljs-string">&#x27;a&#x27;</span>].add(<span class="hljs-number">1</span>)<br>d2[<span class="hljs-string">&#x27;a&#x27;</span>].add(<span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><h2 id="1-7-有序字典"><a href="#1-7-有序字典" class="headerlink" title="1.7 有序字典"></a>1.7 有序字典</h2><p>使用<code>from collections import OrderedDict</code> 会严格按照字典添加的顺序进行。</p><p>可在JSON编码中控制各字段的顺序。</p><p>1.8 字典中的计算</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习入门</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3-机器学习-1-理论</title>
    <link href="/2023/09/08/3-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-1-%E7%90%86%E8%AE%BA/"/>
    <url>/2023/09/08/3-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-1-%E7%90%86%E8%AE%BA/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Linux下的锐捷认证</title>
    <link href="/2023/09/08/test1/"/>
    <url>/2023/09/08/test1/</url>
    
    <content type="html"><![CDATA[<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">ping baidu.com -n 100|foreach -<span class="hljs-string">&quot; -f (Get-Date),<span class="hljs-variable">$_</span>&#125;</span><br><span class="hljs-string">#显示时间的PING指令</span><br></code></pre></td></tr></table></figure><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>第一次使用校园网，才知道校园网原来需要登录认证①，并且限制设备②，在没有办理校内校园流量卡的情况下，原来的流量卡只有100kb&#x2F;s速度，已经严重影响使用，另外校园全覆盖WiFi，同样需要登录认证，速度大概在2mb&#x2F;s，但是无线传输的弊端就是不稳定，间断性断网已经成为常态。</p><p>①：wifi使用学号+密码网页认证登录，有线网需要锐捷客户端v6.84多运营商版本。</p><p>②：wifi+有线，同一帐号仅能支持两个设备登录，这对于联网终端多的人是无法使用的。</p><p>另外，最重要的是常用的编码环境Linux，官方并没有提供对应的linux的认证客户端，所以这也使linux设备的有线网络连接设下了障碍。</p><p>通过网络上前人的研究了解到，使用mentohust或者其迭代版本minieap可以实现linux系统认证通过锐捷，甚至可以将其交叉编译到软路由上；</p><p>那么，理论可行，实践开始。</p><h2 id="1-mentohust"><a href="#1-mentohust" class="headerlink" title="1.mentohust"></a>1.mentohust</h2><p>首先做提前准备，准备mentohust必须的文件。</p><p>gh源码：<code>gh repo clone hyrathb/mentohust</code></p><p>3个客户端源文件和1个mpf抓包</p><p>W32N55.dll0821x.exeSuConfig.datmentohust.mpf</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo <span class="hljs-built_in">mkdir</span> /etc/mentohust<br>sudo <span class="hljs-built_in">cp</span> ./8021x.exe  /etc/mentohust<br>sudo <span class="hljs-built_in">cp</span> ./W32N55.dll /etc/mentohust<br>sudo <span class="hljs-built_in">cp</span> ./SuConfig.dat /etc/mentohust<br>sudo <span class="hljs-built_in">cp</span> ./mentohust.mpf /etc/mentohust<br></code></pre></td></tr></table></figure><p>mpf文件使用MentoHUSTTool进行获取</p><p>可能因为该项目过于久远，最后没能成功认证，换为minieap</p><h2 id="2-minieap"><a href="#2-minieap" class="headerlink" title="2.minieap"></a>2.minieap</h2><p>其原理和mentohust类似，不过需要手动编译两个文件——minieap和libpcap</p><p><a href="https://github.com/updateing/minieap">Minieap</a></p><p>可能由于多运营商的问题，无法获取运营商的id，这个版本的minieap也没能通过认证，所以让linux连接有线网络以失败告终。</p><h2>更变思路</h2><p>一次偶然的机会，联系上了一位学长，他给出了重要的点拨提示。为什么要仿造锐捷进行认证呢？直接使用路由器仿造电脑不就行了。</p><p>至此锐捷认证问题成功解决，使用上了路由器上网。</p><p>不过我一直怀疑寝室里的网存在环路，导致广播风暴时常发生，只能期待校方有朝一日能够解决这个严重的问题。</p>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2023/09/05/hello-world/"/>
    <url>/2023/09/05/hello-world/</url>
    
    <content type="html"><![CDATA[<h3 id="分类文章-文章标签"><a href="#分类文章-文章标签" class="headerlink" title="分类文章\文章标签"></a>分类文章\文章标签</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-string">并列分类，了解一下：</span><br><span class="hljs-attr">categories:</span><br><span class="hljs-bullet">-</span> [<span class="hljs-string">Linux</span>]<br><span class="hljs-bullet">-</span> [<span class="hljs-string">Tools</span>]<br><br><span class="hljs-string">并列+子分类，再了解一下：</span><br><span class="hljs-attr">categories:</span><br><span class="hljs-bullet">-</span> [<span class="hljs-string">Linux</span>, <span class="hljs-string">Hexo</span>]<br><span class="hljs-bullet">-</span> [<span class="hljs-string">Tools</span>, <span class="hljs-string">PHP</span>]<br><br><span class="hljs-attr">categories:</span><br><span class="hljs-bullet">-</span> <span class="hljs-string">Diary</span><br><span class="hljs-attr">tags:</span><br><span class="hljs-bullet">-</span> <span class="hljs-string">PS3</span><br><span class="hljs-bullet">-</span> <span class="hljs-string">Games</span><br></code></pre></td></tr></table></figure><h3 id="归档文章"><a href="#归档文章" class="headerlink" title="归档文章"></a>归档文章</h3><p>如果只是想让文章在首页隐藏，但仍<strong>需要在归档分类页里展示</strong>，可以在文章开头 <a href="https://hexo.io/zh-cn/docs/front-matter">front-matter (opens new window)</a>中配置 <code>archive: true</code> 属性。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">文章标题</span><br><span class="hljs-attr">index_img:</span> <span class="hljs-string">/img/example.jpg</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2019-10-10 10:00:00</span><br><span class="hljs-attr">archive:</span> <span class="hljs-literal">true</span><br><span class="hljs-meta">---</span><br><span class="hljs-string">以下是文章内容</span><br></code></pre></td></tr></table></figure><h3 id="文章排序"><a href="#文章排序" class="headerlink" title="文章排序"></a>文章排序</h3><p>如果想手动将某些文章固定在首页靠前的位置，可以在安装 <code>hexo-generator-index</code> &gt;&#x3D; 2.0.0 版本的情况下，在文章开头 <a href="https://hexo.io/zh-cn/docs/front-matter">front-matter (opens new window)</a>中配置 <code>sticky</code> 属性：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">文章标题</span><br><span class="hljs-attr">index_img:</span> <span class="hljs-string">/img/example.jpg</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2019-10-10 10:00:00</span><br><span class="hljs-attr">sticky:</span> <span class="hljs-number">100</span><br><span class="hljs-meta">---</span><br><span class="hljs-string">以下是文章内容</span><br></code></pre></td></tr></table></figure><p><code>sticky</code> 数值越大，该文章越靠前，达到类似于置顶的效果，其他未设置的文章依然按默认排序。</p><h3 id="文章在首页的封面图"><a href="#文章在首页的封面图" class="headerlink" title="文章在首页的封面图"></a>文章在首页的封面图</h3><p>对于单篇文章，在文章开头 <a href="https://hexo.io/zh-cn/docs/front-matter">front-matter (opens new window)</a>中配置 <code>index_img</code> 属性。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">文章标题</span><br><span class="hljs-attr">tags:</span> [<span class="hljs-string">Hexo</span>, <span class="hljs-string">Fluid</span>]<br><span class="hljs-attr">index_img:</span> <span class="hljs-string">/img/example.jpg</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2019-10-10 10:00:00</span><br><span class="hljs-meta">---</span><br><span class="hljs-string">以下是文章内容</span><br></code></pre></td></tr></table></figure><p>和 Banner 配置相同，<code>/img/example.jpg</code> 对应的是存放在 <code>/source/img/example.jpg</code> 目录下的图片（目录也可自定义，但必须在 source 目录下）。</p><p>也可以使用外链 Url 的绝对路径。</p><p>如果想统一给文章设置一个默认图片（文章不设置 <code>index_img</code> 则默认使用这张图片），可在<strong>主题配置</strong>中设置：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">post:</span><br>  <span class="hljs-attr">default_index_img:</span> <span class="hljs-string">/img/example.jpg</span><br></code></pre></td></tr></table></figure><p>当 <code>default_index_img</code> 和 <code>index_img</code> 都为空时，该文章在首页将不显示图片。</p><h3 id="文章页顶部大图"><a href="#文章页顶部大图" class="headerlink" title="文章页顶部大图"></a>文章页顶部大图</h3><p>默认显示<strong>主题配置</strong>中的 <code>post.banner_img</code>，如需要设置单个文章的 Banner，在 <a href="https://hexo.io/zh-cn/docs/front-matter">front-matter (opens new window)</a>中指定 <code>banner_img</code> 属性。</p><p>本地图片存放位置同上。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">文章标题</span><br><span class="hljs-attr">tags:</span> [<span class="hljs-string">Hexo</span>, <span class="hljs-string">Fluid</span>]<br><span class="hljs-attr">index_img:</span> <span class="hljs-string">/img/example.jpg</span><br><span class="hljs-attr">banner_img:</span> <span class="hljs-string">/img/post_banner.jpg</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2019-10-10 10:00:00</span><br><span class="hljs-meta">---</span><br><span class="hljs-string">以下是文章内容</span><br></code></pre></td></tr></table></figure><h3 id="Tag-插件"><a href="#Tag-插件" class="headerlink" title="Tag 插件"></a>Tag 插件</h3><h4 id="便签"><a href="#便签" class="headerlink" title="#便签"></a><a href="https://hexo.fluid-dev.com/docs/guide/#%E4%BE%BF%E7%AD%BE">#</a>便签</h4><p>在 markdown 中加入如下的代码来使用便签：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs markdown">&#123;% note success %&#125;<br>文字 或者 <span class="hljs-code">`markdown`</span> 均可<br>&#123;% endnote %&#125;<br></code></pre></td></tr></table></figure><p>或者使用 HTML 形式：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">p</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;note note-primary&quot;</span>&gt;</span>标签<span class="hljs-tag">&lt;/<span class="hljs-name">p</span>&gt;</span><br></code></pre></td></tr></table></figure><p>可选便签：</p><p>primary</p><p>secondary</p><p>success</p><p>danger</p><p>warning</p><p>info</p><p>light</p><h4 id="行内标签"><a href="#行内标签" class="headerlink" title="行内标签"></a>行内标签</h4><p>在 markdown 中加入如下的代码来使用 Label：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs markdown">&#123;% label primary @text %&#125;<br></code></pre></td></tr></table></figure><p>或者使用 HTML 形式：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">span</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;label label-primary&quot;</span>&gt;</span>Label<span class="hljs-tag">&lt;/<span class="hljs-name">span</span>&gt;</span><br></code></pre></td></tr></table></figure><p>可选 Label：</p><p>primary default info success warning danger</p><h4 id="折叠块"><a href="#折叠块" class="headerlink" title="折叠块"></a>折叠块</h4><p>使用折叠块，可以折叠代码、图片、文字等任何内容，你可以在 markdown 中按如下格式：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs markdown">&#123;% fold info @title %&#125;<br>需要折叠的一段内容，支持 markdown<br>&#123;% endfold %&#125;<br></code></pre></td></tr></table></figure><p>info: 和行内标签类似的可选参数 title: 折叠块上的标题</p><h4 id="勾选框"><a href="#勾选框" class="headerlink" title="勾选框"></a>勾选框</h4><p>在 markdown 中加入如下的代码来使用 Checkbox：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs markdown">&#123;% cb text, checked?, incline? %&#125;<br></code></pre></td></tr></table></figure><p>text：显示的文字<br>checked：默认是否已勾选，默认 false<br>incline: 是否内联（可以理解为后面的文字是否换行），默认 false</p><p>示例：</p><div>            <input type="checkbox" disabled >普通示例          </div><div>            <input type="checkbox" disabled checked="checked">默认选中          </div>            <input type="checkbox" disabled >内联示例           后面文字不换行<input type="checkbox" disabled > 也可以只传入一个参数，文字写在后边（这样不支持外联）<h4 id="按钮"><a href="#按钮" class="headerlink" title="按钮"></a>按钮</h4><p>你可以在 markdown 中加入如下的代码来使用 Button：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs markdown">&#123;% btn url, text, title %&#125;<br></code></pre></td></tr></table></figure><p>或者使用 HTML 形式：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">a</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;btn&quot;</span> <span class="hljs-attr">href</span>=<span class="hljs-string">&quot;url&quot;</span> <span class="hljs-attr">title</span>=<span class="hljs-string">&quot;title&quot;</span>&gt;</span>text<span class="hljs-tag">&lt;/<span class="hljs-name">a</span>&gt;</span><br></code></pre></td></tr></table></figure><p>url：跳转链接<br>text：显示的文字<br>title：鼠标悬停时显示的文字（可选）</p><p><a href="javascript:;">text</a></p><h4 id="组图"><a href="#组图" class="headerlink" title="组图"></a>组图</h4><p>如果想把多张图片按一定布局组合显示，你可以在 markdown 中按如下格式：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs markdown">&#123;% gi total n1-n2-... %&#125;<br>  ![](<span class="hljs-link">url</span>)<br>  ![](<span class="hljs-link">url</span>)<br>  ![](<span class="hljs-link">url</span>)<br>  ![](<span class="hljs-link">url</span>)<br>  ![](<span class="hljs-link">url</span>)<br>&#123;% endgi %&#125;<br></code></pre></td></tr></table></figure><p>total：图片总数量，对应中间包含的图片 url 数量<br>n1-n2-…：每行的图片数量，可以省略，默认单行最多 3 张图，求和必须相等于 total，否则按默认样式</p><p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
      <category>Hexo</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
