<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>0-Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
    <link href="/2024/05/25/0-Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift/"/>
    <url>/2024/05/25/0-Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift/</url>
    
    <content type="html"><![CDATA[<h2 id="Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift"><a href="#Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift" class="headerlink" title="Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"></a>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</h2><p><img src="/../images/0-Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift/image-20240525163201675.png" alt="Title"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>0-Improving neural networks by preventing coadaptation of feature detectors</title>
    <link href="/2024/05/25/0-Improving-neural-networks-by-preventing-coadaptation-of-feature-detectors/"/>
    <url>/2024/05/25/0-Improving-neural-networks-by-preventing-coadaptation-of-feature-detectors/</url>
    
    <content type="html"><![CDATA[<p><img src="/../images/0-Improving-neural-networks-by-preventing-coadaptation-of-feature-detectors/image-20240525154637137.png" alt="TITLE"></p><h2 id="Improving-neural-networks-by-preventing-co-adaptation-of-feature-detectors"><a href="#Improving-neural-networks-by-preventing-co-adaptation-of-feature-detectors" class="headerlink" title="Improving neural networks by preventing co-adaptation of feature detectors"></a>Improving neural networks by preventing co-adaptation of feature detectors</h2><blockquote><p>arXiv:1207.0580v1 2012 06</p></blockquote><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>简而言之 Dropout 的提出，用于解决神经网络在训练过程中的过拟合问题，提升模型在测试集上的泛化能力。</p><p>Dropout 就是在每个训练步骤中，随机忽略（即“丢弃”）一部分神经元，使得网络不能依赖特定的神经元来进行特征检测，从而减少联合适应模式的发生。其原因就是神经网络中的神经元容易形成复杂的联合适应模式（co-adaptation），这种模式可能导致模型在训练数据上的表现很好，但在新数据上的表现较差。</p><p>过拟合已存在许久，之前也尝试过用诸如L2正则化、L1正则化等方式去解决这个问题，这些方法可以减少过拟合，但对于非常深的网络，其效果可能不如预期，或者增加大量的存储和计算成本。Dropout作为一种随机正则化技术，通过在训练过程中随机忽略一部分神经元，有效地防止了神经元之间的联合适应，显著提升了模型的泛化能力。</p><h3 id="实验方法"><a href="#实验方法" class="headerlink" title="实验方法"></a>实验方法</h3><ul><li>在每次前向传播过程中，对于每个神经元，以一定的概率$p$决定是否将其暂时移除（即设为0）。</li><li>在训练过程中，每次随机选择的被忽略神经元集不同。</li><li>在测试过程中，使用整个网络，但每个神经元的输出按训练中被忽略的概率$p$进行缩放，以反映训练时的期望输出。</li></ul><p><img src="/../images/0-Improving-neural-networks-by-preventing-coadaptation-of-feature-detectors/image-20240525160239002.png" alt="MNIST数据集测试"></p><p>在MNIST数据集，什么都没有用在MNIST没有dropout时160个错误，50%的dropout得到130个错误，在输入加入20%的dropout得到110个错误。可以看到，输入一定量的dropout可以有效减少错误率。</p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>MINIST数据集</p><p>28*28的手写数字图像，10分类，6万训练集，1万测试集</p><p><img src="/../images/0-Improving-neural-networks-by-preventing-coadaptation-of-feature-detectors/image-20240525161312218.png" alt="MINIST"></p><p>SVHN数据集</p><p>32*32*3的房子门牌号图像，识别房子门牌号，60万训练集，2万6测试集</p><p><img src="/../images/0-Improving-neural-networks-by-preventing-coadaptation-of-feature-detectors/image-20240525161440070.png" alt="SVHN"></p><p>CIFAR-10&#x2F;CIFAR-100数据集</p><p>32*32*3的现实图像，10分类和100分类，5万训练集，1万测试集</p><p><img src="/../images/0-Improving-neural-networks-by-preventing-coadaptation-of-feature-detectors/image-20240525161558748.png" alt="CIFAR"></p><p>ImageNet数据集</p><p><img src="/../images/0-Improving-neural-networks-by-preventing-coadaptation-of-feature-detectors/image-20240525161650035.png" alt="Imagenet"></p><p>与其他正则化方法比较</p><p><img src="/../images/0-Improving-neural-networks-by-preventing-coadaptation-of-feature-detectors/image-20240525161723994.png" alt="正则化方法比较"></p><p>在多个数据集上的实验结果显示，使用Dropout的神经网络在分类任务中的错误率显著降低。Dropout已成为神经网络训练中常用的正则化技术，被广泛应用于各种深度学习模型中，如卷积神经网络（CNN）和循环神经网络（RNN）。</p><p>总而言之，本文的主要贡献为：</p><ol><li><strong>防止过拟合</strong>：通过Dropout，网络变得更加鲁棒，不容易过拟合训练数据，提升了在测试数据上的表现。</li><li><strong>提高泛化能力</strong>：Dropout通过减少神经元间的联合适应模式，使得网络对不同特征的依赖更加平均，从而提高了模型的泛化能力。</li><li><strong>简单有效</strong>：Dropout是一种简单但非常有效的正则化技术，不需要对网络结构进行复杂的修改。</li></ol><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>通过引入Dropout技术，有效防止了神经元间的过度联合适应，显著提升了神经网络的泛化能力和性能。Dropout的简单实现和显著效果，使其成为深度学习领域的重要技术。Dropout技术对深度学习的研究和应用产生了深远的影响，推动了更鲁棒、更高效的神经网络模型的发展。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>1-基础部分-4-常用英文</title>
    <link href="/2024/05/25/1-%E5%9F%BA%E7%A1%80%E9%83%A8%E5%88%86-4-%E5%B8%B8%E7%94%A8%E8%8B%B1%E6%96%87/"/>
    <url>/2024/05/25/1-%E5%9F%BA%E7%A1%80%E9%83%A8%E5%88%86-4-%E5%B8%B8%E7%94%A8%E8%8B%B1%E6%96%87/</url>
    
    <content type="html"><![CDATA[<table><thead><tr><th>英文</th><th>翻译</th></tr></thead><tbody><tr><td><strong>review</strong></td><td>总结；综述</td></tr><tr><td><strong>high-resolution</strong></td><td>高分辨率</td></tr><tr><td><strong>top-1 error rate</strong></td><td>top1错误率，即分类预测的最大可能结果不是正确标签的概率</td></tr><tr><td><strong>top-5 error rate</strong></td><td>top5错误率，即分类预测的前5个可能结果都不是正确标签的概率</td></tr><tr><td><strong>neuron</strong></td><td>神经元</td></tr><tr><td><strong>non-saturating</strong></td><td>非饱和的</td></tr><tr><td><strong>object recognition</strong></td><td>目标识别&#x2F;对象检测</td></tr><tr><td><strong>label-preserving transformations</strong></td><td>标签保留转换，一种数据增强从而减少过拟合的方式</td></tr><tr><td><strong>feedforward neural network</strong></td><td>前馈神经网络</td></tr><tr><td><strong>down-sampled</strong></td><td>下采样</td></tr><tr><td><strong>rescaled</strong></td><td>重新调整图像大小</td></tr><tr><td><strong>nonlinearity</strong></td><td>非线性单元</td></tr><tr><td><strong>gradient descent</strong></td><td>梯度下降</td></tr><tr><td><strong>stochastic gradient descent (SGD)</strong></td><td>随机梯度下降法</td></tr><tr><td><strong>Rectified Linear Units (ReLUs)</strong></td><td>修正线性单元</td></tr><tr><td><strong>iteration</strong></td><td>迭代</td></tr><tr><td><strong>cross-GPU parallelization</strong></td><td>跨GPU并行化操作</td></tr><tr><td><strong>local response normalization</strong></td><td>局部响应归一化</td></tr><tr><td><strong>generalization</strong></td><td>泛化</td></tr><tr><td><strong>activity</strong></td><td>激活值</td></tr><tr><td><strong>overlapping pooling</strong></td><td>重叠池化，即移动步长小于核尺寸</td></tr><tr><td><strong>data augmentation</strong></td><td>数据增强</td></tr><tr><td><strong>weight decay</strong></td><td>权重衰减</td></tr><tr><td><strong>fine-tuning</strong></td><td>微调（网络）</td></tr><tr><td><strong>Euclidean distance</strong></td><td>欧几里得距离&#x2F;欧氏距离</td></tr><tr><td><strong>restricted</strong></td><td>受限的</td></tr><tr><td><strong>component</strong></td><td>成分；组成部分；组件、元件</td></tr><tr><td><strong>unit variance</strong></td><td>方差为1、单位方差</td></tr><tr><td><strong>retrieval</strong></td><td>检索</td></tr><tr><td><strong>reconstruction</strong></td><td>重构、重建</td></tr></tbody></table><table><thead><tr><th>英文</th><th>翻译</th></tr></thead><tbody><tr><td>shortcut connections</td><td>快捷连接</td></tr><tr><td>identity mappings</td><td>恒等映射</td></tr><tr><td>degradation</td><td>网络训练退化</td></tr><tr><td>projection mappings</td><td>投影映射</td></tr><tr><td>Deeper Bottleneck Architectures</td><td>深度瓶颈架构</td></tr><tr><td>Residual Learning</td><td>残差学习</td></tr><tr><td>vanishing&#x2F;exploding gradients</td><td>梯度消失&#x2F;发散</td></tr></tbody></table><table><thead><tr><th>英文</th><th>中文</th></tr></thead><tbody><tr><td>embedded computing</td><td>嵌入式计算</td></tr><tr><td>sparsity</td><td>稀疏性</td></tr><tr><td>sparse matrix&#x2F;matrices</td><td>稀疏矩阵</td></tr><tr><td>dense matrix&#x2F;matrices</td><td>密集矩阵</td></tr><tr><td>ensemble</td><td>组合</td></tr><tr><td>reduction&#x2F;projection layers</td><td>降维&#x2F;投影层</td></tr><tr><td>auxiliary classifier</td><td>辅助分类器</td></tr><tr><td>data-parallelism</td><td>数据并行</td></tr><tr><td>asynchronous stochastic gradient descent</td><td>异步随机梯度下降法</td></tr><tr><td>ground truth</td><td>实际结果</td></tr><tr><td>bounding box</td><td>边界框</td></tr><tr><td>false positive</td><td>假阳性</td></tr><tr><td>mean average precision (mAP)</td><td>平均精度均值</td></tr><tr><td>object bounding box recall</td><td>目标边界框召回率</td></tr><tr><td>bounding box regression</td><td>边界框回归</td></tr><tr><td>contextual model</td><td>上下文模型</td></tr></tbody></table><table><thead><tr><th>英文</th><th>中文&#x2F;解释</th></tr></thead><tbody><tr><td>adaptive moment estimation</td><td>自适应矩估计：基于不同阶数矩阵来适应不同参数的学习速率</td></tr><tr><td>sparse features&#x2F;gradients</td><td>稀疏特征&#x2F;稀疏梯度</td></tr><tr><td>exponential moving average</td><td>指数移动平均数</td></tr><tr><td>initialization bias correction</td><td>初始偏差修正</td></tr><tr><td>over-fitting</td><td>过拟合</td></tr><tr><td>robust,robustness</td><td>稳健性&#x2F;鲁棒性：一个系统或组织有抵御或克服不利条件的能力</td></tr><tr><td>normalization</td><td>规范化&#x2F;归一化&#x2F;标准化</td></tr></tbody></table><table><thead><tr><th>英文</th><th>中文释义</th></tr></thead><tbody><tr><td>regularizer</td><td>正则化项</td></tr><tr><td>internal covariate shift</td><td>内部协变量转移</td></tr><tr><td>stochastic gradient descent(SGD)</td><td>随机梯度下降法</td></tr><tr><td>convergence</td><td>收敛</td></tr><tr><td>differentiable</td><td>可微的</td></tr><tr><td>element-wise</td><td>逐元素地</td></tr><tr><td>nonlinearity</td><td>非线性</td></tr><tr><td>benchmark</td><td>基准</td></tr></tbody></table><table><thead><tr><th>英文</th><th>翻译</th></tr></thead><tbody><tr><td><strong>prohibitively</strong></td><td>静止地；过高地；过分地</td></tr><tr><td><strong>exponentially</strong></td><td>以指数方式</td></tr><tr><td><strong>robust</strong></td><td>健壮的，鲁棒</td></tr><tr><td><strong>conspiracy</strong></td><td>阴谋</td></tr><tr><td><strong>max-norm constraint</strong></td><td>最大范式约束</td></tr><tr><td><strong>Salient</strong></td><td>突出</td></tr><tr><td><strong>co-adaptation</strong></td><td>共适性</td></tr><tr><td><strong>marginalization</strong></td><td>边缘化</td></tr><tr><td><strong>inverted</strong></td><td>倒</td></tr><tr><td><strong>deterministic counterpart</strong></td><td>原图</td></tr><tr><td><strong>adversary</strong></td><td>对手</td></tr><tr><td><strong>Monte-Carlo</strong></td><td>蒙特卡罗</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>基础部分</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>0-Going Deeper with Convolutions</title>
    <link href="/2024/05/24/0-Going-Deeper-with-Convolutions/"/>
    <url>/2024/05/24/0-Going-Deeper-with-Convolutions/</url>
    
    <content type="html"><![CDATA[<h2 id="Going-Deeper-with-Convolutions"><a href="#Going-Deeper-with-Convolutions" class="headerlink" title="Going Deeper with Convolutions"></a>Going Deeper with Convolutions</h2><p><img src="/../images/0-Going-Deeper-with-Convolutions/image-20240524144235051.png"></p><blockquote><p>GoogLeNet CVPR2015</p></blockquote><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>在大规模图像识别挑战赛2014上，仅用AlexNet $\frac{1}{12}$ 的参数，就有比其更好的准确率。提出了名为“Inception”的网络架构（也称Inception v1），提高网络内部计算资源的利用率，增加网络深度和广度并保持计算预算不变。同时考虑到了移动与嵌入式设备的应用（减少算力和内存的消耗），增强实际工程的应用。</p><p>相关工作中作者提到了CNN的传统结构$（卷积+Normalization+max pooling）× n + 全连接层 × m$ 用于进行定位和目标检测，但是容易出现过拟合的现象，所以作者使用了稀疏连接（将全连接层换为稀疏的全连接层甚至是卷积层。稀疏连接可以要求更多的复杂计算，可能导致计算效率十分底下，所以需要一种方法，即能保持网络的稀疏性又能利用密集矩阵的高计算性能，所以作者提出来Inception结构。</p><h3 id="实验步骤"><a href="#实验步骤" class="headerlink" title="实验步骤"></a>实验步骤</h3><h4 id="1-Inception模块的设计"><a href="#1-Inception模块的设计" class="headerlink" title="1.Inception模块的设计"></a>1.Inception模块的设计</h4><p>Inception模块的核心思想是通过并行化不同大小的卷积核和池化层来捕捉输入特征的多尺度信息。具体来说，每个Inception模块包括以下几种操作：</p><ol><li><strong>1x1卷积</strong>：用于降维和增加网络的非线性能力。</li><li><strong>3x3卷积</strong>：捕捉中等范围的空间特征。</li><li><strong>5x5卷积</strong>：捕捉更大范围的空间特征。</li><li><strong>3x3最大池化</strong>：提供某种形式的下采样，同时保持局部特征不变。</li></ol><p><img src="/../images/0-Going-Deeper-with-Convolutions/image-20240525142343170.png" alt="Inception模块设计"></p><p>直接应用上述不同大小的卷积核和池化操作会导致计算量急剧增加。为了应对这一挑战，Inception模块通过：</p><ol><li><strong>1x1卷积用于降维</strong>：在应用3x3和5x5卷积之前，首先使用1x1卷积对输入进行降维。这一步骤显著减少了输入通道数，从而降低了后续卷积操作的计算量。这种做法不仅降低了计算复杂度，还通过增加非线性变换提升了模型的表达能力。</li><li><strong>并行计算与合并</strong>：Inception模块通过并行执行不同的卷积和池化操作，然后将这些操作的输出在通道维度上合并。这种并行设计允许网络在相同层次上捕捉到不同尺度的特征，而不会显著增加计算复杂度。</li></ol><p>用来实现高效计算。</p><p><img src="/../images/0-Going-Deeper-with-Convolutions/image-20240525142420927.png" alt="1x1卷积核降维"></p><h4 id="2-GoogleNet的网络结构"><a href="#2-GoogleNet的网络结构" class="headerlink" title="2.GoogleNet的网络结构"></a>2.GoogleNet的网络结构</h4><p>Inception架构师GoogleNet中的基本构建模块，由多个Inception模块堆叠而成，每个模块通过执行大小不同的卷积和池化操作来捕捉输入特征的多尺度信息。Inception模块的设计思想是通过1x1卷积进行降维，减少计算复杂度，同时增加网络的非线性和表达能力。通过这种方式，GoogleNet在保持计算效率的同时，显著增加了网络深度。GoogleNet在其结构中使用了22层（如果计算参数层则更深），其中包含了多个Inception模块。每个Inception模块都独立执行一组并行的卷积和池化操作，并将结果在通道维度上合并。</p><ul><li><strong>初始层</strong>：GoogleNet的初始层由几个标准的卷积层和池化层组成，用于初步提取输入图像的低级特征。</li><li><strong>Inception模块堆叠</strong>：在初始层之后，GoogleNet由多个Inception模块堆叠组成。这些模块并行执行不同大小的卷积和池化操作，捕捉不同尺度的特征，然后将结果合并。</li><li><strong>全局平均池化</strong>：在最后的几层，GoogleNet使用全局平均池化层取代了传统的全连接层，这样不仅减少了参数数量，还降低了过拟合的风险。</li><li><strong>分类层</strong>：最后，GoogleNet使用一个Softmax分类器输出预测结果。</li></ul><p><img src="/../images/0-Going-Deeper-with-Convolutions/image-20240525143716959.png" alt="GoogleNet网络架构"></p><h4 id="3-模型训练"><a href="#3-模型训练" class="headerlink" title="3.模型训练"></a>3.模型训练</h4><p>训练过程中对于数据集（ILSVRC2014）的增强方法也是这篇论文贡献的IDEA。</p><ul><li>独立训练了7个版本的相同的GoogLeNet模型，并用它们进行了整体预测。这些模型的训练具有相同的初始化和学习率策略，仅在采样方法和随机输入图像顺序方面不同。</li><li>在测试中，采用更激进的裁剪方法。<br>具体来说，我们将图像归一化为四个尺度，其中较短维度（高度或宽度）分别为256，288，320和352，取这些归一化的图像的左，中，右方块（在肖像图片中，我们采用顶部，中心和底部方块）。对于每个方块，我们将采用4个角以及中心224×224裁剪图像以及方块尺寸归一化为224×224，以及它们的镜像版本。这导致每张图像会得到4×3×6×2 &#x3D; 144的裁剪图像。<br>在实际应用中，这种激进的裁剪可能是不必要的，因为存在合理数量的裁剪图像后，更多裁剪图像的好处会变得很微小。</li><li>softmax概率在多个裁剪图像上和所有单个分类器上进行平均，然后获得最终预测。<br>分析了验证数据的替代方法，例如裁剪图像上的最大池化和分类器的平均，但是它们比简单平均的性能略逊。</li></ul><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><img src="/../images/0-Going-Deeper-with-Convolutions/image-20240525144636980.png" alt="分类任务"></p><p><img src="/../images/0-Going-Deeper-with-Convolutions/image-20240525144725901.png" alt="检测任务"></p><p>ImageNet大规模视觉识别挑战赛（ILSVRC 2014）GoogleNet（Inception V1)在比赛中取得了最佳成绩，展示了其卓越的性能和高效的计算能力。GoogleNet在当年的分类和检测任务都取得了很好的结果。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>作者通过引入Inception架构，在深度和计算效率之间找到了一个平衡点，显著提升了卷积神经网络的性能。其创新性的设计和方法对深度学习领域产生了重要影响。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>0-SimCSE: Simple Contrastive Learning of Sentence Embeddings</title>
    <link href="/2024/05/24/0-SimCSE-Simple-Contrastive-Learning-of-Sentence-Embeddings/"/>
    <url>/2024/05/24/0-SimCSE-Simple-Contrastive-Learning-of-Sentence-Embeddings/</url>
    
    <content type="html"><![CDATA[<p><img src="/../images/0-SimCSE-Simple-Contrastive-Learning-of-Sentence-Embeddings/image-20240506182339276.png" alt="image-20240506182339276"></p><h2 id="SimCSE-Simple-Contrastive-Learning-of-Sentence-Embeddings"><a href="#SimCSE-Simple-Contrastive-Learning-of-Sentence-Embeddings" class="headerlink" title="SimCSE: Simple Contrastive Learning of Sentence Embeddings"></a><strong>SimCSE: Simple Contrastive Learning of Sentence Embeddings</strong></h2><p>SimCSE：对比学习句向量表示</p><blockquote><p>发布于EMNLP 2021</p></blockquote><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p><strong>Embedding</strong>是指将高维度的数据（例如文字、图片、音频）映射到低维度空间的过程。Embedding向量是包含语义信息的。也就是含义相近的单词，Embedding向量在空间中有相似的位置。Embedding是数据科学工具包中至关重要的部分，已经广泛应用于各种不同领域的生产级机器学习系统，包括自然语言处理、推荐系统和计算机视觉等</p><p><strong>Dropout</strong>：Dropout是一种用于减少神经网络过拟合的正则化技术。在训练过程中，dropout会随机地将神经元的输出置为零，这样可以防止网络对特定的输入特征过度依赖，从而提高了模型的泛化能力。在SimCSE中，dropout被用作一种噪声注入的手段，用于生成不同的句子表示，从而帮助模型学习更加鲁棒和具有表征能力的句子嵌入。</p><p>**Simple Contrastive Learning of Sentence Embeddings(SimCSE)**：一种简单的对比学习框架，用于学习句子表示。SimCSE提出了无监督和监督两种方法来学习句子表示。在无监督方法中，SimCSE利用对比学习的方式，通过预测输入句子本身来学习句子表示。而在监督方法中，SimCSE利用自然语言推理数据集中的标注句对来训练模型，以进一步提高句子表示的性能。SimCSE的方法简单而高效，在语义文本相似性任务上取得了令人满意的性能表现。</p><p>   SimCSE这个对比学习框架，它可以通过预测输入句子本身来学习句子表示。例如，对于输入句子”The cat is sleeping on the mat.”，SimCSE会使用标准的dropout噪声来生成两个不同的嵌入向量，然后将其他句子作为“负样本”，并让模型预测哪个嵌入向量是“正样本”。这个过程可以在无标注数据上进行，因此是一种无监督的方法。SimCSE的监督方法则利用自然语言推理数据集中的标注句对来训练模型。例如，对于一个标注句对”The cat is sleeping on the mat.”和”The feline is resting on the mat.”，SimCSE会将它们的嵌入向量作为“正样本”，并将其他句子的嵌入向量作为“负样本”，从而训练模型。</p><p><strong>对比学习</strong>：对比学习的两个优化目标:</p><p>1.正例之间表示保持较近的距离;</p><p>2.随机样例的表示应分散在超平面上。并且这两个目标分别可以用指标alignment和uniformity来衡量。</p><p>样本对数据 D ，X_i和X_i^+是一对相似样本对。训练目标函数为</p><p><img src="/../images/0-SimCSE-Simple-Contrastive-Learning-of-Sentence-Embeddings/image-20240506182601761.png" alt="image-20240506182601761"></p><p><strong>正样本</strong>:构造(X_i,X_i^+)样本对。在CV领域可通过裁剪、翻转等方法很容易构建，在NLP领域则很难构造语义一致的样本。比较常见的是通过数据增强来构造，如同义词的替换，删除某个或某些不重要的词等，但这些方法很容易引入噪声，致使模型的效果提升有限。</p><p><strong>alignment</strong>和<strong>uniformity</strong>:alignment是正样本 （x_i,x_i^+）的平均距离. （对齐性）越小越好</p><p>uniformity计算向量整体分布的均匀程度，越均匀，保留的信息越多。 （均匀性） 越小越好</p><h3 id="实验方法"><a href="#实验方法" class="headerlink" title="实验方法"></a>实验方法</h3><p><strong>无监督</strong>SimCSE：</p><p>无监督SimCSE的工作原理，它使用对比学习的方法，通过预测输入句子本身来学习句子表示。使用不同的隐藏层dropout掩码来生成不同的句子表示，并将它们与同一批次中的其他句子表示进行比较，以学习更好的句子表示。 </p><p><img src="/../images/0-SimCSE-Simple-Contrastive-Learning-of-Sentence-Embeddings/image-20240506182827852.png" alt="image-20240506182827852"></p><p>一般来说，我们会使用一些数据扩增手段，让正例的两个样本有所差异，但是在 NLP 中如何做数据扩增本身也是一个问题，SimCSE 提出了一个极为简单优雅的方案：直接把 Dropout 当做数据扩增！</p><p><img src="/../images/0-SimCSE-Simple-Contrastive-Learning-of-Sentence-Embeddings/image-20240506182856730.png" alt="image-20240506182856730"></p><p><strong>有监督</strong>SimCSE：</p><p>监督SimCSE的工作原理，它利用自然语言推理数据集（NLI数据集）中的标注句对来训练模型，以进一步提高句子表示的性能。使用自然语言推理数据，以学习更好的句子表示。</p><p><img src="/../images/0-SimCSE-Simple-Contrastive-Learning-of-Sentence-Embeddings/image-20240506182928868.png" alt="image-20240506182928868"></p><p><strong>SimCSE</strong>的各向异性（Anisotropy）：</p><p>各向异性，又叫做表征退化问题，表示词嵌入在向量空间中占据了一个狭窄的圆锥体。与各向异性对应的是各向同性，指的是数据的分布在各个方向都一样，如图5。SimCSE论文中讨论的各向异性是和我们前面的均匀性类似的概念。目前缓解模型坍塌的策略消除主成分，加入正则项以及将各向异性映射为各向同性等等。SimCSE则证明了对比学习的训练目标可以降低模型的各向异性。</p><p>在SimCSE中，作者表明，对比目标还可以通过提高嵌入空间的均匀性来缓解各向异性问题。他们从奇异谱的角度证明了对比学习目标“平坦”了句子嵌入空间的奇异值分布，从而提高了一致性。这是通过将对比学习目标中的负面事例分开来实现的。因此，SimCSE为语言表征中的各向异性问题提供了一种新的解决方案。</p><p>对比学习能平缓奇异值的分布。</p><p><img src="/../images/0-SimCSE-Simple-Contrastive-Learning-of-Sentence-Embeddings/image-20240506183000010.png" alt="image-20240506183000010"></p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>Sentence embedding performance on STS tasks</p><p>语义文本相似性</p><p><img src="/../images/0-SimCSE-Simple-Contrastive-Learning-of-Sentence-Embeddings/image-20240506183047519.png" alt="image-20240506183047519"></p><p><strong>alignment</strong>和<strong>uniformity</strong>对比其他方法，可以看出SimCSE在alignment和uniformity两个方面均要好于BERT算法。</p><p><img src="/../images/0-SimCSE-Simple-Contrastive-Learning-of-Sentence-Embeddings/image-20240506183056349.png" alt="image-20240506183056349"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>​SimCSE是一个原理并不复杂的算法，它提出了使用Dropout构建正样本对这个简单易行的方案，解决了模型预训练过程中容易出现的模型坍塌的问题。Sim-CSE非常简单但效果非常好，其背后的数学原理是引入深思的。SimCSE一文对对比学习之后的数学原理进行了深入的探讨。证明了对比学习的损失函数是具有同时优化对齐性和均匀性这两个方向的。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>论文笔记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>0-Knowledge Mining with Scene Text for Fine-Grained Recognition</title>
    <link href="/2024/05/24/0-Knowledge-Mining-with-Scene-Text-for-Fine-Grained-Recognition/"/>
    <url>/2024/05/24/0-Knowledge-Mining-with-Scene-Text-for-Fine-Grained-Recognition/</url>
    
    <content type="html"><![CDATA[<h3 id="Knowledge-Mining-with-Scene-Text-for-Fine-Grained-Recognition"><a href="#Knowledge-Mining-with-Scene-Text-for-Fine-Grained-Recognition" class="headerlink" title="Knowledge Mining with Scene Text for Fine-Grained Recognition"></a><strong>Knowledge Mining with Scene Text for Fine-Grained Recognition</strong></h3><p>基于场景文字知识挖掘的细粒度图像识别算法</p><blockquote><p>CVPR 2022</p></blockquote><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p><strong>场景文字的识别：</strong>和文档文本不同，场景文字具有稀疏性，通常以少许关键词的形式存在于自然环境中。通过稀疏的关键词，机器难以获取精准的语义。然而，人类能够较为充分地理解稀疏的场景文字。原因在于，人类具有大量的外部知识库，能够通过知识库来弥补稀疏的场景文字所带来的语义损失。</p><p><img src="/../images/0-Knowledge-Mining-with-Scene-Text-for-Fine-Grained-Recognition/image-20240506184438727.png" alt="Bottle 数据集中的案例，3 张图像均属于 soda 类别"></p><p>如图所示：</p><p>​该数据集是关于细粒度图像分类任务，旨在区分图像中的瓶子属于哪种饮品或。图中 3 张图像均属于 soda 类饮品，尽管案例（c）同样属于 soda 类饮品，但是其附属的场景文本的表面信息无法提供明显的线索。场景文字在 百科 中的描述，百科 告知我们，场景文本 leninade 代表某种品牌，其属于 soda 类饮品。因此，挖掘场景文本背后丰富的语义信息能够进一步弥补场景文本的语义损失，从而更为准确地理解图像中的目标。</p><p><strong>Fine-Grained Image Classification</strong>（细粒度图像分类）：区分某些领域中物体类别之间具有细微视觉差异的图像。</p><p>   1.仅使用视觉线索对对象进行分类，并旨在找到有区别的图像路径。</p><p>   2.通过使用场景文本的视觉线索来利用场景文本进行细粒度图像分类任务。</p><p>   3.利用场景文本的文本线索作为判别信号，并结合GoogLeNet获得的视觉特征来区分商业场所。</p><p>尽管取得了有希望的进展，但现有的方法利用了场景文本的字面意义，而忽略了有意义的人类的文本知识。</p><p>**Knowledge-aware Language Models(<strong>知识感知语言模型</strong>)**：预训练语言模型经过优化，可以预测给定序列中的下一个单词或一些屏蔽单词。这种知识通常是从预训练模型生成的潜在上下文表示中获得的，或者通过使用预训练模型的参数来初始化特定于任务的模型以进行进一步微调来获得。</p><p>   在论文的方法中，采用 BERT 和 KnowBert 作为知识感知语言模型，并应用它们来提取知识特征。尽管以前的方法从视觉语言任务的句子中提取知识特征，但它们需要图像文本对的注释。</p><h3 id="实验方法"><a href="#实验方法" class="headerlink" title="实验方法"></a>实验方法</h3><p>算法框架：</p><p><img src="/../images/0-Knowledge-Mining-with-Scene-Text-for-Fine-Grained-Recognition/image-20240506184601627.png" alt="image-20240506184601627"></p><p>算法框架图，由视觉特征分支、知识提取分支和知识增强分支、视觉-知识注意力模块（VKAC）和分类器构成。</p><p>网络框架由视觉特征分支、知识提取分支和知识增强分支、视觉-知识注意力模块和分类器构成。算法输入包括 3 部分：图像，图像中包含的场景文本实例，外部知识库。其中场景文本实例通过已有的文字识别器从输入图像中获取，外部知识库采用了 Wikipedia。知识提取分支提取场景文本实例背后的语义信息（知识特征），知识增强分支融合场景文本实例和挖掘出的知识特征。随后，视觉-知识注意力模块融合视觉和知识特征，并将其输入给分类器进行分类。</p><p><strong>知识提取分支</strong>：该分支由实体候选选择器和实体编码器构成。实体候选选择器预先在大量语料库上统计单词在所有可能实体上的概率分布，根据概率分布选取前 10 个候选实体，并将其输入给实体编码器进行特征编码。实体编码器在 Wikipedia 的数据库上进行预训练，预训练任务旨在通过 Wikipedia 上实体的描述来预测该页面的标题（实体名称）。</p><p><strong>知识增强特征分支</strong>：该分支主要由 bert 构成，在 bert 的第 10 层后插入知识注意力模块（KARC），该模块融合了文本实例特征和知识特征后，接着输入给 bert 剩余的层。Bert 第 12 层输出的特征给 VKAC 模块。</p><p><strong>视觉</strong>-知识注意力模块：并非所有的场景文本或知识对理解图像有积极作用，为选取和图像内容相关的场景文本和知识来加强对图像的理解。该模块以图像全局特征作为访问特征，从增强的知识特征中选取相关的知识特征来加强视觉特征。其网络结构由注意力模型构成。</p><p><img src="/../images/0-Knowledge-Mining-with-Scene-Text-for-Fine-Grained-Recognition/image-20240506184654534.png" alt="image-20240506184654534"></p><p>通过在BERT模型中的特定层(第十层)插入一个KARC（knowledge attention and recontextualization component）</p><p>•将一个单词序列输入到BERT中10层连续的编码层，得到语境 Hi</p><p>•将 Hi ，knowledge extraction得到的entity embedding输入到KARC中，输出知识增强的text representation Hi ’</p><p>•将 Hi ‘输入到BERT中剩余的编码层，并获得最终的知识增强的特征，输送给下一子模型(<em>Visual-knowledge attention component</em> )。</p><p><strong>注意力机制</strong>：</p><p>主要目标：将注意力放在和场景内容有较强相关的文字上，忽略和场景关系不大的文字。</p><p> 方法：提取全局的视觉特征，将视觉特征和提取到的知识特征做对比，选取相似度高的视觉特征（注意力机制）。</p><p>  为研究场景文本背后的知识对图像识别的帮助，收集了一个关于人群活动的数据集。该数据集中的类别主要分为游行示威和日常人群密集活动两大类，细分为 21 类。</p><p><img src="/../images/0-Knowledge-Mining-with-Scene-Text-for-Fine-Grained-Recognition/image-20240506184732594.png" alt="image-20240506184732594"></p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><img src="/../images/0-Knowledge-Mining-with-Scene-Text-for-Fine-Grained-Recognition/image-20240506184743920.png" alt="image-20240506184743920"></p><p><strong>和</strong> <strong>SOTA</strong> <strong>对比：</strong>在公开数据集 Con-Text、Bottles 以及我们收集的 Activity 数据集上，在使用 resnet50[3]和 E2E-MLT[4]作为视觉特征提取器和文字提取器时，我们方法能在同等情况下取得最佳结果。当使用 ViT 和 Google OCR 时，其模型性能结果能进一步提升。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>​本文提出了一种通过挖掘场景文本背后语义来增强分类模型理解图像内容的方法，该方法的核心是利用场景文字作为关键词，到 wikipedia 知识库中检索出相关的知识，并获取其特征表达，和图像视觉特征进行融合理解，而并非仅仅利用场景文字的表面语义信息。得益于挖掘场景文本背后的知识，该方法能够更好地理解文字语义并不非常直观的内容。实验表明，该方法在 3 个数据集上均取得了最佳结果。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>论文笔记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>0-Instance and Panoptic Segmentation Using Conditional Convolutions</title>
    <link href="/2024/05/24/0-Instance-and-Panoptic-Segmentation-Using-Conditional-Convolutions/"/>
    <url>/2024/05/24/0-Instance-and-Panoptic-Segmentation-Using-Conditional-Convolutions/</url>
    
    <content type="html"><![CDATA[<h2 id="Instance-and-Panoptic-Segmentation-Using-Conditional-Convolutions"><a href="#Instance-and-Panoptic-Segmentation-Using-Conditional-Convolutions" class="headerlink" title="Instance and Panoptic Segmentation Using Conditional Convolutions"></a>Instance and Panoptic Segmentation Using Conditional Convolutions</h2><p><img src="/../images/0-Instance-and-Panoptic-Segmentation-Using-Conditional-Convolutions/image-20240506183518983.png" alt="image-20240506183518983"></p><blockquote><p>IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 45, NO. 1, JANUARY 2023</p></blockquote><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>​作者提出了一种实例和全景分割框架,其在COCO数据集上的表现优于其他几种最先进的方法,这种实例和全景分割框架称为”Condlnst(实例分割和条件卷积)”.</p><p>​实例分割和全景分割是计算机视觉中的重要任务，需要算法对图像中的每个感兴趣实例进行像素级的分割，并为图像中的每个像素分配语义标签。全景分割在实例分割框架的基础上进一步要求对场景中的“stuff”进行分割，为图像中的每个像素分配语义标签。实例分割和全景分割面临着一个共同的挑战，即如何高效有效地区分个体实例。传统的方法通常采用Mask R-CNN等两阶段方法，通过ROI操作来关注每个实例，但这种方法存在一些缺点，如需要更大的计算量、固定的掩模头等。作者提出的Condlnst 框架，通过将实例分割和全景分割统一为完全卷积网络，消除了ROI裁剪和特征对齐的需要,由于动态生成的条件卷积容量大大提高，掩模头可以非常紧凑，从而大大加快每个实例的推理时间，在实例和全景分割任务上实现了最先进的性能，同时速度快且简单。</p><p><img src="/../images/0-Instance-and-Panoptic-Segmentation-Using-Conditional-Convolutions/image-20240504165817465.png" alt="Condlnst动态生成过滤器,每个输出映射只包含一个实例的掩码."></p><h3 id="实验方法"><a href="#实验方法" class="headerlink" title="实验方法"></a>实验方法</h3><p>Condlnst用于实例分割的整体架构:通过使用实例敏感的卷积滤波器和相对坐标来动态生成掩模头，实现了对每个实例的关注。与传统的固定权重掩模头不同，Condlnst的掩模头参数根据要预测的实例进行调整，使得网络参数能够编码实例的特征，并且只在该实例的像素上激活，从而绕过了标准FCN中的困难。</p><p><img src="/../images/0-Instance-and-Panoptic-Segmentation-Using-Conditional-Convolutions/image-20240504170419450.png" alt="CondInst整体框架"></p><p>Condlnst结构可以分为四个主要部分:</p><ol><li>**特征提取网络:**负责从输入图像中提取多尺度的特征图。这些特征图通常具有深度学习中的层级结构，表现为从浅层到深层的不同抽象级别。</li><li><strong>特征金字塔网络:</strong> 特征金字塔网络是一种受人体视觉启发的结构，能够将高层次的语义信息与低层次的细节信息结合起来，产生一系列尺度的特征图，这对于检测不同尺寸的对象非常有用。</li><li><strong>CondInst:</strong> 在这一模块，网络通过一组称作“head”的子网络来进行实际的实例分割。这些头部分别负责预测类别得分，边界框回归，以及产生实例分辨率的特定掩膜。掩膜生成是实例分割的核心，需要精准地为每个检测到的物体实例生成一个像素级的掩膜。</li><li><strong>输出和后处理：</strong> 最后，网络结合来自头部网络的预测结果，并通过逐像素的分类来生成最终的实例分割掩膜。</li></ol><p>​    以对实例分割掩码、类别预测和边界框预测进行监督学习为训练目标，使网络能够准确地预测每个物体实例的位置、类别和掩码。</p><p><strong>模型细节:</strong></p><p>​实验使用了的MS COCO和Cityscapes数据集进行训练和评估。</p><p>​在训练过程中，作者采用了多尺度数据增强策略，以提高模型的泛化能力。</p><p>​基准掩码头采用了三个1x1卷积层，每个卷积层有8个通道，并使用ReLU作为激活函数，最后一层使用sigmoid函数预测前景的概率。</p><p>​掩码头总共有169个参数，非常轻量级，相比Mask R-CNN等模型，计算复杂度大大降低。</p><p>​改变底部分支输出特征图的通道数（C_bottom），实验结果表明，在合理范围内（从2到16），性能基本保持稳定。</p><p>​生成的动态滤波器可以被视为轮廓的表示，与Mask R-CNN不同，Condlnst通过生成的滤波器编码实例的轮廓，因此可以轻松表示包括不规则形状在内的各种形状，更加灵活。</p><p>​作者建议在Condlnst模型中使用<strong>上采样因子</strong>为2，因为这种设置在各项指标上表现较好。</p><p>​在推断过程中去除边界框分支并使用基于掩码的NMS(非极大值抑制)，可以获得与基于边界框的NMS相似的性能。</p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><img src="/../images/0-Instance-and-Panoptic-Segmentation-Using-Conditional-Convolutions/image-20240504173341240.png" alt="与先前最先进方法的比较"></p><p>​结果显示，Condlnst在1次学习率计划（90K迭代）下的性能优于原始的Mask R-CNN，并且比原始Mask R-CNN更快（每张图像在单个V100 GPU上）。</p><p>​Condlnst还在性能上优于Detectron2中的Mask R-CNN。</p><p>​通过更长的训练计划或更强大的骨干网络，如ResNet-101，也可以实现一致的改进。</p><p>​通过辅助语义分割任务，Condlnst的性能可以从37.7%提升到38.6%（ResNet-50），或从39.1%提升到40.0%（ResNet-101），而推理时间不增加。</p><p><img src="/../images/0-Instance-and-Panoptic-Segmentation-Using-Conditional-Convolutions/image-20240504173634513.png" alt="实时CondInst模型在COCO测试开发数据上的Mask AP和推理速度"></p><p>​实验结果表明，基于ResNet-50的Condlnst-RT在AP方面优于YOLACT++，并且几乎具有相同的推理速度。</p><p>​使用更强大的骨干网络DLA-34，CondInst-RT可以实现47 FPS的速度，并保持类似的性能水平。</p><p><img src="/../images/0-Instance-and-Panoptic-Segmentation-Using-Conditional-Convolutions/image-20240504173851199.png" alt="Cityscapes数据集上进行实例分割的实验"></p><p>​在Cityscapes数据集上，作者使用COCO风格的mask AP作为性能评估指标。实验结果表明，Condlnst在Cityscapes数据集上的表现优于之前的强基线模型Mask R-CNN，提高了超过1%的mask AP。</p><p>​另外在Cityscapes数据集上，CondInst在全景分割任务上表现优异，超过了之前的方法，包括Panoptic-FPN等。与类似方法AdaptIS相比，CondInst在ResNet-101基础上取得了显着更好的性能，这表明在这里使用动态滤波器可能更为有效。与最近的方法（如Panoptic-FCN）相比，CondInst在全景分割任务上也取得了显著的性能提升。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>​Condlnst是一个新的实例分割框架，通过动态生成掩码头部的滤波器，减少了参数和计算复杂度，提高了速度和准确性，同时无需更长的训练周期。它还可以简单地扩展到解决全景分割问题，并在COCO数据集上达到最先进的性能。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>论文笔记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>0-Faster R-CNN</title>
    <link href="/2024/05/21/0-Faster-R-CNN/"/>
    <url>/2024/05/21/0-Faster-R-CNN/</url>
    
    <content type="html"><![CDATA[<p><img src="/../images/0-Faster-R-CNN/image-20240521164920800.png" alt="Faster R-CNN"></p><blockquote><p>发布于NIPS 2015</p></blockquote><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><ul><li><strong>R-CNN</strong>：使用选择性搜索算法生成候选区域，然后对每个区域进行卷积神经网络（CNN）特征提取和分类。这种方法的计算效率低，因为每个候选区域都要单独进行特征提取。</li><li><strong>Fast R-CNN</strong>：改进了R-CNN，通过在整个图像上进行一次特征提取，然后使用区域兴趣（RoI）池化层对候选区域进行分类和回归。这减少了冗余计算，但候选区域的生成仍是一个瓶颈。</li></ul><p>目标检测不仅需要识别图像中的对象，还需要确定每个对象的位置（分类与回归），先前的方法在速度和准确性上难以均衡，由于生成候选区的过程比较慢，所以候选区域生成成为了检测速度的瓶颈。</p><p><strong>Faster R-CNN</strong>通过引入区域建议网络（RPN，Region Proposal Network）解决了候选区域生成的瓶颈问题。RPN是一个全卷积网络，能够直接从图像的特征图中生成高质量的候选区域。这一设计使得整个目标检测过程可以端到端地进行，大大提高了检测速度和准确性。</p><p>通过将RPN和Fast R-CNN结合，Faster R-CNN实现了更快且更精确的目标检测，成为当时目标检测领域的一个重要突破。这一创新影响深远，奠定了后续目标检测方法的发展基础。</p><h3 id="实验方法"><a href="#实验方法" class="headerlink" title="实验方法"></a>实验方法</h3><p><img src="/../images/0-Faster-R-CNN/image-20240521191841145.png" alt="处理多尺度和多尺寸问题"></p><p>Fast R-CNN使用参考框金字塔，即在在单一特征图上使用不同尺寸的参考框进行回归，既高效又灵活。</p><p><img src="/../images/0-Faster-R-CNN/image-20240521192258207.png" alt="Faster R-CNN基本结构"></p><p>从图中即可看到，主要分为4层：Conv layers,Region Proposal Network(RPN),RoI pooling, Classification.</p><h4 id="1-Conv-layers"><a href="#1-Conv-layers" class="headerlink" title="1.Conv layers"></a>1.Conv layers</h4><p>包含了卷积层，池化层，激活函数这三层。</p><p>在卷积（13层）操作时，原图的四周被加了一圈零，这使得3x3卷积后的图像与原图大小一致。在池化（4层）操作时，将输出的长宽都变为输入的二分之一，激活函数有13层。最终图片变为原来的1&#x2F;16。</p><p>生成的特征图保留了输入图像的重要空间信息和上下文信息，为后续的RPN提供基础特征。</p><h4 id="2-RPN"><a href="#2-RPN" class="headerlink" title="2.RPN"></a>2.RPN</h4><p>是一个轻量级的全卷积网络，直接在卷积层生成的特征图上运行。</p><p>主要由一个3x3的卷积层，以及两个并行的1x1卷积层组成，一个用于分类（是否包含对象），一个用于边界框回归（预测候选区域的位置）。</p><p>生成一系列潜在的候选区域（anchor boxes），这些区域可能包含对象。每个anchor box通过分类分支判断是否包含对象，通过回归分支调整其位置和尺寸。输出高质量的候选区域，这些区域将被进一步处理。</p><p>这里作者提到了一个词叫做“anchors”，就是RPN生成的一堆矩形，根据图像的大小来生成很多不重合的矩形选框,为每一个点都进行anchors选框，再进行遍历计算。</p><p><img src="/../images/0-Faster-R-CNN/image-20240521195617224.png" alt="anchors选框"></p><p>这些大量的anchors，使用CNN判断那些是有目标的anchor，哪些是没有目标的anchor，退化为二分类问题，例如一个800x600的图像就有17100个anchor。</p><p>另外，光靠anchor定位判断positive和negative是不够的，所以采用了bounding box regression：</p><p>当Ground Truch（GT 实际框）和提取出的框不准时，我们希望有方法能微调，所以我们的思路是先做平移，再做缩放。训练时作者给出了应该的平移量$(x_t ,x_y)$与尺度因子$(t_w,t_h)$：</p><p><img src="/../images/0-Faster-R-CNN/image-20240521201000480.png" alt="平移量与尺度因子"></p><p>训练时让其尽量接近，就可以修正anchor的位置了。</p><p>最后就是生成proposals。</p><h4 id="3-Roi-Pooling"><a href="#3-Roi-Pooling" class="headerlink" title="3.Roi Pooling"></a>3.Roi Pooling</h4><p>RoI池化层将RPN生成的候选区域映射到特征图上，并将每个区域池化到相同的固定大小。通过一个固定大小的窗口对候选区域内的特征进行池化。</p><p>大小不一的候选区域转换为固定大小的特征图，便于后续的全连接层处理。保留候选区域内的空间信息，并生成统一大小的特征向量。</p><h4 id="4-Classification"><a href="#4-Classification" class="headerlink" title="4.Classification"></a>4.Classification</h4><p>经过RoI池化层处理后的特征向量通过一系列全连接层进行进一步处理。最终通过两个并行的全连接层，一个用于分类，一个用于边界框回归。</p><p>分类层：对每个候选区域进行分类，确定其类别（包括背景类）。</p><p>边界框回归层：进一步调整每个候选区域的边界框，使其更精确地定位对象。</p><h4 id="损失函数："><a href="#损失函数：" class="headerlink" title="损失函数："></a>损失函数：</h4><p><img src="/../images/0-Faster-R-CNN/image-20240521201436051.png" alt="损失函数"></p><p>i代表第i个anchor，pi是正softmax概率，pi*是GT概率，t代表预测边界框，t*是正预测边界框。</p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>将检测速度一举提升到17 FPS，而且在VOC 2012测试集上实现了70.4%的检测效果</p><p><img src="/../images/0-Faster-R-CNN/image-20240521201705947.png" alt="实验结果"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Faster R-CNN通过引入区域建议网络（RPN）极大地提高了目标检测的速度和精度，解决了多尺度对象检测的难题，对计算机视觉领域产生了深远的影响。这一方法不仅提高了目标检测的性能，还为后续研究提供了新的思路和方法。Faster R-CNN的提出标志着目标检测技术的一个重要里程碑，开启了更多关于高效目标检测算法的研究和发展。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>论文笔记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>0-CDDSA: Contrastive Domain Disentanglement and Style Augmentation for Generalizable Medical Image Segmentation</title>
    <link href="/2024/05/16/0-CDDSA-Contrastive-Domain-Disentanglement-and-Style-Augmentation-for-Generalizable-Medical-Image-Segmentation/"/>
    <url>/2024/05/16/0-CDDSA-Contrastive-Domain-Disentanglement-and-Style-Augmentation-for-Generalizable-Medical-Image-Segmentation/</url>
    
    <content type="html"><![CDATA[<p><img src="/../images/0-CDDSA-Contrastive-Domain-Disentanglement-and-Style-Augmentation-for-Generalizable-Medical-Image-Segmentation/image-20240516203813793.png" alt="image-20240516203813793"></p><h2 id="CDDSA-用于广义医学图像分割的对比域去纠缠和风格增强"><a href="#CDDSA-用于广义医学图像分割的对比域去纠缠和风格增强" class="headerlink" title="CDDSA:用于广义医学图像分割的对比域去纠缠和风格增强"></a>CDDSA:用于广义医学图像分割的对比域去纠缠和风格增强</h2><p>发表于：Medical Image Analysis  Volume 89 ,2023年10月, 102904 </p><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>在分割未见过的临床医学图像的过程中，区分域特定特征和域不变特征的能力是实现域泛化（鼓励模型DG）的关键。现有的DG方法难以有效的解纠缠，从而获得高泛化能力，故提出了本文的方法：CDDSA框架（对比域去纠缠和风格增强），用于推广医学图像分割。CDDSA的大概步骤如下：</p><ol><li><strong>特征分解</strong>：首先，提出了一个分解网络，将图像分解为领域不变的解剖表示和领域特定的风格编码。解剖表示被送入一个分割模型，该模型不受领域转移的影响。</li><li><strong>重建图像</strong>：分解网络通过一个解码器进行正则化，结合解剖和风格编码来重建输入图像。这有助于学习如何有效地分离领域特定和领域不变的特征。</li><li><strong>分割模型</strong>：分割器采用领域不变的解剖表示作为输入，以获得分割结果。这样可以确保分割模型不受领域变化的影响，从而提高泛化能力。</li><li><strong>风格增强</strong>：引入风格增强策略，将给定图像的解剖表示与增强的风格编码相结合，生成新领域中的图像。这有助于模型学习如何适应不同风格的图像，提高泛化性能。</li></ol><p>使用视杯和椎间盘分割的公共多位点眼底图像数据集和用于鼻咽总肿瘤体积（GTVnx）分割的内部多位点鼻咽癌磁共振图像（NPC-MRI）数据集上进行了验证，实验结果表明，所提出的CDDSA在不同领域具有显著的可推广性，并且在领域可推广分割方面优于几种最先进的方法。</p><p>在医学图像分割中，深度学习方法取得了显著的性能，但现有模型通常建立在训练和测试图像来自相同领域且具有非常相似（甚至相同）分布的假设上。然而，在临床实践中，由于多种因素（如扫描设备、成像协议、患者群体和图像质量的差异），测试图像通常来自于与训练集不同的医疗中心，这种假设经常不成立。这种领域转移会显著降低模型在测试时的性能。为了解决这一问题，许多领域自适应方法被探索，以将源领域中的一组标记图像的知识转移到目标领域中的图像。</p><h3 id="实验方法"><a href="#实验方法" class="headerlink" title="实验方法"></a>实验方法</h3><p><img src="/../images/0-CDDSA-Contrastive-Domain-Disentanglement-and-Style-Augmentation-for-Generalizable-Medical-Image-Segmentation/image-20240516213050418.png" alt="CDDSA方法的工作流程"></p><p>a）图像分割；b）图像增强。仅使用一对解剖编码器和样式编码器将不同领域中的医学图像组合成领域不变的解剖表示和领域特定的样式代码，该编码器和样式代码由接受解剖表示和样式代码的解码器正则化以重建图像。</p><p><img src="/../images/0-CDDSA-Contrastive-Domain-Disentanglement-and-Style-Augmentation-for-Generalizable-Medical-Image-Segmentation/image-20240516222055623.png" alt="CDDSA网络概述"></p><p>图中几个主要组件：</p><ol><li>**编码器部分 (E_anat 和 E_sty)**：解剖编码器（E_anat）和风格编码器（E_sty）分别提取输入图像的解剖结构和风格信息。</li><li>**域对比学习 (Domain-wise contrastive learning)**：在潜在空间中进行对比学习，以使来源域和目标域的数据表示彼此靠近，并远离其他域的数据表示。</li><li>**域增强 (Domain augmentation)**：通过加入风格扰动（s_seg）来增强图像，从而提高网络的鲁棒性。</li><li>**重构模块 (D_rec)**：用来重构输入图像，以确保编码后的解剖和风格表示的质量。</li><li>**判别器模块 (D_dom)**：用来区分不同域的图像，并在损失函数（L_adv）中发挥作用，进一步指导表示学习。</li></ol><p>通过这些模块和损失函数的协同作用，网络能够学习到解耦的解剖结构和风格信息，实现多域医学图像的处理和分析。</p><p><img src="/../images/0-CDDSA-Contrastive-Domain-Disentanglement-and-Style-Augmentation-for-Generalizable-Medical-Image-Segmentation/image-20240516222254046.png" alt="重建解码器"></p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><img src="/../images/0-CDDSA-Contrastive-Domain-Disentanglement-and-Style-Augmentation-for-Generalizable-Medical-Image-Segmentation/image-20240516222720457.png" alt="不同数据收集方法的眼底数据集的比较"></p><p><img src="/../images/0-CDDSA-Contrastive-Domain-Disentanglement-and-Style-Augmentation-for-Generalizable-Medical-Image-Segmentation/image-20240516222818541.png" alt="不同DG方法对眼底数据集的比较"></p><p><img src="/../images/0-CDDSA-Contrastive-Domain-Disentanglement-and-Style-Augmentation-for-Generalizable-Medical-Image-Segmentation/image-20240516222920629.png" alt="实验结果"></p><p><img src="/../images/0-CDDSA-Contrastive-Domain-Disentanglement-and-Style-Augmentation-for-Generalizable-Medical-Image-Segmentation/image-20240516223004772.png" alt="图像增强"></p><p>作者也对其他不同的数据集做了实验， 并附加了消融学习结果。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>CDDSA框架在多领域医学图像分割任务上有效性。通过对多领域眼底图像和多领域鼻咽癌磁共振图像（NPC-MRI）的全面实验结果，作者展示了CDDSA在未见领域上取得了高泛化性能，并且优于几种最先进的领域泛化方法。表明CDDSA框架在处理医学图像分割中的领域泛化问题上具有潜在的应用前景。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>论文笔记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>0-ImageNet Classification with Deep Convolutional Neural Networks</title>
    <link href="/2024/04/22/0-ImageNet-Classification-with-Deep-Convolutional-Neural-Networks/"/>
    <url>/2024/04/22/0-ImageNet-Classification-with-Deep-Convolutional-Neural-Networks/</url>
    
    <content type="html"><![CDATA[<p><img src="/../images/0-ImageNet-Classification-with-Deep-Convolutional-Neural-Networks/image-20240422232943298.png"></p><h2 id="AlexNet-2012-NIPS"><a href="#AlexNet-2012-NIPS" class="headerlink" title="AlexNet 2012 NIPS"></a>AlexNet 2012 NIPS</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>为了从数以百万计的图像中学习出数千种的目标，需要一个具有很强学习能力的模型。尽管CNNs有效率的局部结构，但大规模地应用于高分辨率图像消耗资源仍然过多。本文介绍了一种可以进行图像识别的卷积神经网络，包含了大量的不常见和新的特征来提升网络性能，减少训练时间。</p><p>包含6千万个参数和65万个神经元，包含了5个卷积层，其中有几层后面跟着最大池化层，以及3个全连接层，最后还有一个1000路的softmax层。为了加快训练速度，本文使用了不饱和神经元以及一种高效的基于GPU的卷积运算方法。为了减少全连接层的过拟合，采用了正则化方法“dropout”，该方法被证明非常有效。</p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><h4 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h4><p>8层学习层——5层卷积层和三层全连接层</p><p><img src="/../images/0-ImageNet-Classification-with-Deep-Convolutional-Neural-Networks/image-20240422233750222.png" alt="image-20240422233750222"></p><p>用ReLUs主要是对训练集的拟合进行加速。快速学习对由大规模数据集上训练出大模型的性能有相当大的影响。</p><p>ReLUs具有符合本文要求的一个性质：它不需要对输入进行归一化来防止饱和。</p><p><img src="/../images/0-ImageNet-Classification-with-Deep-Convolutional-Neural-Networks/image-20240422234147225.png" alt="实线代表ReLUs，虚线代表tanh"></p><p>（1）输入图像大小：224 * 224 * 3</p><p>（2）第一层卷积设置：卷积–&gt;ReLU–&gt;局部响应归一化（LRN）–&gt;池化</p><p>（3）第二层卷积：卷积–&gt;ReLU–&gt;局部响应归一化（LRN）–&gt;池化</p><p>（4）第三层卷积：卷积–&gt;ReLU</p><p>（5）第四层卷积：卷积–&gt;ReLU</p><p>（6）第五层卷积：卷积–&gt;ReLU–&gt;池化</p><p>（7）全连接层</p><p>（8）全连接层2</p><p>（9）输出层（全连接层3）</p><h3 id="降低过拟合所采用的方法"><a href="#降低过拟合所采用的方法" class="headerlink" title="降低过拟合所采用的方法"></a>降低过拟合所采用的方法</h3><h5 id="数据扩增"><a href="#数据扩增" class="headerlink" title="数据扩增"></a>数据扩增</h5><p>为了降低过拟合，提高模型的鲁棒性，这里采用了两种Data Augmentation数据扩增方式：<br>a.生成图像平移和水平反射。通过从256×256幅图像中提取随机224×224块图像(及其水平反射)，并在这些提取的图像上训练AlexNet。这将训练集的大小增加了2048倍。<br>b.改变训练图像中RGB通道的强度。在整个ImageNet训练集中对RGB像素值集执行PCA（Principal Component Analysis)[<a href="https://zhuanlan.zhihu.com/p/366337471#ref_5">5]</a>操作。</p><h5 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h5><p>训练采用了0.5丢弃率的传统Dropout，对于使用了Dropout的layer中的每个神经元，训练时都有50%的概率被丢弃。所以每次输入时，神经网络都会对不同的结构进行采样，但是所有这些结构都共享权重。这种技术减少了神经元之间复杂的相互适应，因为神经元不能依赖于其他神经元的存在，因此，它被迫获得更健壮的特征。测试时使用所有的神经元，但将它们的输出乘以0.5。 论文中还提到了：Dropout使收敛所需的迭代次数增加了一倍。</p><h4 id="实验-1"><a href="#实验-1" class="headerlink" title="实验"></a>实验</h4><p>batch size&#x3D;128，动量项v&#x3D;0.9，权值衰减(weight decay) wd&#x3D;0.0005，W服从均值为0、标准差为0.01的高斯分布。</p><p>偏置项：第2、4、5卷积层和全连接层的b&#x3D;1（促进最初阶段ReLU的学习）；其它层b&#x3D;0。</p><p>学习率：初始为0.01，当验证集的错误率停止降低时，手动缩减学习率（除以10）。</p><h4 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h4><p><img src="/../images/0-ImageNet-Classification-with-Deep-Convolutional-Neural-Networks/image-20240422234515097.png" alt="本文的算法错误率明显比前两个算法低"></p><p>最后结果top-1是67.4%，top-5是40.9%，比发布的最好的结果还要好。</p><p><img src="/../images/0-ImageNet-Classification-with-Deep-Convolutional-Neural-Networks/image-20240422234549188.png" alt="验证误差"></p><p><img src="/../images/0-ImageNet-Classification-with-Deep-Convolutional-Neural-Networks/image-20240422234630193.png" alt="定性分析"></p><p>左边部分，作者展示了8张图片的预测结果来说明网络在预测top-5时都从测试图片中学到了什么。右边部分则对比了测试集中的五张图片和在训练集中与之最相似的6张图片，如果两张图片产生的特征激活向量（即CNN的输出结果）的欧几里得距离小，就认为这两张图片相似。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>对于一个较大的数据集，给出了一种解决分类任务的方法，在当时取得了很重大的突破，<strong>AlexNet</strong>在深度学习</p><p>发展史上的<strong>历史意义远大于其模型的影响</strong>。卷积神经网络也成为计算机视觉的核心算法模型。</p><blockquote><p>如果我们今天回过头看看，将人工智能领域的蓬勃发展归功于某个事件的话，这份殊荣应属于2012年 ImageNet大赛的比赛成果。<br>2012年 ImageNet 的那场赛事的的确确引发了今天人工智能井喷式的发展。之前在语音识别领域是有一些成果，但大众并不知道，也不关心，而 ImageNet 让人工智能开始进入公众视野。</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>论文笔记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>0-Very Deep Convolutional Networks For Large-Scale Image Recognition</title>
    <link href="/2024/04/22/0-Very-Deep-Convolutional-Networks-For-Large-Scale-Image-Recognition/"/>
    <url>/2024/04/22/0-Very-Deep-Convolutional-Networks-For-Large-Scale-Image-Recognition/</url>
    
    <content type="html"><![CDATA[<p><img src="/../images/0-Very-Deep-Convolutional-Networks-For-Large-Scale-Image-Recognition/1713777253973.png" alt="VGG"></p><h2 id="用于大规模图像识别的超深卷积网络-2015-VGG"><a href="#用于大规模图像识别的超深卷积网络-2015-VGG" class="headerlink" title="用于大规模图像识别的超深卷积网络 2015 (VGG)"></a>用于大规模图像识别的超深卷积网络 2015 (VGG)</h2><blockquote><p>ImageNet Large-ScaleVisual Recognition Challenge (ILSVRC)：ImageNet大规模视觉识别挑战</p></blockquote><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>卷积核到底该设置为多少？AlexNet采用了极大的size(11x11)、ZFNet将size调小了但仍然使用到了7x7，GoogLeNet同时使用了不同的filter size…</p><p>本篇文章VGG使用3x3不断叠加，使得CNN模型可以达到更深的层数且得到更好的精准度。本方法其实是对于AlexNet的基础上做了更好的改进。VGG的模型架构如下图所示：</p><p><img src="/../images/0-Very-Deep-Convolutional-Networks-For-Large-Scale-Image-Recognition/1713778810157.png" alt="1713778810157"></p><p><img src="/../images/0-Very-Deep-Convolutional-Networks-For-Large-Scale-Image-Recognition/1713778871429.png" alt="1713778871429" title="参数数目"></p><p>作者根据配置进行了分析：7x7的卷积和3个3x3的卷积感受野实际上是一样的，那为什么要用小卷积来代替大卷积呢？</p><h3 id="实验方法"><a href="#实验方法" class="headerlink" title="实验方法"></a>实验方法</h3><h4 id="VGGNet-1x1卷积"><a href="#VGGNet-1x1卷积" class="headerlink" title="VGGNet 1x1卷积"></a>VGGNet 1x1卷积</h4><p>选用1x1卷积核的最直接原因是在维度上继承全连接，conv1x1更加专注于跨通道的特征组合，conv3x3既考虑跨通道，也考虑局部信息整合。使用1x1卷积也可以在3x3或5x5卷积计算前先降低feature map的维度。</p><h4 id="VGGNet-卷积核变小"><a href="#VGGNet-卷积核变小" class="headerlink" title="VGGNet 卷积核变小"></a>VGGNet 卷积核变小</h4><p>卷积核全部替换为3×3（极少用了1×1）步长为1，而使用小的卷积核可以提升性能，加深网络结构。</p><p><img src="/../images/0-Very-Deep-Convolutional-Networks-For-Large-Scale-Image-Recognition/1713779093966.png" alt="1713779093966"></p><h4 id="VGGNet-层数更深更宽"><a href="#VGGNet-层数更深更宽" class="headerlink" title="VGGNet 层数更深更宽"></a>VGGNet <strong>层数更深更宽</strong></h4><p>3个激活函数（ReLU）去代替1个，可使决策函数更加具有辨别能力；</p><p>3x3比5x5,7x7,11x11的Conv filter的参数减少，减少卷积数量带来性能提升。</p><h4 id="VGGNet-池化核变小且为偶数"><a href="#VGGNet-池化核变小且为偶数" class="headerlink" title="VGGNet 池化核变小且为偶数"></a>VGGNet <strong>池化核变小且为偶数</strong></h4><p>AlexNet中的max-pool全是3×3的，但VGGNet中是2×2的，可能的原因是2×2的max-pool带来的信息损失相对于3×3的来说要小一些，相比于3×3更容易捕获细小的特征变化起伏。在网络的层数增长的过程中，池化忽略的信息加上缓冲，并降低softmax的学习压力。</p><p>卷积只增加feature map的通道数，而池化只减少feature map的宽高。如今也有不少做法用大stride卷积去替代池化，未来可能没有池化。</p><h4 id="训练阶段"><a href="#训练阶段" class="headerlink" title="训练阶段"></a>训练阶段</h4><ul><li>优化方法：带动量（momentum）的小批量梯度下降</li><li>batch size：256</li><li>learning rate：0.01<br>和AlexNet一样，当val-acc 不下降则学习率缩小十倍，训练过程缩小了三次</li><li>momentum：0.9</li><li>weight decay（L2惩罚乘子）：0.0005</li><li>dropout rate（前两个全连接层）：0.5</li><li>目标函数：多项式逻辑斯特回归（SoftMax）</li><li>迭代次数：37万次iteration（74 epochs）后，停止训练</li></ul><h4 id="测试阶段"><a href="#测试阶段" class="headerlink" title="测试阶段"></a>测试阶段</h4><ul><li>测试图像的尺寸Q和训练图像的尺寸 S 没必要完全一样。</li><li>全连接层先转化为卷积层第一个全连接层转为7x7的卷积层，后两个转化为1x1的卷积层。</li><li>再将这样得到的全卷积网络运用在整幅图像上。</li><li>使用水平翻转对测试图像进行增强。</li></ul><h3 id="实验结论"><a href="#实验结论" class="headerlink" title="实验结论"></a>实验结论</h3><h5 id="单一尺寸上的卷积网络"><a href="#单一尺寸上的卷积网络" class="headerlink" title="单一尺寸上的卷积网络"></a>单一尺寸上的卷积网络</h5><p><img src="/../images/0-Very-Deep-Convolutional-Networks-For-Large-Scale-Image-Recognition/1713795644679.png" alt="1713795644679"></p><p>注意使用局部相应标准化网络（A-LRN）的性能并没有比未用标准化层的A高。</p><p>更大的数据集使用更深的模型会更好。小滤波器的卷积网络比大滤波器的千层网络性能更好。</p><h5 id="多尺寸上的卷积网络"><a href="#多尺寸上的卷积网络" class="headerlink" title="多尺寸上的卷积网络"></a>多尺寸上的卷积网络</h5><p><img src="/../images/0-Very-Deep-Convolutional-Networks-For-Large-Scale-Image-Recognition/1713795949526.png" alt="1713795949526"></p><p>测试时图片尺寸波动会使性能更好。</p><h5 id="多裁剪的评估"><a href="#多裁剪的评估" class="headerlink" title="多裁剪的评估"></a>多裁剪的评估</h5><p><img src="/../images/0-Very-Deep-Convolutional-Networks-For-Large-Scale-Image-Recognition/1713796032170.png" alt="1713796032170"></p><p>多重裁切比密集评估的效果好，并且两者互补。</p><h5 id="融合卷积网络"><a href="#融合卷积网络" class="headerlink" title="融合卷积网络"></a>融合卷积网络</h5><p><img src="/../images/0-Very-Deep-Convolutional-Networks-For-Large-Scale-Image-Recognition/1713796158321.png" alt="1713796158321"></p><p>将两个表现最好的多尺寸模型组合禁用，将会进一步减少错误率。</p><h5 id="比较"><a href="#比较" class="headerlink" title="比较"></a>比较</h5><p><img src="/../images/0-Very-Deep-Convolutional-Networks-For-Large-Scale-Image-Recognition/1713796323671.png" alt="1713796323671"></p><p>使用了7个模型组合的测试错误率，为7.3%，使用2个模型的组合，将错误率降低到了6.8%。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>VGG网络继承了AlexNet中的不少网络结构，同时继承了OverFeat在Localization任务中的做法，学习这种经典的网络应该可以对日后在Computer Vision领域的学习起到一定的作用，小卷积核的应用以及VGGNet的输入图像rescale应该是本论文中重点关注的点。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>论文笔记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Leetcode</title>
    <link href="/2024/03/06/Leetcode/"/>
    <url>/2024/03/06/Leetcode/</url>
    
    <content type="html"><![CDATA[<p class="note note-success">简单</p><p class="note note-warning">中等</p> <p class="note note-danger">困难</p><h2 id="2024-03-06简单"><a href="#2024-03-06简单" class="headerlink" title="2024-03-06简单"></a>2024-03-06<p class="note note-success">简单</p></h2><p><a href="https://leetcode.cn/problems/find-the-k-or-of-an-array/">2917. 找出数组中的 K-or 值</a></p><p>位运算问题，考虑到K-or数只看第i位的值是否为1，并且需要知道的仅仅是超过k值的数组中的数，使用O(n)来解决此问题</p><p>简单的位运算模拟。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span> &#123;<br><span class="hljs-keyword">public</span>:<br>    <span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">findKOr</span><span class="hljs-params">(vector&lt;<span class="hljs-type">int</span>&gt;&amp; nums, <span class="hljs-type">int</span> k)</span> </span>&#123;<br>        <span class="hljs-type">int</span> res = <span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">31</span>; ++i) &#123;<br>            <span class="hljs-type">int</span> cnt = <span class="hljs-number">0</span>;<br>            <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> num:nums) &#123;<br>                <span class="hljs-keyword">if</span>((num &gt;&gt; i) &amp; <span class="hljs-number">1</span>)&#123;<br>                    ++cnt;<br>                &#125;<br>            &#125;<br>            <span class="hljs-keyword">if</span> (cnt &gt;= k) &#123;<br>                res |= <span class="hljs-number">1</span> &lt;&lt; i;<br>            &#125;<br>        &#125;<br>        <span class="hljs-keyword">return</span> res;<br>    &#125;<br>&#125;;<br></code></pre></td></tr></table></figure><h2 id="2024-03-07中等"><a href="#2024-03-07中等" class="headerlink" title="2024-03-07中等"></a>2024-03-07<p class="note note-warning">中等</p></h2><p><a href="https://leetcode.cn/problems/find-the-divisibility-array-of-a-string/">2575. 找出字符串的可整除数组</a></p><p>考虑到 word 长达 1e5，故无法每一个数字取模确认，因为十进制数字退一位有着 10 个数一循环的特性，故一次取模不影响后面是否会被 m 整除，遂只需要 O(n)即可解决问题。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span> &#123;<br><span class="hljs-keyword">public</span>:<br>    <span class="hljs-function">vector&lt;<span class="hljs-type">int</span>&gt; <span class="hljs-title">divisibilityArray</span><span class="hljs-params">(string word, <span class="hljs-type">int</span> m)</span> </span>&#123;<br>        <span class="hljs-type">int</span> n=word.<span class="hljs-built_in">size</span>();<br>        <span class="hljs-function">vector&lt;<span class="hljs-type">int</span>&gt; <span class="hljs-title">res</span><span class="hljs-params">(n)</span></span>;<br>        <span class="hljs-type">long</span> cur=<span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">0</span>;i&lt;n;i++)&#123;<br>            cur = <span class="hljs-number">10</span>*cur+((<span class="hljs-type">int</span>)word[i]<span class="hljs-number">-48</span>);<br>            cur %= m;<br>            <span class="hljs-keyword">if</span>(cur==<span class="hljs-number">0</span>) res[i]=<span class="hljs-number">1</span>;<br>            <span class="hljs-keyword">else</span> res[i]=<span class="hljs-number">0</span>;<br>        &#125;<br>            <span class="hljs-keyword">return</span> res;<br>    &#125;<br>&#125;;<br></code></pre></td></tr></table></figure><p>如何更快？使用位运算可加快计算速度。</p><h2 id="2024-03-08中等"><a href="#2024-03-08中等" class="headerlink" title="2024-03-08中等"></a>2024-03-08<p class="note note-warning">中等</p></h2><p><a href="https://leetcode.cn/problems/find-the-minimum-possible-sum-of-a-beautiful-array/">2834. 找出美丽数组的最小和</a></p><p>考虑到一个数可分解为两个数相加，题意为数组中的“两个”数相加不得target，那么如果相等则向下延顺，比如9可分为1 + 8，2 + 7，3 + 6,4 + 5这四组不同的加法，而默认最小数组就是1到n这n个数字，则去除延顺后的最小数组就是[1,2,3,4,9,10,………,n]推出正常情况下的公示，再对1和target不影响默认最小数组的情况特殊判断即可。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span> &#123;<br><span class="hljs-keyword">public</span>:<br>    <span class="hljs-type">int</span> mod=<span class="hljs-number">1e9</span> + <span class="hljs-number">7</span>;<br>    <span class="hljs-function"><span class="hljs-type">long</span> <span class="hljs-type">long</span> <span class="hljs-title">a_sum</span><span class="hljs-params">(<span class="hljs-type">int</span> a,<span class="hljs-type">int</span> b,<span class="hljs-type">int</span> l)</span></span>&#123;<span class="hljs-keyword">if</span>(l==<span class="hljs-number">1</span>) <span class="hljs-keyword">return</span> a; <span class="hljs-keyword">return</span> (<span class="hljs-type">long</span> <span class="hljs-type">long</span>)(a+b)*l/<span class="hljs-number">2</span>;&#125;<br><br>    <span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">minimumPossibleSum</span><span class="hljs-params">(<span class="hljs-type">int</span> n, <span class="hljs-type">int</span> target)</span> </span>&#123;<br>        <span class="hljs-keyword">if</span>(n==<span class="hljs-number">1</span> <span class="hljs-keyword">or</span> target==<span class="hljs-number">1</span>) <span class="hljs-keyword">return</span> <span class="hljs-built_in">a_sum</span>(<span class="hljs-number">1</span>,n,n)%mod;<br>        <span class="hljs-type">long</span> <span class="hljs-type">long</span> pd= target/<span class="hljs-number">2</span>;<br>        <span class="hljs-keyword">if</span>(pd&gt;n) <span class="hljs-keyword">return</span> <span class="hljs-built_in">a_sum</span>(<span class="hljs-number">1</span>,n,n)%mod;<br>        <span class="hljs-type">long</span> <span class="hljs-type">long</span> n1=<span class="hljs-built_in">a_sum</span>(<span class="hljs-number">1</span>,target+n-pd<span class="hljs-number">-1</span>,target+n-pd<span class="hljs-number">-1</span>);<br>        <span class="hljs-type">long</span> <span class="hljs-type">long</span> n2=<span class="hljs-built_in">a_sum</span>(pd+<span class="hljs-number">1</span>,target<span class="hljs-number">-1</span>,target-pd<span class="hljs-number">-1</span>);<br>        <span class="hljs-keyword">return</span> (n1-n2)%mod;<br>    &#125;<br>&#125;;<br></code></pre></td></tr></table></figure><p><img src="/../images/Leetcode/image-20240314171703628.png" alt="time"></p><h2 id="2024-03-09困难"><a href="#2024-03-09困难" class="headerlink" title="2024-03-09困难"></a>2024-03-09<p class="note note-danger">困难</p></h2><p><a href="https://leetcode.cn/problems/find-the-k-sum-of-an-array/">2386. 找出数组的第 K 大和</a></p><p>一个有n个元素的数组有2^n个子数组，既然是找出第K大的子数组和，那么本题对于给出数组的排序并不敏感，关键是子数组的排序，既然求最大的子数组，那么所有正数相加就是第1个大的子数组，其他子数组就是减去一个最小的正数，或者加上一个最大的负数就是第二个最大子数组….再向后可能就是减去多个正数，加上多个负数….</p><p>那么问题转化为只需要求第K个最大子数组的值减去第一个最大子数组的值。使用优先队列的小顶堆维护即可：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span> &#123;<br><span class="hljs-keyword">public</span>:<br>    <span class="hljs-function"><span class="hljs-type">long</span> <span class="hljs-type">long</span> <span class="hljs-title">kSum</span><span class="hljs-params">(vector&lt;<span class="hljs-type">int</span>&gt;&amp; nums, <span class="hljs-type">int</span> k)</span> </span>&#123;<br>        <span class="hljs-type">long</span> <span class="hljs-type">long</span> s=<span class="hljs-number">0</span>;<br>        <span class="hljs-type">int</span> n=nums.<span class="hljs-built_in">size</span>();<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> &amp;x:nums)&#123;<br>            <span class="hljs-keyword">if</span>(x&gt;=<span class="hljs-number">0</span>) s+=x;<br>            <span class="hljs-keyword">else</span> x=-x;<br>        &#125;<br>        <span class="hljs-built_in">sort</span>(nums.<span class="hljs-built_in">begin</span>(),nums.<span class="hljs-built_in">end</span>());<br>        priority_queue&lt;pair&lt;<span class="hljs-type">long</span> <span class="hljs-type">long</span>,<span class="hljs-type">int</span>&gt;,vector&lt;pair&lt;<span class="hljs-type">long</span> <span class="hljs-type">long</span>,<span class="hljs-type">int</span>&gt;&gt;,greater&lt;&gt;&gt;q;<br>        q.<span class="hljs-built_in">push</span>(&#123;<span class="hljs-number">0</span>,<span class="hljs-number">0</span>&#125;);<br>        <span class="hljs-keyword">while</span> (--k)<br>        &#123;<br>            <span class="hljs-keyword">auto</span> ts=q.<span class="hljs-built_in">top</span>();q.<span class="hljs-built_in">pop</span>();<br>            <span class="hljs-keyword">if</span>(ts.second&gt;=n) <span class="hljs-keyword">continue</span>;<br>            q.<span class="hljs-built_in">push</span>(&#123;ts.first+nums[ts.second],ts.second+<span class="hljs-number">1</span>&#125;);<br>            <span class="hljs-keyword">if</span>(ts.second) q.<span class="hljs-built_in">push</span>(&#123;ts.first-nums[ts.second<span class="hljs-number">-1</span>]+nums[ts.second],ts.second+<span class="hljs-number">1</span>&#125;);<br>        &#125;<br>        <span class="hljs-keyword">return</span> s-q.<span class="hljs-built_in">top</span>().first;<br>    &#125;<br>&#125;;<br></code></pre></td></tr></table></figure><h2 id="2024-03-10中等"><a href="#2024-03-10中等" class="headerlink" title="2024-03-10中等"></a>2024-03-10<p class="note note-warning">中等</p></h2><p><a href="https://leetcode.cn/problems/bulls-and-cows/">299. 猜数字游戏</a></p><p>使用键值对可以轻松解决此问题。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span> &#123;<br><span class="hljs-keyword">public</span>:<br>    <span class="hljs-function">string <span class="hljs-title">getHint</span><span class="hljs-params">(string secret, string guess)</span> </span>&#123;<br>        unordered_map&lt;<span class="hljs-type">char</span>,<span class="hljs-type">int</span>&gt; mp1,mp2;<br>        <span class="hljs-type">int</span> n=secret.<span class="hljs-built_in">size</span>();<br>        <span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">bulls</span><span class="hljs-params">(<span class="hljs-number">0</span>)</span>,<span class="hljs-title">cows</span><span class="hljs-params">(<span class="hljs-number">0</span>)</span></span>;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">0</span>;i&lt;n;i++)&#123;<br>            <span class="hljs-keyword">if</span>(secret[i]==guess[i])&#123;bulls++;<span class="hljs-keyword">continue</span>;&#125;<br>            mp1[secret[i]]++,mp2[guess[i]]++;<br>        &#125;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-keyword">auto</span> &amp;it:mp1)<br>            <span class="hljs-keyword">if</span>(mp2[it.first]) cows+=<span class="hljs-built_in">min</span>(it.second,mp2[it.first]);<br>        string ans=<span class="hljs-built_in">to_string</span>(bulls)+<span class="hljs-string">&quot;A&quot;</span>+<span class="hljs-built_in">to_string</span>(cows)+<span class="hljs-string">&quot;B&quot;</span>;<br>        <span class="hljs-keyword">return</span> ans;<br>    &#125;<br>&#125;;<br></code></pre></td></tr></table></figure><h2 id="2024-03-11简单"><a href="#2024-03-11简单" class="headerlink" title="2024-03-11简单"></a>2024-03-11<p class="note note-success">简单</p></h2><p><a href="https://leetcode.cn/problems/capitalize-the-title/">2129. 将标题首字母大写</a></p><p>使用字符流容易解决，还可以避免指针越界。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span> &#123;<br><span class="hljs-keyword">public</span>:<br>    <span class="hljs-function">string <span class="hljs-title">capitalizeTitle</span><span class="hljs-params">(string title)</span> </span>&#123;<br>        <span class="hljs-type">int</span> n=title.<span class="hljs-built_in">size</span>();<br>        string ans=<span class="hljs-string">&quot;&quot;</span>;<br>        <span class="hljs-function">stringstream <span class="hljs-title">ss</span><span class="hljs-params">(title)</span></span>;<br>        string str;<br>        <span class="hljs-keyword">while</span>(ss&gt;&gt;str)&#123;<br>            <span class="hljs-keyword">if</span>(str.<span class="hljs-built_in">size</span>()&lt;=<span class="hljs-number">2</span>)&#123;<br>                ans+=<span class="hljs-built_in">tolower</span>(str[<span class="hljs-number">0</span>]);<br>                <span class="hljs-keyword">if</span>(str.<span class="hljs-built_in">size</span>()==<span class="hljs-number">2</span>) ans+=<span class="hljs-built_in">tolower</span>(str[<span class="hljs-number">1</span>]); <br>            &#125;<span class="hljs-keyword">else</span>&#123;<br>                <span class="hljs-type">int</span> nn=str.<span class="hljs-built_in">size</span>();<br>                ans+=<span class="hljs-built_in">toupper</span>(str[<span class="hljs-number">0</span>]);<br>                <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">1</span>;i&lt;nn;i++)&#123;<br>                    ans+=<span class="hljs-built_in">tolower</span>(str[i]);<br>                &#125;<br>            &#125;<br>            ans+=<span class="hljs-string">&quot; &quot;</span>;<br>        &#125;<br>        ans.<span class="hljs-built_in">pop_back</span>();<br>        <span class="hljs-keyword">return</span> ans;<br>    &#125;<br>&#125;;<br></code></pre></td></tr></table></figure><h2 id="2024-03-12中等"><a href="#2024-03-12中等" class="headerlink" title="2024-03-12中等"></a>2024-03-12<p class="note note-warning">中等</p></h2><p><a href="https://leetcode.cn/problems/find-elements-in-a-contaminated-binary-tree/">1261. 在受污染的二叉树中查找元素</a></p><p>二叉树….</p><h2 id="2024-03-13简单"><a href="#2024-03-13简单" class="headerlink" title="2024-03-13简单"></a>2024-03-13<p class="note note-success">简单</p></h2><p><a href="https://leetcode.cn/problems/maximum-odd-binary-number/">2864. 最大二进制奇数</a></p><p>贪心</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span> &#123;<br><span class="hljs-keyword">public</span>:<br>    <span class="hljs-function">string <span class="hljs-title">maximumOddBinaryNumber</span><span class="hljs-params">(string s)</span> </span>&#123;<br>        <span class="hljs-type">int</span> n=s.<span class="hljs-built_in">size</span>();<br>        <span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">n0</span><span class="hljs-params">(<span class="hljs-number">0</span>)</span>,<span class="hljs-title">n1</span><span class="hljs-params">(<span class="hljs-number">0</span>)</span></span>;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">0</span>;i&lt;n;i++)<br>            <span class="hljs-keyword">if</span>(s[i]==<span class="hljs-string">&#x27;0&#x27;</span>) n0++;<span class="hljs-keyword">else</span> n1++;<br>        string ans=<span class="hljs-string">&quot;&quot;</span>;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">0</span>;i&lt;n1<span class="hljs-number">-1</span>;i++) ans+=<span class="hljs-string">&#x27;1&#x27;</span>;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=n1<span class="hljs-number">-1</span>;i&lt;n<span class="hljs-number">-1</span>;i++) ans+=<span class="hljs-string">&#x27;0&#x27;</span>;<br>        ans+=<span class="hljs-string">&#x27;1&#x27;</span>;<br>        <span class="hljs-keyword">return</span> ans;<br>    &#125;<br>&#125;;<br></code></pre></td></tr></table></figure><h2 id="2024-03-14中等"><a href="#2024-03-14中等" class="headerlink" title="2024-03-14中等"></a>2024-03-14<p class="note note-warning">中等</p></h2><p><a href="https://leetcode.cn/problems/largest-element-in-an-array-after-merge-operations/">2789. 合并后数组中的最大元素</a></p><p>一开始想如果两对两对的看成一个树的结果，维护一个最大的根，后来发现既然只有两个相邻的数，那么直接从后往前加在一块，在不符合规则前取最大的那个不就行了，后来有发现既然他不符合规则，那么后面遍历完的数据就必不可能是答案喽，还是比较容易解决的。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span> &#123;<br><span class="hljs-keyword">public</span>:<br>    <span class="hljs-function"><span class="hljs-type">long</span> <span class="hljs-type">long</span> <span class="hljs-title">maxArrayValue</span><span class="hljs-params">(vector&lt;<span class="hljs-type">int</span>&gt;&amp; nums)</span> </span>&#123;<br>        <span class="hljs-type">int</span> n=nums.<span class="hljs-built_in">size</span>();<br>        <span class="hljs-type">long</span> <span class="hljs-type">long</span> ans=nums[n<span class="hljs-number">-1</span>];<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=n<span class="hljs-number">-2</span>;i&gt;=<span class="hljs-number">0</span>;i--)&#123;<br>            <span class="hljs-keyword">if</span>(nums[i]&lt;=ans)&#123;<br>                ans=(<span class="hljs-type">long</span> <span class="hljs-type">long</span>)ans+(<span class="hljs-type">long</span> <span class="hljs-type">long</span>)nums[i];<br>            &#125;<span class="hljs-keyword">else</span>&#123;<br>                ans=(<span class="hljs-type">long</span> <span class="hljs-type">long</span>)nums[i];<br>            &#125;<br>        &#125;<br>        <span class="hljs-keyword">return</span> ans;<br>    &#125;<br>&#125;;<br></code></pre></td></tr></table></figure><h2 id="2024-03-15困难"><a href="#2024-03-15困难" class="headerlink" title="2024-03-15困难"></a>2024-03-15<p class="note note-danger">困难</p></h2><p><a href="https://leetcode.cn/problems/selling-pieces-of-wood/">2312. 卖木头块</a></p><p>求出卖出方案的最大值，可以确定是DP，题目中说明切割一次只能完全切割，那么如何正确的遍历切割方案和递推公式就是本题的难点。</p><p>观察到一个M*N的矩形有2种切割方法，首先是切割成（M1+M2）*N，或者是M*(N1+N2)，其中会存在相同的M*N，如果将其记录则优化时间，使用二维数组更新切割的最大值，创建DP。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span> &#123;<br><span class="hljs-keyword">public</span>:<br>    <span class="hljs-keyword">typedef</span> <span class="hljs-type">long</span> <span class="hljs-type">long</span> ll;<br>    <span class="hljs-function"><span class="hljs-type">long</span> <span class="hljs-type">long</span> <span class="hljs-title">sellingWood</span><span class="hljs-params">(<span class="hljs-type">int</span> m, <span class="hljs-type">int</span> n, vector&lt;vector&lt;<span class="hljs-type">int</span>&gt;&gt;&amp; prices)</span> </span>&#123;<br>        vector&lt;vector&lt;<span class="hljs-type">int</span>&gt;&gt; <span class="hljs-built_in">mp</span>(m+<span class="hljs-number">1</span>,<span class="hljs-built_in">vector</span>&lt;<span class="hljs-type">int</span>&gt; (n+<span class="hljs-number">1</span>,<span class="hljs-number">0</span>));<br>        <span class="hljs-keyword">for</span>(<span class="hljs-keyword">auto</span> &amp;p:prices) mp[p[<span class="hljs-number">0</span>]][p[<span class="hljs-number">1</span>]]=p[<span class="hljs-number">2</span>];<br>        vector&lt;vector&lt;ll&gt;&gt; <span class="hljs-built_in">dp</span>(m+<span class="hljs-number">1</span>,<span class="hljs-built_in">vector</span>&lt;ll&gt; (n+<span class="hljs-number">1</span>,<span class="hljs-number">-1</span>));<br><br>        function&lt;<span class="hljs-type">void</span>(<span class="hljs-type">int</span>,<span class="hljs-type">int</span>)&gt; dfs = [&amp;](<span class="hljs-type">int</span> i,<span class="hljs-type">int</span> j)&#123;<br>            <span class="hljs-keyword">if</span>(dp[i][j] != <span class="hljs-number">-1</span>) <span class="hljs-keyword">return</span>;<br>            dp[i][j]=mp[i][j];<br>            <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> x=i/<span class="hljs-number">2</span>;x&gt;<span class="hljs-number">0</span>;x--)&#123;<br>                <span class="hljs-built_in">dfs</span>(x,j);<br>                <span class="hljs-built_in">dfs</span>(i-x,j);<br>                dp[i][j] = <span class="hljs-built_in">max</span>(dp[i][j],dp[x][j]+dp[i-x][j]);<br>            &#125;<br>            <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> y=j/<span class="hljs-number">2</span>;y&gt;<span class="hljs-number">0</span>;y--)&#123;<br>                <span class="hljs-built_in">dfs</span>(i,y);<br>                <span class="hljs-built_in">dfs</span>(i,j-y);<br>                dp[i][j] = <span class="hljs-built_in">max</span>(dp[i][j],dp[i][y]+dp[i][j-y]);<br>            &#125;<br>        &#125;;<br>        <span class="hljs-built_in">dfs</span>(m,n);<br>        <span class="hljs-keyword">return</span> dp[m][n];<br><br>    &#125;<br>&#125;;<br><br><br><span class="hljs-comment">//后来在speed rank里看到比我快8倍的代码，真是又快又好，很值得学习</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span> &#123;<br><span class="hljs-keyword">public</span>:<br>    <span class="hljs-type">long</span> <span class="hljs-type">long</span> dp[<span class="hljs-number">201</span>][<span class="hljs-number">201</span>];<br>    <span class="hljs-function"><span class="hljs-type">long</span> <span class="hljs-type">long</span> <span class="hljs-title">sellingWood</span><span class="hljs-params">(<span class="hljs-type">int</span> m, <span class="hljs-type">int</span> n, vector&lt;vector&lt;<span class="hljs-type">int</span>&gt;&gt;&amp; prices)</span> </span>&#123;<br>        <span class="hljs-type">int</span> i,j,k;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">auto</span> &amp;x : prices) &#123;<br>            dp[x[<span class="hljs-number">0</span>]][x[<span class="hljs-number">1</span>]] = x[<span class="hljs-number">2</span>];<br>        &#125;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>; i &lt;= m; i++) &#123;<br>            <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> j = <span class="hljs-number">1</span>; j &lt;= n; j++) &#123;<br>                <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> k = <span class="hljs-number">1</span>; k &lt;= j/<span class="hljs-number">2</span>; k++) dp[i][j] = <span class="hljs-built_in">max</span>(dp[i][j], dp[i][k] + dp[i][j - k]); <span class="hljs-comment">// 垂直切割</span><br>                <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> k = <span class="hljs-number">1</span>; k &lt;= i/<span class="hljs-number">2</span>; k++) dp[i][j] = <span class="hljs-built_in">max</span>(dp[i][j], dp[k][j] + dp[i - k][j]); <span class="hljs-comment">// 水平切割</span><br>            &#125;<br>        &#125;<br>        <span class="hljs-keyword">return</span> dp[m][n];<br>    &#125;<br>&#125;;<br><br></code></pre></td></tr></table></figure><h2 id="2024-03-16"><a href="#2024-03-16" class="headerlink" title="2024-03-16"></a>2024-03-16</h2><h2 id="2024-03-17"><a href="#2024-03-17" class="headerlink" title="2024-03-17"></a>2024-03-17</h2><h2 id="2024-03-18"><a href="#2024-03-18" class="headerlink" title="2024-03-18"></a>2024-03-18</h2><h2 id="2024-03-19"><a href="#2024-03-19" class="headerlink" title="2024-03-19"></a>2024-03-19</h2><h2 id="2024-03-20中等"><a href="#2024-03-20中等" class="headerlink" title="2024-03-20中等"></a>2024-03-20<p class="note note-warning">中等</p></h2><p><a href="https://leetcode.cn/problems/minimum-non-zero-product-of-the-array-elements/">1969. 数组元素的最小非零乘积</a></p><p>​基于贪心算法，优先将小的变小，大的变大，最后数组就会成为[0,0,0,0,0,……..,2^p -1,2^p -1,…..]，因为要求数组非零，所以再将后面2^p -1的最后一个1补到前面即可，但是不知道为什么，pow（2，p）的效果居然和(ll)1&lt;&lt;(p-1)不一样，这点还有待考量。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span> &#123;<br><span class="hljs-keyword">public</span>:<br>    <span class="hljs-keyword">typedef</span> <span class="hljs-type">long</span> <span class="hljs-type">long</span> ll;<br>    ll mod=<span class="hljs-number">1e9</span>+<span class="hljs-number">7</span>;<br><span class="hljs-function"><span class="hljs-type">long</span> <span class="hljs-type">long</span> <span class="hljs-title">my_pow</span><span class="hljs-params">(<span class="hljs-type">long</span> <span class="hljs-type">long</span> x, <span class="hljs-type">long</span> <span class="hljs-type">long</span> n)</span> </span>&#123;<br>        <span class="hljs-type">long</span> <span class="hljs-type">long</span> res = <span class="hljs-number">1</span>;<br>        <span class="hljs-keyword">for</span> (; n != <span class="hljs-number">0</span>; n &gt;&gt;= <span class="hljs-number">1</span>) &#123;<br>            <span class="hljs-keyword">if</span> (n &amp; <span class="hljs-number">1</span>) &#123;<br>                res = res * x % mod;<br>            &#125;<br>            x = x * x % mod;<br>        &#125;<br>        <span class="hljs-keyword">return</span> res;<br>    &#125;<br><br>    <span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">minNonZeroProduct</span><span class="hljs-params">(<span class="hljs-type">int</span> p)</span> </span>&#123;<br>        <span class="hljs-keyword">if</span>(p==<span class="hljs-number">1</span>) <span class="hljs-keyword">return</span> p;<br>        ll a=<span class="hljs-built_in">my_pow</span>((ll)<span class="hljs-number">2</span>,p)%mod;<span class="hljs-comment">//2^p</span><br><br>        <span class="hljs-comment">// ll y=my_pow(a-(ll)2,a/(ll)2-(ll)1)%mod;</span><br>        <span class="hljs-comment">// ll x=a-(ll)1;</span><br>        <span class="hljs-comment">// x%=mod;</span><br>        <span class="hljs-comment">// ll ans=x*y%mod;</span><br>        <span class="hljs-comment">// return ans%mod;</span><br>        ll x=<span class="hljs-built_in">my_pow</span>(<span class="hljs-number">2</span>,p)<span class="hljs-number">-1</span>;<br>        ll y=(ll)<span class="hljs-number">1</span>&lt;&lt;(p<span class="hljs-number">-1</span>);<br>        <span class="hljs-comment">// ll y=my_pow((ll)2,p-(ll)1);</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">my_pow</span>(x<span class="hljs-number">-1</span>,y<span class="hljs-number">-1</span>)*x%mod;<br>    &#125;<br>&#125;;<br><span class="hljs-comment">/*</span><br><span class="hljs-comment">p=2</span><br><span class="hljs-comment">1 2 3</span><br><span class="hljs-comment">1 2 3</span><br><span class="hljs-comment"></span><br><span class="hljs-comment">p=3</span><br><span class="hljs-comment">1 2 3 4 5 6 7</span><br><span class="hljs-comment"> +4 -2+2-4</span><br><span class="hljs-comment">1 6 1 6 1 6 7</span><br><span class="hljs-comment"></span><br><span class="hljs-comment">p=4</span><br><span class="hljs-comment">1 2 3 4 5 6 7 8 9 10 11 12 13 14 15</span><br><span class="hljs-comment"> +8+4  -4       -8            </span><br><span class="hljs-comment"> 1     2    3    4   5    6    7    8     9    10  11   12   13   14   15</span><br><span class="hljs-comment">0001 0010 0011 0100 0101 0110 0111 1000 1001 1010 1011 1100 1101 1110 1111</span><br><span class="hljs-comment">0001 1010 0011 0100 0101 0110 0111 1000 0001 1010 1011 1100 1101 1110 1111</span><br><span class="hljs-comment">0001 1010 0111 0100 0001 0110 0111 1000 0001 1010 1011 1100 1101 1110 1111</span><br><span class="hljs-comment">*/</span><br></code></pre></td></tr></table></figure><h2 id="2024-03-21中等"><a href="#2024-03-21中等" class="headerlink" title="2024-03-21中等"></a>2024-03-21<p class="note note-warning">中等</p></h2><p><a href="https://leetcode.cn/problems/frequency-tracker/">2671. 频率跟踪器</a></p><p>第一眼看见直接用map就解决了吗？看到查询次数可能很多，那就动态记录一下num的存量。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">class</span> <span class="hljs-title class_">FrequencyTracker</span> &#123;<br><span class="hljs-keyword">public</span>:<br>    unordered_map&lt;<span class="hljs-type">int</span>,<span class="hljs-type">int</span>&gt; mp;<br>    unordered_map&lt;<span class="hljs-type">int</span>,<span class="hljs-type">int</span>&gt; cnt;<br><br>    <span class="hljs-built_in">FrequencyTracker</span>() &#123;<br>        mp.<span class="hljs-built_in">clear</span>();<br>        cnt.<span class="hljs-built_in">clear</span>();<br>    &#125;<br>    <br>    <span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">add</span><span class="hljs-params">(<span class="hljs-type">int</span> number)</span> </span>&#123;<br>        mp[number]++;<br>        cnt[mp[number]<span class="hljs-number">-1</span>]--;<br>        cnt[mp[number]]++;<br>    &#125;<br>    <br>    <span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">deleteOne</span><span class="hljs-params">(<span class="hljs-type">int</span> number)</span> </span>&#123;<br>        <span class="hljs-keyword">if</span>(mp[number]&gt;<span class="hljs-number">0</span>)&#123;<br>            mp[number]--;<br>            cnt[mp[number]+<span class="hljs-number">1</span>]--;<br>            cnt[mp[number]]++;<br><br>        &#125;<br>    &#125;<br>    <br>    <span class="hljs-function"><span class="hljs-type">bool</span> <span class="hljs-title">hasFrequency</span><span class="hljs-params">(<span class="hljs-type">int</span> frequency)</span> </span>&#123;<br>        <span class="hljs-keyword">if</span>(cnt[frequency]&gt;<span class="hljs-number">0</span>) <span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>;<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>;<br>    &#125;<br>&#125;;<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * Your FrequencyTracker object will be instantiated and called as such:</span><br><span class="hljs-comment"> * FrequencyTracker* obj = new FrequencyTracker();</span><br><span class="hljs-comment"> * obj-&gt;add(number);</span><br><span class="hljs-comment"> * obj-&gt;deleteOne(number);</span><br><span class="hljs-comment"> * bool param_3 = obj-&gt;hasFrequency(frequency);</span><br><span class="hljs-comment"> */</span><br></code></pre></td></tr></table></figure><h2 id="2024-03-22"><a href="#2024-03-22" class="headerlink" title="2024-03-22"></a>2024-03-22</h2><h2 id="第-33-次-CCF-CSP-认证考试总结"><a href="#第-33-次-CCF-CSP-认证考试总结" class="headerlink" title="第 33 次 CCF CSP 认证考试总结"></a>第 33 次 CCF CSP 认证考试总结</h2><p>第一题和第二题都很简单，不到 20 分钟就敲完了，全程使用 STL。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;bits/stdc++.h&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-meta">#<span class="hljs-keyword">define</span> closeSync            \</span><br><span class="hljs-meta">    ios::sync_with_stdio(0); \</span><br><span class="hljs-meta">    cin.tie(0);              \</span><br><span class="hljs-meta">    cout.tie(0)</span><br><br>map&lt;<span class="hljs-type">int</span>, <span class="hljs-type">int</span>&gt; mp1;<br>unordered_set&lt;<span class="hljs-type">int</span>&gt; st1;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span><br><span class="hljs-function"></span>&#123;<br>    closeSync;<br>    <span class="hljs-type">int</span> n, m;<br>    cin &gt;&gt; n &gt;&gt; m;<br>    <span class="hljs-function">vector&lt;<span class="hljs-type">int</span>&gt; <span class="hljs-title">v</span><span class="hljs-params">(m + <span class="hljs-number">1</span>, <span class="hljs-number">0</span>)</span></span>;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; n; i++)<br>    &#123;<br>        <span class="hljs-type">int</span> t;<br>        cin &gt;&gt; t;<br>        unordered_set&lt;<span class="hljs-type">int</span>&gt; st;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> j = <span class="hljs-number">0</span>; j &lt; t; j++)<br>        &#123;<br>            <span class="hljs-type">int</span> word;<br>            cin &gt;&gt; word;<br>            mp1[word]++;<br>            st.<span class="hljs-built_in">insert</span>(word);<br>        &#125;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">auto</span> &amp;it : st)<br>        &#123;<br>            v[it]++;<br>        &#125;<br>    &#125;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>; i &lt;= m; i++)<br>    &#123;<br>        cout &lt;&lt; v[i] &lt;&lt; <span class="hljs-string">&quot; &quot;</span> &lt;&lt; mp1[i] &lt;&lt; <span class="hljs-string">&#x27;\n&#x27;</span>;<br>    &#125;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;bits/stdc++.h&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-meta">#<span class="hljs-keyword">define</span> closeSync            \</span><br><span class="hljs-meta">    ios::sync_with_stdio(0); \</span><br><span class="hljs-meta">    cin.tie(0);              \</span><br><span class="hljs-meta">    cout.tie(0)</span><br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span><br><span class="hljs-function"></span>&#123;<br>    closeSync;<br>    <span class="hljs-type">int</span> n, m;<br>    cin &gt;&gt; n &gt;&gt; m;<br>    unordered_set&lt;string&gt; st1;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; n; i++)<br>    &#123;<br>        string s;<br>        cin &gt;&gt; s;<br>        <span class="hljs-type">int</span> sz = s.<span class="hljs-built_in">size</span>();<br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> j = <span class="hljs-number">0</span>; j &lt; sz; j++)<br>        &#123;<br>            <span class="hljs-keyword">if</span> (s[j] &gt;= <span class="hljs-string">&#x27;A&#x27;</span> <span class="hljs-keyword">and</span> s[j] &lt;= <span class="hljs-string">&#x27;Z&#x27;</span>)<br>            &#123;<br>                s[j] += <span class="hljs-number">32</span>;<br>            &#125;<br>        &#125;<br>        st1.<span class="hljs-built_in">insert</span>(s);<br>        <span class="hljs-comment">//    cout &lt;&lt; s &lt;&lt; &quot; &quot;;</span><br>    &#125;<br>    unordered_set&lt;string&gt; st2;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; m; i++)<br>    &#123;<br>        string s;<br>        cin &gt;&gt; s;<br>        <span class="hljs-type">int</span> sz = s.<span class="hljs-built_in">size</span>();<br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> j = <span class="hljs-number">0</span>; j &lt; sz; j++)<br>        &#123;<br>            <span class="hljs-keyword">if</span> (s[j] &gt;= <span class="hljs-string">&#x27;A&#x27;</span> <span class="hljs-keyword">and</span> s[j] &lt;= <span class="hljs-string">&#x27;Z&#x27;</span>)<br>            &#123;<br>                s[j] += <span class="hljs-number">32</span>;<br>            &#125;<br>        &#125;<br>        st2.<span class="hljs-built_in">insert</span>(s);<br>        <span class="hljs-comment">//  cout &lt;&lt; s &lt;&lt; &quot; &quot;;</span><br>    &#125;<br>    unordered_set&lt;string&gt; st3;<br>    <span class="hljs-type">int</span> n1 = st1.<span class="hljs-built_in">size</span>();<br>    <span class="hljs-type">int</span> n2 = st2.<span class="hljs-built_in">size</span>();<br>    <span class="hljs-comment">// U</span><br>    <span class="hljs-built_in">set_union</span>(st1.<span class="hljs-built_in">begin</span>(), st1.<span class="hljs-built_in">end</span>(), st2.<span class="hljs-built_in">begin</span>(), st2.<span class="hljs-built_in">end</span>(), <span class="hljs-built_in">inserter</span>(st3, st3.<span class="hljs-built_in">begin</span>()));<br>    <span class="hljs-type">int</span> onaji = st3.<span class="hljs-built_in">size</span>();<br>    cout &lt;&lt; n1 + n2 - onaji &lt;&lt; <span class="hljs-string">&#x27;\n&#x27;</span>;<br>    cout &lt;&lt; onaji &lt;&lt; <span class="hljs-string">&#x27;\n&#x27;</span>;<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>第三题是对矩阵求秩，有时候想把代码写的好看一点，可是这反而浪费了很多时间，再加上找不到 bug 在哪，让我一度又推翻重写的想法，大概 2 个小时才修改完。</p><p>（可惜我没有高斯消元板子，只能现场手搓）</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;bits/stdc++.h&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-meta">#<span class="hljs-keyword">define</span> closeSync            \</span><br><span class="hljs-meta">    ios::sync_with_stdio(0); \</span><br><span class="hljs-meta">    cin.tie(0);              \</span><br><span class="hljs-meta">    cout.tie(0)</span><br><br><span class="hljs-type">int</span> q, n;<br>vector&lt;vector&lt;<span class="hljs-type">int</span>&gt;&gt; <span class="hljs-built_in">a</span>(<span class="hljs-number">52</span>, <span class="hljs-built_in">vector</span>&lt;<span class="hljs-type">int</span>&gt;(<span class="hljs-number">52</span>, <span class="hljs-number">0</span>));<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">scan</span><span class="hljs-params">(<span class="hljs-type">int</span> ssum)</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-type">int</span> r;<br>    <span class="hljs-type">int</span> cnt = <span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>; i &lt;= ssum; i++)<br>    &#123;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> j = <span class="hljs-number">1</span>; j &lt;= n; j++)<br>        &#123;<br>            <span class="hljs-keyword">if</span> (a[i][j] != <span class="hljs-number">0</span>)<br>            &#123;<br>                <span class="hljs-keyword">break</span>;<br>            &#125;<br>            <span class="hljs-keyword">else</span><br>                cnt++;<br>        &#125;<br>        <span class="hljs-keyword">if</span> (cnt == n)<br>        &#123;<br>            <span class="hljs-keyword">return</span> i - <span class="hljs-number">1</span>;<br>        &#125;<br>        cnt = <span class="hljs-number">0</span>;<br>    &#125;<br>    <span class="hljs-keyword">return</span> ssum;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">gcd</span><span class="hljs-params">(<span class="hljs-type">int</span> a, <span class="hljs-type">int</span> b)</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-keyword">return</span> b == <span class="hljs-number">0</span> ? a : <span class="hljs-built_in">gcd</span>(b, a % b);<br>&#125;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">lcm</span><span class="hljs-params">(<span class="hljs-type">int</span> a, <span class="hljs-type">int</span> b)</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-keyword">return</span> a / <span class="hljs-built_in">gcd</span>(a, b) * b;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-type">int</span> ssum)</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>; i &lt;= ssum; i++)<br>    &#123;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> j = <span class="hljs-number">1</span>; j &lt;= n; j++)<br>        &#123;<br>            cout &lt;&lt; a[i][j] &lt;&lt; <span class="hljs-string">&quot; &quot;</span>;<br>        &#125;<br>        cout &lt;&lt; endl;<br>    &#125;<br>    cout &lt;&lt; endl;<br>&#125;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">solve</span><span class="hljs-params">(<span class="hljs-type">int</span> ssum)</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-comment">// print(ssum);</span><br>    <span class="hljs-type">int</span> l = <span class="hljs-number">1</span>;<br><br>    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>; i &lt;= ssum; i++)<br>    &#123;<br>        <span class="hljs-type">int</span> pd0 = <span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> j = <span class="hljs-number">1</span>; j &lt;= n; j++)<br>        &#123;<br>            pd0 += a[i][j];<br>        &#125;<br>        <span class="hljs-keyword">if</span> (pd0 == <span class="hljs-number">0</span>)<br>        &#123;<br>            l++;<br>            <span class="hljs-keyword">continue</span>;<br>        &#125;<br>        <span class="hljs-keyword">if</span> (a[i][l] == <span class="hljs-number">0</span>)<br>        &#123;<br>            <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> k = i + <span class="hljs-number">1</span>; k &lt;= ssum; k++)<br>            &#123;<br>                <span class="hljs-keyword">if</span> (a[k][l] != <span class="hljs-number">0</span>)<br>                &#123;<br>                    <span class="hljs-built_in">swap</span>(a[i], a[k]);<br>                    <span class="hljs-keyword">break</span>;<br>                &#125;<br>            &#125;<br>        &#125;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> k = i + <span class="hljs-number">1</span>; k &lt;= ssum; k++)<br>        &#123;<br>            <span class="hljs-type">int</span> aa = a[i][l], bb = a[k][l];<br>            <span class="hljs-keyword">if</span> (bb == <span class="hljs-number">0</span>)<br>                <span class="hljs-keyword">continue</span>;<br><br>            <span class="hljs-type">int</span> beishu1 = <span class="hljs-built_in">lcm</span>(aa, bb);<br>            <span class="hljs-comment">// cout &lt;&lt; beishu1 &lt;&lt; endl;</span><br>            <span class="hljs-type">int</span> beishu2 = bb * beishu1 / aa;<br>            <span class="hljs-comment">// cout &lt;&lt; bb &lt;&lt; &quot;aa/a &quot; &lt;&lt; aa &lt;&lt; endl;</span><br>            <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> ll = l; ll &lt;= n; ll++)<br>            &#123;<br>                a[k][ll] *= beishu1;<br>                <span class="hljs-comment">// cout &lt;&lt; &quot;a[k][ll]前 =&quot; &lt;&lt; a[k][ll] &lt;&lt; endl;</span><br>                a[k][ll] = a[k][ll] - beishu2 * a[i][ll];<br>                <span class="hljs-comment">// cout &lt;&lt; &quot;a[k][ll]后 =&quot; &lt;&lt; a[k][ll] &lt;&lt; endl;</span><br>            &#125;<br>        &#125;<br>        <span class="hljs-comment">// print(ssum);</span><br><br>        <span class="hljs-keyword">if</span> (l &lt; n)<br>            l++;<br>        <span class="hljs-keyword">else</span><br>        &#123;<br>            <span class="hljs-keyword">return</span> l;<br>        &#125;<br>    &#125;<br><br>    <span class="hljs-keyword">return</span> l;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span><br><span class="hljs-function"></span>&#123;<br>    closeSync;<br>    cin &gt;&gt; q;<br>    <span class="hljs-keyword">while</span> (q--)<br>    &#123;<br>        cin &gt;&gt; n;<br>        unordered_map&lt;string, vector&lt;<span class="hljs-type">int</span>&gt;&gt; mp;<br>        string s;<br><br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> k = <span class="hljs-number">1</span>; k &lt;= n; k++)<br>        &#123;<br>            cin &gt;&gt; s;<br>            string elem = <span class="hljs-string">&quot;&quot;</span>;<br>            <span class="hljs-type">int</span> sz = s.<span class="hljs-built_in">size</span>();<br>            string num_s = <span class="hljs-string">&quot;&quot;</span>;<br>            <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; sz; i++)<br>            &#123;<br>                <span class="hljs-keyword">if</span> (s[i] &gt;= <span class="hljs-string">&#x27;a&#x27;</span> <span class="hljs-keyword">and</span> s[i] &lt;= <span class="hljs-string">&#x27;z&#x27;</span>)<br>                    elem += s[i];<br>                <span class="hljs-keyword">else</span><br>                &#123;<br>                    <span class="hljs-keyword">while</span> (s[i] &gt;= <span class="hljs-string">&#x27;0&#x27;</span> <span class="hljs-keyword">and</span> s[i] &lt;= <span class="hljs-string">&#x27;9&#x27;</span>)<br>                        num_s += s[i++];<br>                    i--;<br>                    <span class="hljs-type">int</span> num = <span class="hljs-built_in">atoi</span>(num_s.<span class="hljs-built_in">c_str</span>());<br>                    <span class="hljs-keyword">if</span> (mp[elem].<span class="hljs-built_in">empty</span>())<br>                    &#123;<br>                        <span class="hljs-function">vector&lt;<span class="hljs-type">int</span>&gt; <span class="hljs-title">v</span><span class="hljs-params">(<span class="hljs-number">52</span>, <span class="hljs-number">0</span>)</span></span>;<br>                        mp[elem] = v;<br>                        mp[elem][k] += num;<br>                    &#125;<br>                    <span class="hljs-keyword">else</span><br>                    &#123;<br>                        mp[elem][k] += num;<br>                    &#125;<br>                    <span class="hljs-comment">// debuge: cout &lt;&lt; elem &lt;&lt; &quot; &quot; &lt;&lt; num_s &lt;&lt; endl;</span><br>                    elem = <span class="hljs-string">&quot;&quot;</span>, num_s = <span class="hljs-string">&quot;&quot;</span>;<br>                &#125;<br>            &#125;<br>        &#125;<br><br>        <span class="hljs-type">int</span> Ssum = mp.<span class="hljs-built_in">size</span>();<br>        <span class="hljs-comment">// debug: cout &lt;&lt; Ssum &lt;&lt; endl;</span><br>        <span class="hljs-comment">// 消元:</span><br><br>        <span class="hljs-type">int</span> ind = <span class="hljs-number">1</span>;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">auto</span> &amp;it : mp)<br>        &#123;<br>            <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> b = <span class="hljs-number">1</span>; b &lt;= <span class="hljs-number">50</span>; b++)<br>            &#123;<br>                a[ind][b] = it.second[b];<br>            &#125;<br>            ind++;<br>        &#125;<br>        <span class="hljs-type">int</span> R = <span class="hljs-built_in">solve</span>(Ssum);<br>        R = <span class="hljs-built_in">scan</span>(Ssum);<br>        <span class="hljs-comment">// print(Ssum);</span><br>        <span class="hljs-comment">// cout &lt;&lt; R &lt;&lt; endl;</span><br>        mp.<span class="hljs-built_in">clear</span>();<br>        cout &lt;&lt; ((R &lt; n) ? <span class="hljs-string">&quot;Y\n&quot;</span> : <span class="hljs-string">&quot;N\n&quot;</span>);<br>    &#125;<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>第四题维护一个点以及相邻两个非零点的数值，因为数据非常大（1e9），还需要对数据进行离散化处理，考试的时候是这样想的，大概用了 30 分钟，感觉自己大概率是没发把这个题 100 分过掉，索性在开 3e5 的数组爆搜拿了 40 分。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-comment">//40&#x27;  满分代码还需官网上传题目再完善</span><br><span class="hljs-meta">#<span class="hljs-keyword">pragma</span> GCC optimize(2)</span><br><span class="hljs-meta">#<span class="hljs-keyword">pragma</span> GCC optimize(3, <span class="hljs-string">&quot;Ofast&quot;</span>, <span class="hljs-string">&quot;inline&quot;</span>)</span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;bits/stdc++.h&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-meta">#<span class="hljs-keyword">define</span> closeSync            \</span><br><span class="hljs-meta">    ios::sync_with_stdio(0); \</span><br><span class="hljs-meta">    cin.tie(0);              \</span><br><span class="hljs-meta">    cout.tie(0)</span><br><br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">3e5</span> + <span class="hljs-number">9</span>;<br><span class="hljs-type">int</span> a[N];<br><br><span class="hljs-type">int</span> c, m, n;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">search</span><span class="hljs-params">()</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-type">int</span> cnt = <span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>; i &lt;= c; i++)<br>    &#123;<br>        <span class="hljs-comment">// cout &lt;&lt; a[i] &lt;&lt; &quot; &quot;;</span><br>        <span class="hljs-keyword">if</span> (a[i] != <span class="hljs-number">0</span>)<br>            cnt++;<br>    &#125;<br>    <span class="hljs-keyword">return</span> cnt;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">opp</span><span class="hljs-params">(<span class="hljs-type">int</span> op)</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-type">int</span> l = op - <span class="hljs-number">1</span>, r = op + <span class="hljs-number">1</span>;<br>    <span class="hljs-keyword">if</span> (l &lt; <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> r &gt; c + <span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">return</span>;<br>    <span class="hljs-keyword">if</span> (a[op] &gt;= <span class="hljs-number">5</span>)<br>    &#123;<br>        <span class="hljs-keyword">while</span> (a[l] == <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> l &gt;= <span class="hljs-number">0</span>)<br>        &#123;<br>            l--;<br>        &#125;<br>        <span class="hljs-keyword">while</span> (a[r] == <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> r &lt;= c)<br>        &#123;<br>            r++;<br>        &#125;<br>        a[op] = <span class="hljs-number">0</span>;<br>        a[l]++;<br>        a[r]++;<br>    &#125;<br>    <span class="hljs-keyword">else</span><br>    &#123;<br>        <span class="hljs-keyword">return</span>;<br>    &#125;<br>    <span class="hljs-built_in">opp</span>(l);<br>    <span class="hljs-built_in">opp</span>(r);<br>&#125;<br><span class="hljs-comment">// map&lt;int, int&gt; mp;</span><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span><br><span class="hljs-function"></span>&#123;<br>    closeSync;<br>    cin &gt;&gt; c &gt;&gt; m &gt;&gt; n;<br>    <span class="hljs-type">int</span> ind = <span class="hljs-number">1</span>;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>; i &lt;= m; i++)<br>    &#123;<br>        <span class="hljs-type">int</span> t1, t2;<br>        cin &gt;&gt; t1 &gt;&gt; t2;<br>        a[t1] = t2;<br>    &#125;<br><br>    <span class="hljs-keyword">while</span> (n--)<br>    &#123;<br>        <span class="hljs-type">int</span> op;<br>        cin &gt;&gt; op;<br>        a[op]++;<br>        <span class="hljs-built_in">opp</span>(op);<br>        cout &lt;&lt; <span class="hljs-built_in">search</span>() &lt;&lt; <span class="hljs-string">&#x27;\n&#x27;</span>;<br>    &#125;<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>第五题貌似也是维护一个树形结构？或许也能使用线段树解决，当时只剩下 20 分钟的时间，想使用暴力 STL 嵌套来模拟，最后时间不够了，没用写完。</p><p>&#x2F;&#x2F;代码等待官网上传题目再完善</p><p>总结：第一次打 csp，貌似听说同考场有 390 的选手，不由得赞叹校友们的能力，我还缺乏对于模拟过程中的代码的可读性的提高，因为有时候我不知道我在写什么，这也是我第三题浪费了很多时间，再加上机房的电脑没有配置好断点调试，导致我很难找到之前的 bug；另外虽然可带纸质材料，但是基本上是用不上的，翻书更加浪费时间:( 今年还会再打 1~2 次，希望能刷到 400+。</p>]]></content>
    
    
    <categories>
      
      <category>C++</category>
      
    </categories>
    
    
    <tags>
      
      <tag>算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0-Fast R-CNN</title>
    <link href="/2024/03/06/0-Fast-R-CNN/"/>
    <url>/2024/03/06/0-Fast-R-CNN/</url>
    
    <content type="html"><![CDATA[<p><img src="/../images/0-Fast-R-CNN/image-20240321162127988.png" alt="Fast R-CNN"></p><h2 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>​上回说到R-CNN，而Fast R-CNN是原作者在2015年发表的续作，性能比之前的R-CNN块9倍。目标检测要面临的两大问题是（1）需要处理的候选框过多（2）候选框的位置不精确要进行微调。</p><p>​这就不得不提到R-CNN的缺点：训练以及测试的过程复杂，需要大量的RAM，R-CNN网络需要对候选框进行形变操作后再输入CNN网络提取特征，形变会产生一些列问题。</p><p>相比于RCNN主要在以下方面进行了改进：</p><p>（1）Fast RCNN仍然使用selective search选取2000个建议框，但是这里不是将这么多建议框都输入卷积网络中，而是将原始图片输入卷积网络中得到特征图，再使用建议框对特征图提取特征框。这样做的好处是，原来建议框重合部分非常多，卷积重复计算严重，而这里每个位置都只计算了一次卷积，大大减少了计算量</p><p>（2）由于建议框大小不一，得到的特征框需要转化为相同大小，这一步是通过ROI Pooling层来实现的（ROI表示region of interest即目标）</p><p>（3）Fast RCNN里没有SVM分类器和回归器了，分类和预测框的位置大小都是通过卷积神经网络输出的</p><p>（4）为了提高计算速度，网络最后使用SVD代替全连接层</p><h3 id="实验方法"><a href="#实验方法" class="headerlink" title="实验方法"></a>实验方法</h3><h4 id="inference过程"><a href="#inference过程" class="headerlink" title="inference过程"></a>inference过程</h4><p><img src="/../images/0-Fast-R-CNN/image-20240321164621432.png" alt="inference过程"></p><ul><li><strong>CNN</strong>：将任意size的图像输入网络，计算整张图的feature maps</li><li><strong>Selective search</strong>：在任意size图片上采用selective search算法提取约2k个候选框</li><li><strong>RoI projection</strong>：在特征图中找到每个候选框对应的特征框（深度和特征图一致）</li><li><strong>RoI pooling</strong>：相当于只有一层的空间金字塔池化SPP，将每个特征框划分为H<em>W个网格（eg: 7</em>7 for VGG16），每个网格中执行最大池化，输出为H<em>W</em>C，特征图深度不变。RoI pooling的输出需要满足下一层全连接层输入要求</li><li><strong>FC+softmax&#x2F;bbox regerssion</strong></li><li><strong>NMS：</strong>利用窗口得分分别对每一类物体进行非极大值抑制剔除重叠候选框，最终得到每个类别中回归修正后的得分最高的窗口</li></ul><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p><img src="/../images/0-Fast-R-CNN/image-20240321170935327.png" alt="损失函数"></p><p>[u≥1]是艾弗森括号，当u≥1，这一项为1，否则，这一项为0</p><h4 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h4><p>​对训练集中的图片，用selective search提取出每一个图片对应的一些proposal，保存图片路径和bounding box信息</p><p>​对每张图片，根据图片中bounding box的ground truth信息，给该图片的每一个proposal标记类标签，并保存。具体操作：对于每一个proposal，如果和ground truth中的proposal的IOU值超过了阈值（IOU&gt;&#x3D;0.5），则把ground truth中的proposal对应的类标签给原始产生的这个proposal，其余的proposal都标为背景；</p><p>​使用mini-batch&#x3D;128，25%来自非背景标签的proposal，其余来自标记为背景的proposal；</p><p>​训练CNN，最后一层的结果包含分类信息和位置修正信息，用多任务的loss，一个是分类的损失函数，一个是位置的损失函数。</p><p><img src="/../images/0-Fast-R-CNN/image-20240321171524578.png" alt="训练过程"></p><h4 id="测试过程"><a href="#测试过程" class="headerlink" title="测试过程"></a>测试过程</h4><p>​用selective search方法提取图片的2000个proposal，并保存到文件；将图片输入到已经训好的多层全卷积网络，对每一个proposal，获得对应的RoI Conv featrue map；对每一个RoI Conv featrue map，按照3.1中的方法进行池化，得到固定大小的feture map，并将其输入到后续的FC层，最后一层输出类别相关信息和4个boundinf box的修正偏移量；</p><p>​对bounding box 按照上述得到的位置偏移量进行修正，再根据nms对所有的proposal进行筛选，即可得到对该张图片的bounding box预测值以及每个bounding box对应的类和score。</p><p><img src="/../images/0-Fast-R-CNN/image-20240321171625086.png" alt="测试过程"></p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><img src="/../images/0-Fast-R-CNN/image-20240321171646898.png" alt="结果"></p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>论文笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>目标检测</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0-Rich feature hierarchies for accurate object detection and semantic segmentation</title>
    <link href="/2024/03/05/0-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/"/>
    <url>/2024/03/05/0-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/</url>
    
    <content type="html"><![CDATA[<p><img src="/../images/0-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/image-20240305204216064.png" alt="R-CNN"></p><h2 id="基于区域的卷积神经网络-R-CNN"><a href="#基于区域的卷积神经网络-R-CNN" class="headerlink" title="基于区域的卷积神经网络 (R-CNN)"></a>基于区域的卷积神经网络 (R-CNN)</h2><p>Rich feature hierarchies for accurate object detection and semantic segmentation </p><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>​2013 年 11 月：R-CNN。给定输入图像，R-CNN 首先应用一种称为选择性搜索的机制来提取感兴趣区域(ROI)，其中每个 ROI 是一个可以表示图像中对象边界的矩形。根据场景的不同，ROI 可能多达两千个。之后，每个 ROI 都会通过神经网络产生输出特征。对于每个 ROI 的输出特征，使用支持向量机分类器的集合来确定 ROI 中包含什么类型的对象（如果有）。</p><blockquote><p>2013 年 11 月：R-CNN。给定输入图像，R-CNN 首先应用一种称为选择性搜索的机制来提取感兴趣区域(ROI)，其中每个 ROI 是一个可以表示图像中对象边界的矩形。根据场景的不同，ROI 可能多达两千个。之后，每个 ROI 都会通过神经网络产生输出特征。对于每个 ROI 的输出特征，使用支持向量机分类器的集合来确定 ROI 中包含什么类型的对象（如果有）。<br>2015 年 4 月：Fast R-CNN。原始 R-CNN 在多达 2000 个感兴趣区域中独立计算神经网络特征，而 Fast R-CNN 在整个图像上运行一次神经网络。网络的末端是一种称为 ROIPooling 的新颖方法，它从网络的输出张量中切出每个 ROI，对其进行整形并进行分类。与原始 R-CNN 一样，Fast R-CNN 使用选择性搜索来生成其区域建议。<br>2015 年 6 月：Faster R-CNN。Fast R-CNN 使用选择性搜索来生成 ROI，而 Faster R-CNN 将 ROI 生成集成到神经网络本身中。<br>2017 年 3 月：Mask R-CNN。之前版本的 R-CNN 专注于对象检测，而 Mask R-CNN 添加了实例分割。Mask R-CNN 还用一种名为 ROIAlign 的新方法取代了 ROIPooling，该方法可以表示像素的分数。<br>2019 年 6 月：Mesh R-CNN增加了从 2D 图像生成 3D 网格的功能。</p></blockquote><p>​在本论文研究之前的方法：SIFT和HOG是块方向直方图，但是效果并不好。论文作者通过连接图像分类和目标检测，主要关注了1.使用深度网络定位物体和在小规模的标注数据集上进行大型网络模型的训练。2.与图像分类不同的是检测需要定位一个图像内的许多物体；使用滑动窗口探测器，但是由于网络层次更深，输入图片有非常大的感受野和步长，使得滑动窗口的方法充满挑战，通过操作”recognition using regions”范式，解决了CNN的定位问题。</p><p>​由于结合了Region proposals和CNNs，所以起名<em><strong>R-CNN：Regions with CNN features。</strong></em></p><p>​第二个挑战是标签数据太少，传统方法多是采用无监督与训练，再进行有监督调优，本论文使用了也就是第二个核心贡献是在辅助数据集（ILSVRC）上进行有监督预训练，再在小数据集上针对特定问题进行调优。这是在训练数据稀少的情况下一个非常有效的训练大型卷积神经网络的方法。</p><h3 id="实验方法"><a href="#实验方法" class="headerlink" title="实验方法"></a>实验方法</h3><h4 id="模型设计"><a href="#模型设计" class="headerlink" title="模型设计"></a>模型设计</h4><p>使用selective search进行<strong>Region proposals</strong>，使用AlexNet对每个region提取一个4096维的特征向量的特征提取，采用各向异性缩放变换。</p><p><img src="/../images/0-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/image-20240306113347937.png" alt="模型设计"></p><p>使用selective search(‘fast mode’模式)得到2000个左右的proposals，进行形状变换后传入CNN得到对应特征，然后将特征向量送入SVM中得到对应的类别。现在，我们得到图像中所有已经打分的region，应用greedy non-maximum suppression，去除重复框。</p><h4 id="训练阶段"><a href="#训练阶段" class="headerlink" title="训练阶段"></a>训练阶段</h4><p>​在ImageNet数据集上对CNN进行预训练。</p><p>​为了将预训练的CNN迁移到本任务(warped proposal windows分类)上，在warped region proposals上使用SGD进行fine-tune，不改变整体的网络结构，只将最后的1000-way分类层改为(N+1)-way，其中N为物体类别数，1为背景类别。SGD的初始学习率为预训练的1&#x2F;10，这样可以进行fine-tune，并且不破坏初始化。batch size为128，其中32个positive windows(所有类别，将IoU≥0.5的proposal视为该类别的positive，其他的为negative)，96个背景windows。 并且，在采样时倾向于采样positive windows，因为与背景相比它们是罕见的。<br>对于R-CNN的分类器，正例就是每一类ground -truth bounding box，IoU小于0.3的作为负类，其他的全部丢弃，不考虑。再训练SVM过程中，为了加速收敛使用了”Hard Negative Mining”策略(将每次loss很大的样本继续送到下一次训练中)。<br><img src="/../images/0-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/image-20240306113750317.png" alt="实验结果"></p><h4 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h4><p>​作者在这里提出了一个可视化的想法，核心思想就是让神经元”speak for itself”：挑选出网络中的某个特定uint(当做检测器)，计算所有proposal在这个uint上的输出，按输出大小进行排序之后使用非极大值抑制(NNS)显示那些top-scoring区域。下图为关于CNN的池化层pool5的一个可视化效果。<br><img src="/../images/0-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/image-20240306113906119.png" alt="可视化"></p><h4 id="消融研究"><a href="#消融研究" class="headerlink" title="消融研究"></a>消融研究</h4><p>​证明CNN的表征能力基本来自卷积层</p><h4 id="错误率分析"><a href="#错误率分析" class="headerlink" title="错误率分析"></a>错误率分析</h4><p>​引入Bounding Box Regression可以减少定位问题，fine-tuning可以提高模型的鲁棒性等</p><p><img src="/../images/0-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/image-20240306114037739.png" alt="错误率分析"></p><p><img src="/../images/0-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/image-20240306114137183.png" alt="对物体特征的敏感性"></p><h3 id="结论："><a href="#结论：" class="headerlink" title="结论："></a>结论：</h3><p>​第一是应用了自底向上的候选框训练的高容量的卷积神经网络进行定位和分割物体。另外一个是使用在标签数据匮乏的情况下训练大规模神经网络的一个方法。论文展示了在有监督的情况下使用丰富的数据集（图片分类）预训练一个网络作为辅助性的工作是很有效的，然后采用稀少数据（检测）去调优定位任务的网络。猜测“有监督的预训练+特定领域的调优”这一范式对于数据稀少的视觉问题是很有效的。</p><p>​最后，论文能得到这些结果，将计算机视觉中经典的工具和深度学习(自底向上的区域候选框和卷积神经网络）组合是非常重要的。而不是违背科学探索的主线，这两个部分是自然而且必然的结合。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>论文笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>目标检测</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>1-基础部分-3-读论文</title>
    <link href="/2024/03/05/1-%E5%9F%BA%E7%A1%80%E9%83%A8%E5%88%86-3-%E8%AF%BB%E8%AE%BA%E6%96%87/"/>
    <url>/2024/03/05/1-%E5%9F%BA%E7%A1%80%E9%83%A8%E5%88%86-3-%E8%AF%BB%E8%AE%BA%E6%96%87/</url>
    
    <content type="html"><![CDATA[<h1 id="学位论文"><a href="#学位论文" class="headerlink" title="学位论文"></a>学位论文</h1><p>摘要<br>第一章绪论<br>第二章材料与方法<br>第三章结果与讨论(1)<br>第四章结果与讨论(2)<br>第五章结果与讨论(3)<br>结论<br>参考文献<br>攻读硕士学位期间取得创新性成果<br>学位论文原创性声明及使用授权<br>致谢<br>个人简历</p><p><img src="/../images/1-%E5%9F%BA%E7%A1%80%E9%83%A8%E5%88%86-3-%E8%AF%BB%E8%AE%BA%E6%96%87/1709190804696.png" alt="1709190804696"></p><h2 id="功能："><a href="#功能：" class="headerlink" title="功能："></a>功能：</h2><p>·题目→点睛，文章的极致浓缩;题目信息量≥50%文章的内容·</p><p>·摘要→浓缩的论文（重要程度超过论文主体)</p><p>·关键词→漂流瓶上的GPS(频道要一致)</p><p>·引言→背景（目的) -现状（那个等待修补的重要拼图）-创新性（我了解了拼图的基本信息)-方法（路线图)。</p><p>·材料与方法→我们有什么(有&#x3D;限制，思维、方法、技术)</p><p>·结果与讨论→我发现了什么，我的发现怎么样?</p><p>·结论→有得有失</p><p>·参考文献→一封感谢信（定位)</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>题目的扩写(检查)<br>内容的缩写（方法)<br>四要素全是基本要求<br>摘要是论文主体的浓缩<br>简洁，要有取舍、详略</p><h2 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h2><p>·通过对题目拆解得来<br>·题目:通过对关键字组合而来。<br>·准确，过于泛没有针对性。<br>·冷，过于生僻无人认<br>·精准＋宽泛<br>·技术在进步，关键词可能没那么重要了?</p><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>·引言决定论文的格局</p><p>·以为是套路，其实是逻辑</p><p>·背景</p><p>·进展</p><p>·存在的问题</p><p>·我的解决方案</p><p>·引言是你思考的逻辑顺序</p><h2 id="结果与讨论"><a href="#结果与讨论" class="headerlink" title="结果与讨论"></a>结果与讨论</h2><p>·科技论文中，撰写结果与讨论的目的可总结为:用论据论证论点。<br>·拆分1段完整的结果与讨论，我们会发现，其中一定包括以下几点:</p><p>​1）指出图表;2)结果描述;3)规律总结;4)对比优劣&#x2F;机理阐明;5)给出结论。<br>·以上5部分除了第5部分外，其他4个部分基本上是一定要有的。这5个部分其实也反应了作者在做研究时，对研究本身一个“由表及里”逐步了解的一个过程。</p><p><img src="/../images/1-%E5%9F%BA%E7%A1%80%E9%83%A8%E5%88%86-3-%E8%AF%BB%E8%AE%BA%E6%96%87/1709193287719.png" alt="1709193287719"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>·完美的结论应是超出论文本身的内容</p><p>·提炼的、升华的</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>·真正对本研究有用的文献(实事求是)<br>·重量级相当的文献<br>·注意格式<br>·40%的内容来自于参考文献</p><h2 id="方法来源"><a href="#方法来源" class="headerlink" title="方法来源"></a>方法来源</h2><p><strong>三遍阅读法方法</strong>是ACM和IEEE Fellow 剑桥大学计算机教授Srinivasan Keshav的论文阅读技巧</p><h3 id="第一遍：该不该读？"><a href="#第一遍：该不该读？" class="headerlink" title="第一遍：该不该读？"></a>第一遍：该不该读？</h3><p>1.阅读标题、摘要和简介<br>2.忽略内容，读一读文章中的每个小标题<br>3.如果数学内容，先大致浏览，确定其理论基础<br>4.读结论<br>5.浏览参考文献，如果有读过的，勾选出来<br>第一遍阅读后应得出结论：</p><p>文章分类<br>文章背景<br>结论的正确性<br>所做出的主要贡献<br>结构清晰度</p><h3 id="第二遍：抓住要点，暂略细节"><a href="#第二遍：抓住要点，暂略细节" class="headerlink" title="第二遍：抓住要点，暂略细节"></a>第二遍：抓住要点，暂略细节</h3><p>时间：About 1 hour<br>1.过程中，仔细查看论文的图表，关注细节<br>2.标记论文中涉及的、并未读过的参考文献，之后做进一步阅读<br>第二遍阅读后应做到：</p><p>掌握内容，总结主旨</p><h3 id="第三遍：重构论文，注重细节"><a href="#第三遍：重构论文，注重细节" class="headerlink" title="第三遍：重构论文，注重细节"></a>第三遍：重构论文，注重细节</h3><p>跟随作者的思路，在脑海里重现论文内容<br>细节！细节！细节！<br>第三遍阅读后应做到：</p><p>看出论文的创新点<br>找到隐含假设<br>捕捉试验和技术分析中的潜在问题<br>引文缺失</p><h3 id="读，必须要读，不然从哪里开始学习入门呢？"><a href="#读，必须要读，不然从哪里开始学习入门呢？" class="headerlink" title="读，必须要读，不然从哪里开始学习入门呢？"></a>读，必须要读，不然从哪里开始学习入门呢？</h3><p>不过，读文献也要讲方法讲策略，否则读完就忘，也起不到任何作用。个人建议</p><ol><li>开始的时候找一个领域内相关的综述，越新越好，发表的档次越高越好（可以看影响因子）</li><li>仔细读完综述，做笔记，写下自己对该研究领域的认知，包括领域现状、关键原理、重点热点方向、面临的挑战等</li><li>挑一个感兴趣的方向，最好结合自己所在课题组的实际情况，以能完成为前提</li><li>找出该方向上的最新工作，不限于综述里引用的，最好能找到 10-15 篇，档次尽量不要太差</li><li>仔细读完这些文章，总结提炼出每一个工作的 idea，包括研究背景、要解决的问题、前人的方法和局限、作者提出的新方法、新方法为什么能避免前人的问题、关键数据和证明等</li><li>横向比较这十来个工作的 idea，找到其中的共性和不同，尤其是共性的研究背景和要解决的问题，作者提出的不同的新方法</li><li>此时对领域应该已经有了一个较全面的把握，可以开始构思自己的 idea 了，注意依然要包含上面的要素</li><li>拿自己的 idea 去跟导师讨论确认</li></ol><h1 id="time"><a href="#time" class="headerlink" title="{time}"></a>{time}</h1><p>这篇文章主要是想给大家分享一些论文阅读的技巧。网上相关的经验贴其实很多，我主要是看了沈向洋老师和吴恩达老师的两个视频，结合自己日常的一些体会，写下的这篇文章，两位老师的关于论文的视频我就放在文末了。</p><p>本篇文章的目录为</p><blockquote><p><em>论文的常见框架</em><br><em>读论文的四个层次</em><br><em>读论文的三个阶段</em><br><em>读论文的不同部分应该得出的结论</em><br><em>读论文带着的12个问题</em><br><em>读论文的笔记模板</em></p></blockquote><h2 id="论文的常见框架"><a href="#论文的常见框架" class="headerlink" title="论文的常见框架"></a>论文的常见框架</h2><p>一般的论文都会按顺序包含一下几个部分：</p><ul><li>title</li><li>keywords</li><li>introduction</li><li>related work</li><li>method</li><li>experimental results and discussion</li><li>summary&#x2F;conclusion</li><li>reference</li><li>appendix</li></ul><h2 id="读论文的四个层次（沈向洋）"><a href="#读论文的四个层次（沈向洋）" class="headerlink" title="读论文的四个层次（沈向洋）"></a><strong>读论文的四个层次（沈向洋）</strong></h2><ul><li>消极阅读（passive reading），即大概知道文章讲了什么；</li><li>积极阅读（ active reading），主动思考这些知识有什么用；</li><li>批判性阅读（critical reading），思考这篇文章是否言之成理；</li><li>创造性阅读（creative reading），搞清楚文章对接下来的工作有什么帮助。</li></ul><h2 id="读论文的三个阶段"><a href="#读论文的三个阶段" class="headerlink" title="读论文的三个阶段"></a>读论文的三个阶段</h2><p>不是所有的论文一拿到就直接从头读到尾的，特别是现在论文这么多，这样做也不现实，所以读论文其实应该分步骤去读，先粗略地看一下，看看这是不是你感兴趣的文章；再简略地过一下全文，尤其是那些很多数学的部分，可以先跳过，大部分的论文看到这就行了，这个时候就基本上已经了解了论文的创新点；精读论文，还可以看一下开源的代码来帮助理解，知道论文的具体的实现的细节，如果是一些特别经典或者是你的工作主要就是基于这篇论文做的，那是肯定得精读的，即使可能得花掉一个星期甚至一个月的时间。具体的每个阶段看的内容如下：</p><ul><li>第一阶段（速读）：Title、Author、Abstract、Figure and Table(Introduction)(沈向洋觉得是论文的前两页，吴恩达觉得看完摘要要先看图表，这对于计算机来说确实如此）—–（快速知道论文讲了什么）。我个人觉得拿到一篇论文之后，先看题目、摘要、结论、图表会好一点，然后如果你不太了解这个领域，想要了解一下作者的motivation，那就可以把引言也读了。</li><li>第二解决（精读）：Read but skim math，这个解决算是对论文的精读，在这个部分可以对论文进行批判性和创造性地阅读，也就是要对论文进行否定、质疑，仔细挑毛病。在读论文的时候可以带着这些问题：</li></ul><p>1、论文是否正确、真正地解决了问题？</p><p>2、作者论文中所用方法是否有局限性？对论文有了足够的了解之后，如果发现论文中提 到的想法非常优秀，那么要创造性地思考你能用这篇论文做什么</p><p>3、如果所读的论文没有解决问题，那么我能解决么？</p><p>4、我能采用比论文中更简单的方法解决么？</p><ul><li>第三阶段（研读）：论文的每个部分的具体细节包括代码实现，一般论文特别经典的时候才需要。不过需要主要的是，即使论文很经典，也不见得是每个部分都需要研读，论文的一些部分可能发展到现在已经不 make sense了，比如AlexNet论文里面的各种trick，像这种就没有必要读了。</li></ul><h2 id="读论文的不同部分要得到的结论"><a href="#读论文的不同部分要得到的结论" class="headerlink" title="读论文的不同部分要得到的结论"></a>读论文的不同部分要得到的结论</h2><p>1、Abstract</p><ul><li>作者想解决什么问题？ question</li><li>作者通过什么理论&#x2F;模型来解决这个问题？method</li><li>作者给出的答案是什么？ answer</li></ul><p>2、Introduction</p><ul><li>作者为什么研究这个课题？</li><li>目前这个课题的研究进行到了哪一阶段？</li><li>作者使用的理论是基于哪些假设？</li></ul><p>3、Conclusion</p><ul><li>这篇文章存在哪些缺陷？</li><li>作者关于这个课题的构思有哪几点？</li></ul><p>4、Table and Figure</p><ul><li>文章阶段性的成果</li></ul><p>5、Method and experiment</p><ul><li>研究的数据从哪里来？</li><li>研究中用到的重要指标有哪些？</li><li>模型分哪几步？ 每一步分别得出了什么结论？</li></ul><h2 id="读论文带着的12个问题"><a href="#读论文带着的12个问题" class="headerlink" title="读论文带着的12个问题"></a>读论文带着的12个问题</h2><p>我觉得带着论文去读问题效率会高很多。当然不是看所有的论文都是带着这些问题，可能有一些论文你自己看它已经有自己的目的了，那你就根据自己的情况来就好了，下面的是普遍性的。</p><ol><li>What is the problem addressed in the paper? What‘s the input and output? （当然不是每篇论文都有严格的输入输出）</li><li>Is this a new problem? If it is a new problem, why does it matter? If it is not an entirely new problem, why does it still matter?</li><li>What is the scientific hypothesis that the paper is trying to verify? Address what new knowledge is advanced in the pape</li><li>What are the key related works and who are the key people working on this topic? （比如说对于Adam来说，其他的一些优化算法比如说SGD、Adagrad、Rmsprop等就可以稍微回想一下）</li><li>What is the key Of the proposed solution in the paper?</li><li>How are the experiments designed?（虽然一般论文的实验结果都是说自己的结果要好，但是也要注重实验，一来是可以参考别人是怎么设置实验的，还有就是看看作者的实验设置是否合理）</li><li>What datasets are built&#x2F;used for the quantitative evaluation? Is the code open source?</li><li>Is the scientific hypothesis well supported by evidence in the experiments? Are the claims in the paper well supported by the experimental results?</li><li>What are the contributions of the paper?</li><li>What should&#x2F;could be done next? （limitation of this paper）</li><li>Are there important related papers I missed?</li><li>What question should I ask the author( Any query？？？ 比如说你哪里没看懂或者觉得不合理的）</li></ol><h2 id="读论文的笔记模板"><a href="#读论文的笔记模板" class="headerlink" title="读论文的笔记模板"></a>读论文的笔记模板</h2><p>我觉得在阅读论文的过程中，记录一些笔记是非常重要的，包括在论文中进行一些标注，以及自己写成笔记的形式。论文中的标注大家可能自己有自己的方法，这里就不介绍了，这里主要介绍一下我平时写论文笔记的过程中整理的笔记模板，虽然用这些模板记录真的很花时间，但是等到以后你突然回过头再看这篇论文的时候，有了这些笔记就方便很多了，正所谓：“磨刀不误砍柴工”。</p><p><strong>0 论文标题</strong></p><p>一句话概括论文的主要内容（用什么样的方法解决了什么样的问题？）</p><p><strong>1 论文的结构(简要概括)</strong></p><p><strong>按标题的顺序</strong>写一下论文的每个部分大概写了那些内容，每个部分大概用一两句话或者几句话来概括。下面给的每个部分的问题不一定作者都提到了，比如说文章的缺陷，如果没提到大家可以不用在这个部分写，也就是<strong>论文的结构这一部分只写论文里面提到的</strong>，<strong>而且只是一个相对简要的概括，所以可能有些问题会和下面的其他部分重复</strong>。或者大家觉得还有其他重要的问题但是我下面没有提到的大家也可以自己补充。<strong>写这个的主要目的是自己了解一些各种论文结构。</strong></p><p>不一定每篇论文当中都是下面的几个部分，大家根据实际的论文去总结，有些论文包含其他部分的大家就根据自己的理解去总结。</p><p><strong>1.1 Abstract</strong></p><ul><li>作者想解决什么问题？</li><li>作者通过什么理论&#x2F;模型来解决这个问题？</li><li>作者给出的答案是什么？</li></ul><p><strong>1.2 Introduction</strong></p><ul><li>作者为什么研究这个课题？</li><li>目前这个课题的研究进行到了哪一阶段？存在哪些缺陷？作者是想通过本文解决哪个问题？</li><li>作者使用的理论是基于哪些假设？</li></ul><p><strong>1.3 Related work</strong></p><ul><li>和作者这篇论文相关的工作有哪些？</li><li>之前工作的优缺点是什么？</li><li>作者主要是对之前的哪个工作进行改进？</li></ul><p><strong>1.4 Theoretical Analysis</strong></p><ul><li>作者是用什么理论证明了自己的方法在理论上也是有保障的？</li></ul><p><strong>1.5 Experiment</strong></p><ul><li>作者是在哪些数据集或者说场景下进行了测试？</li><li>实验中的重要指标有哪些？</li><li>文章提出的方法在哪些指标上表现好？在哪些指标上表现不好？</li><li>在实验的设置过程中作者有没有提到自己用到了什么trick？</li></ul><p><strong>1.6 Conclusion</strong></p><ul><li>这篇论文最大的贡献是什么？</li><li>论文中的方法还存在什么问题？</li><li>作者觉得还可以怎么改进？</li></ul><p><strong>2 论文想要解决的问题？</strong></p><p><strong>2.1 背景是什么？</strong></p><p><strong>2.2 之前的方法存在哪些问题</strong></p><p><strong>2.3 输入和输出是什么？</strong></p><p><strong>3 论文研究的是否是一个新问题</strong></p><p><strong>4 论文试图验证的科学假设</strong></p><p><strong>5 相关的关键人物与工作</strong></p><p><strong>5.1 之前存在哪些相关的工作</strong></p><p><strong>5.2 本文是对哪个工作进行的改进</strong></p><p><strong>5.3 这个领域的关键研究者</strong></p><p><strong>6 论文提出的解决方案的关键</strong></p><p><strong>7 论文的解决方案有完备的理论证明吗</strong></p><p><strong>8 实验设计</strong></p><p><strong>8.1用到了哪些数据集</strong></p><p><strong>8.2与什么算法进行了比较</strong></p><p><strong>8.3评价指标是什么</strong></p><p><strong>8.4有没有什么独特的实验实验设计？</strong></p><p><strong>9 实验支撑</strong></p><p><strong>9.1 论文的数据集哪里获取</strong></p><p><strong>9.2 源代码哪里可以获取</strong></p><p><strong>9.3 关键代码的讲解</strong></p><p><strong>10 实验结果是否验证了科学假设？</strong></p><p><strong>11 论文最大的贡献</strong></p><p><strong>12 论文的不足之处</strong></p><p><strong>12.1 这篇论文之后的工作有哪些其他的改进</strong></p><p><strong>12.2你觉得可以对这篇论文有什么改进</strong></p><p><strong>13 重要的相关论文</strong></p><p><strong>14 不懂之处</strong></p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>基础部分</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>0-Deep Residual Learning for Image Recognition</title>
    <link href="/2024/03/05/0-Deep-Residual-Learning-for-Image-Recognition/"/>
    <url>/2024/03/05/0-Deep-Residual-Learning-for-Image-Recognition/</url>
    
    <content type="html"><![CDATA[<p><img src="/../images/0-Deep-Residual-Learning-for-Image-Recognition/image-20240305100734044.png" alt="文章发表于2015年"></p><h2 id="用于图像识别的深度残差学习ResNet"><a href="#用于图像识别的深度残差学习ResNet" class="headerlink" title="用于图像识别的深度残差学习ResNet"></a>用于图像识别的深度残差学习ResNet</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>越深的神经网络训练起来越发困难，利用残差学习框架，能够简化深层的的网络训练。根据输入来学习残差函数而非原神函数，在ImageNet数据集使用了152曾的网络来评价残差网络，具有很低的复杂度，并且多个ensemble在测试集上的错误率很低。</p><p>在深度学习神经网络的训练中，层次越深，训练越困难，优化越困难，并且会出现梯度消失&#x2F;爆炸等问题阻碍网络收敛，使用归一初始化（normalized initialization）和中间归一化（intermediate normalization）在很大程度上解决了这一问题，使得在前数十层的网络在反向传播的随机梯度下降（SGD）上能够收敛。</p><p>层数更深后，精度饱和，训练模型迅速变差</p><p>​残差神经网络（也称为残差网络或<strong>ResNet</strong>）是一种深度学习模型，其中权重层参考层输入学习残差函数。</p><p>​残差学习框架通过引入残差学习的概念，使得训练比以往更深的网络变得更加容易。这种框架允许网络学习残差映射，即学习残差函数而不是直接学习底层特征映射。通过这种方式，网络可以更轻松地学习残差，从而减轻了训练深度网络时出现的梯度消失或梯度爆炸等问题。</p><h3 id="步骤："><a href="#步骤：" class="headerlink" title="步骤："></a>步骤：</h3><p>​在ImageNet 2015比赛之前，2012 年 ImageNet 开发的AlexNet模型是一个八层卷积神经网络。牛津大学视觉几何小组 (VGGNet) 于 2014 年开发的神经网络通过堆叠 3×3 卷积层达到了 19 层的深度，然而，堆叠更多层会导致训练精度急剧下降，这被称为“退化”问题。</p><p><img src="/../images/0-Deep-Residual-Learning-for-Image-Recognition/image-20240305110540544.png" alt="梯度下降"></p><p>如上图所示，将20层神经网络加深到56层之后，模型的training error和test error反而更高了。</p><p>论文提出了一个解决方案，就是使用深度残差网络：</p><p><img src="/../images/0-Deep-Residual-Learning-for-Image-Recognition/image-20240305161546348.png" alt="深度残差网络"></p><p>​从深层网络出发，深层网路&#x3D;浅层网络+附加层，如果浅层网络已经做的非常好了，附加层只会进行一些微小的改动，得到的结果就是网络随着深度的增加，准确率会上升，而不是degredation描述的下降。</p><p><img src="/../images/0-Deep-Residual-Learning-for-Image-Recognition/image-20240305162027063.png" alt="残差网络Residual Network"></p><p>​从图中我们可以看到ResNet中的快捷连接有实线和虚线，实线表示输入输出维度相同，虚线表示维度不同。对于 ResNet，当输入维度小于输出维度时，有3 种类型的快捷连接方式：</p><ul><li>(A) Shortcut 执行恒等映射，使用额外的零填充来增加维度。因此，没有额外的参数。</li><li>(B) 投影快捷方式仅用于增加维度，其他快捷方式是恒等映射。需要额外的参数。</li><li>(C) 所有捷径都是投影。额外的参数比（B）的要多。</li></ul><p>实验表明方式C的精度最高，但作者建议使用方式B，因为C的计算量和参数量都有所增加。</p><p><img src="/../images/0-Deep-Residual-Learning-for-Image-Recognition/image-20240305163658461.png" alt="Table 3. Error rates"></p><h3 id="实验方法："><a href="#实验方法：" class="headerlink" title="实验方法："></a>实验方法：</h3><ul><li>将图像扩充到[256,480]之间，再resize为224 × 224</li><li>使用颜色增强</li><li>使用BN</li><li>将学习率通过乘0.1减小(这个方法现在已经不太用了，因为不知道具体在什么时候乘这个0.1，有的时候可能乘早了，在晚一点乘效果会更好，图中断崖式下降的地方就是学习率乘了0.1的地方)</li><li>没有使用dropout操作(dropout对卷积层的正则化作用很小：卷积层的参数必FC少很多，本身不需要正则化；同时，特征图编码的是空间的关系，他们之间是高度相关的，这也导致了dropout的失效)</li><li>在测试中使用了10-crop(10-crop是指在test的时候，从原始图片及翻转后的图片中，从四个corner和一个center各crop一个(224,224)的图片，一次是5张，镜像之后再操作一次就是10张。然后对这10张图片进行分类，对10次预测结果做average)</li><li>使用了{224，256，384，480，640}这5种不同的分辨率</li></ul><p><img src="/../images/0-Deep-Residual-Learning-for-Image-Recognition/image-20240305164121060.png" alt="实验结果"></p><p><img src="/../images/0-Deep-Residual-Learning-for-Image-Recognition/image-20240305164201058-17096281334771.png" alt="与普通网络的对比结果"></p><p>当使用普通网络时，由于退化问题，18 层的结果优于 34 层；使用 ResNet 时，34 层优于 18 层，通过快捷连接解决了梯度消失问题。(比较 18 层普通网络和 18 层 ResNet，没有太大区别。这是因为浅层网络不会出现梯度消失问题。)</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>论文笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>目标检测</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>markdown中的数学公式</title>
    <link href="/2023/09/12/markdown%E4%B8%AD%E7%9A%84%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/"/>
    <url>/2023/09/12/markdown%E4%B8%AD%E7%9A%84%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/</url>
    
    <content type="html"><![CDATA[<p>设置 math: true</p><h3 id="符号大全"><a href="#符号大全" class="headerlink" title="符号大全"></a>符号大全</h3><table><thead><tr><th align="left">写法</th><th align="center">符号</th><th align="left">备注</th></tr></thead><tbody><tr><td align="left">\sin(x)</td><td align="center">$$\sin(x)$$</td><td align="left">正弦函数</td></tr><tr><td align="left">\log(x)</td><td align="center">$$\log(x)$$</td><td align="left">对数函数</td></tr><tr><td align="left">\sum_{i&#x3D;0}^n</td><td align="center">$$\sum_{i&#x3D;0}^n$$</td><td align="left">累加和</td></tr><tr><td align="left">\prod_{i&#x3D;0}^n</td><td align="center">$$\prod_{i&#x3D;0}^n$$</td><td align="left">累积乘</td></tr><tr><td align="left">\displaystyle</td><td align="center">$$\displaystyle$$</td><td align="left">块显示</td></tr><tr><td align="left">\ldots</td><td align="center">$$\ldots$$</td><td align="left">底部省略号</td></tr><tr><td align="left">\cdots</td><td align="center">$$\cdots$$</td><td align="left">中部省略号</td></tr><tr><td align="left">\int_a^b</td><td align="center">$$\int_a^b$$</td><td align="left">积分符号</td></tr><tr><td align="left">\lim</td><td align="center">$$\lim$$</td><td align="left">极限函数</td></tr><tr><td align="left">\to</td><td align="center">$$\to$$</td><td align="left">箭头</td></tr><tr><td align="left">\vec{a}</td><td align="center">$$\vec{a}$$</td><td align="left">矢量a</td></tr><tr><td align="left">90^\circ</td><td align="center">$$90^\circ$$</td><td align="left">度数的圆圈</td></tr><tr><td align="left">\uparrow</td><td align="center">$$\uparrow$$</td><td align="left">上箭头</td></tr><tr><td align="left">\Uparrow</td><td align="center">$$\Uparrow$$</td><td align="left">双上箭头</td></tr><tr><td align="left">\partial y</td><td align="center">$$\partial y$$</td><td align="left">导数&#x2F;偏导</td></tr><tr><td align="left">\infty</td><td align="center">$$\infty$$</td><td align="left">无穷</td></tr><tr><td align="left">\Pi</td><td align="center">$$\Pi$$</td><td align="left">累乘</td></tr><tr><td align="left">\sqrt{x}</td><td align="center">$$\sqrt{x}$$</td><td align="left">求平方根</td></tr><tr><td align="left">\overline{a+b}</td><td align="center">$$\overline{a+b}$$</td><td align="left">上划线</td></tr><tr><td align="left">\underline{a+b}</td><td align="center">$$\underline{a+b}$$</td><td align="left">下划线</td></tr><tr><td align="left">\overbrace{a+b}</td><td align="center">$$\overbrace{a+b}$$</td><td align="left">上括号</td></tr><tr><td align="left">\underbrace{a+b}</td><td align="center">$$\underbrace{a+b}$$</td><td align="left">下括号</td></tr><tr><td align="left">\pm{a}{b}</td><td align="center">$$\pm{a}{b}$$</td><td align="left">正负号</td></tr><tr><td align="left">\mp{a}{b}</td><td align="center">$$\mp{a}{b}$$</td><td align="left">负正号</td></tr><tr><td align="left">\times</td><td align="center">$$\times$$</td><td align="left">乘法</td></tr><tr><td align="left">\cdot</td><td align="center">$$\cdot$$</td><td align="left">点乘</td></tr><tr><td align="left">\ast</td><td align="center">$$\ast$$</td><td align="left">星乘</td></tr><tr><td align="left">\div</td><td align="center">$$\div$$</td><td align="left">除法</td></tr><tr><td align="left">\frac{1}{5}</td><td align="center">$$\frac{1}{5}$$</td><td align="left">分数</td></tr><tr><td align="left">\drac{1}{5}</td><td align="center">$$已废弃$$</td><td align="left">分数，字体更大</td></tr><tr><td align="left">\leq</td><td align="center">$$\leq$$</td><td align="left">小于等于</td></tr><tr><td align="left">\not</td><td align="center">$$\not$$</td><td align="left">非</td></tr><tr><td align="left">\geq</td><td align="center">$$\geq$$</td><td align="left">大于等于</td></tr><tr><td align="left">\neq</td><td align="center">$$\neq$$</td><td align="left">不等于</td></tr><tr><td align="left">\nleq</td><td align="center">$$\nleq$$</td><td align="left">不小于等于</td></tr><tr><td align="left">\ngeq</td><td align="center">$$\ngeq$$</td><td align="left">不大于等于</td></tr><tr><td align="left">\sim</td><td align="center">$$\sim$$</td><td align="left">相关符号</td></tr><tr><td align="left">\approx</td><td align="center">$$\approx$$</td><td align="left">约等于</td></tr><tr><td align="left">\equiv</td><td align="center">$$\equiv$$</td><td align="left">常等于&#x2F;横等于</td></tr><tr><td align="left">\bigodot</td><td align="center">$$\bigodot$$</td><td align="left">加运算符</td></tr><tr><td align="left">\bigotimes</td><td align="center">$$\bigotimes$$</td><td align="left">乘运算符</td></tr></tbody></table><h3 id="集合符号"><a href="#集合符号" class="headerlink" title="集合符号"></a>集合符号</h3><table><thead><tr><th>写法</th><th align="center">符号</th><th>备注</th></tr></thead><tbody><tr><td>\in</td><td align="center">$$\in$$</td><td>属于</td></tr><tr><td>\notin</td><td align="center">$$\notin$$</td><td>不属于</td></tr><tr><td>\subset</td><td align="center">$$\subset$$</td><td>真子集</td></tr><tr><td>\not \subset</td><td align="center">$$\not \subset$$</td><td>非子集</td></tr><tr><td>\subseteq</td><td align="center">$$\subseteq$$</td><td>子集</td></tr><tr><td>\supset</td><td align="center">$$\supset$$</td><td>超集</td></tr><tr><td>\supseteq</td><td align="center">$$\supseteq$$</td><td>超集</td></tr><tr><td>\cup</td><td align="center">$$\cup$$</td><td>并集</td></tr><tr><td>\cap</td><td align="center">$$\cap$$</td><td>交集</td></tr><tr><td>\mathbb{R}</td><td align="center">$$\mathbb{R}$$</td><td>实数集</td></tr><tr><td>\emptyset</td><td align="center">$$\emptyset$$</td><td>空集</td></tr></tbody></table><h3 id="希腊符号"><a href="#希腊符号" class="headerlink" title="希腊符号"></a>希腊符号</h3><table><thead><tr><th>写法</th><th align="center">符号</th></tr></thead><tbody><tr><td>\alpha</td><td align="center">α</td></tr><tr><td>\beta</td><td align="center">β</td></tr><tr><td>\gamma</td><td align="center">γ</td></tr><tr><td>\Gamma</td><td align="center">Γ</td></tr><tr><td>\theta</td><td align="center">θ</td></tr><tr><td>\Theta</td><td align="center">Θ</td></tr><tr><td>\delta</td><td align="center">δ</td></tr><tr><td>\Delta</td><td align="center">Δ</td></tr><tr><td>\triangledown</td><td align="center">▽</td></tr><tr><td>\epsilon</td><td align="center">ϵ</td></tr><tr><td>\zeta</td><td align="center">ζ</td></tr><tr><td>\eta</td><td align="center">η</td></tr><tr><td>\kappa</td><td align="center">κ</td></tr><tr><td>\lambda</td><td align="center">λ</td></tr><tr><td>\mu</td><td align="center">μ</td></tr><tr><td>\nu</td><td align="center">ν</td></tr><tr><td>\xi</td><td align="center">ξ</td></tr><tr><td>\pi</td><td align="center">π</td></tr><tr><td>\sigma</td><td align="center">σ</td></tr><tr><td>\tau</td><td align="center">τ</td></tr><tr><td>\upsilon</td><td align="center">υ</td></tr><tr><td>\phi</td><td align="center">ϕ</td></tr><tr><td>\omega</td><td align="center">ω</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>markdown</category>
      
    </categories>
    
    
    <tags>
      
      <tag>markdown</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2-工具与软件-5-TensorFlow</title>
    <link href="/2023/09/08/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/"/>
    <url>/2023/09/08/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/</url>
    
    <content type="html"><![CDATA[<h2 id="环境的安装"><a href="#环境的安装" class="headerlink" title="环境的安装"></a>环境的安装</h2><p>首先去conda官网下载  <a href="https://repo.anaconda.com/">conda</a></p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230912155352950.png" alt="选择合适的版本"></p><p>linux系统先使用bash安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo bash xxx.sh<br></code></pre></td></tr></table></figure><p>安装后在pycharm配置conda环境，然后新建AI项目，选择conda，然后在所选择的解释器中安装tensorflow</p><p>选择pycharm自动安装（会自动安装其他依赖，十分方便）</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230912155649291.png" alt="tensorflow安装"></p><p>所需安装：</p><ul><li>conda</li><li>tensorflow</li></ul><p>如果你是N卡，可继续在项目终端中输入</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">conda install cudatoolkit<br></code></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">conda install cudnn<br></code></pre></td></tr></table></figure><p>安装GUP加速</p><h2 id="1-1-人工智能三学派"><a href="#1-1-人工智能三学派" class="headerlink" title="1.1 人工智能三学派"></a>1.1 人工智能三学派</h2><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240305095606298.png" alt="三学派"></p><p>行为主义：机器人的摔倒预测</p><p>符号主义：用公式描述的人工智能，让PC具有了理性思维</p><p>连接主义：仿造人的感性思维</p><h2 id="1-2-神经网络的设计过程"><a href="#1-2-神经网络的设计过程" class="headerlink" title="1.2 神经网络的设计过程"></a>1.2 神经网络的设计过程</h2><p>用神经网络实现鸢尾花的分类：<strong>梯度下降</strong></p><p>目的：找到一组参数w和b，使得损失函数最小。</p><p>梯度：函数对各参数<strong>求偏导</strong>后的向量。 <u>梯度下降的方向是函数减小的方向</u></p><p>梯度下降法：沿损失函数梯度下降的方向，寻找损失函数的最小值，得到最优参数的方法</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240305095654806.png" alt="学习率"></p><p>学习率（lr）：设置过小，收敛缓慢；设置过大，无法收敛（找不到最小值）</p><p>反向传播：从后向前，逐层求损失函数对每层神经元参数的偏导数，迭代更新所有参数。</p><p>损失函数:<br>$$<br>loss &#x3D; （w + 1 )^2<br>$$</p><p>$$<br>\frac{\part loss}{\part w} &#x3D; 2w +2<br>$$</p><p>代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><br>w = tf.Variable(tf.constant(<span class="hljs-number">5</span>, dtype=tf.float32))<br>lr = <span class="hljs-number">0.2</span>  <span class="hljs-comment"># 学习率</span><br>epoch = <span class="hljs-number">40</span>   <span class="hljs-comment"># 循环迭代数</span><br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epoch):  <span class="hljs-comment"># for epoch 定义顶层循环，表示对数据集循环epoch次，此例数据集数据仅有1个w,初始化时候constant赋值为5，循环40次迭代。</span><br>    <span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:  <span class="hljs-comment"># with结构到grads框起了梯度的计算过程。</span><br>        loss = tf.square(w + <span class="hljs-number">1</span>)<br>    grads = tape.gradient(loss, w)  <span class="hljs-comment"># .gradient函数告知谁对谁求导</span><br><br>    w.assign_sub(lr * grads)  <span class="hljs-comment"># .assign_sub 对变量做自减 即：w -= lr*grads 即 w = w - lr*grads</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;After %s epoch,w is %f,loss is %f&quot;</span> % (epoch, w.numpy(), loss))<br><br><span class="hljs-comment"># lr初始值：0.2   请自改学习率  0.001  0.999 看收敛过程</span><br><span class="hljs-comment"># 最终目的：找到 loss 最小 即 w = -1 的最优参数w</span><br><br></code></pre></td></tr></table></figure><p>Output:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python">After <span class="hljs-number">0</span> epoch,w <span class="hljs-keyword">is</span> <span class="hljs-number">2.600000</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">36.000000</span><br>After <span class="hljs-number">1</span> epoch,w <span class="hljs-keyword">is</span> <span class="hljs-number">1.160000</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">12.959999</span><br>After <span class="hljs-number">2</span> epoch,w <span class="hljs-keyword">is</span> <span class="hljs-number">0.296000</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">4.665599</span><br>After <span class="hljs-number">3</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.222400</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">1.679616</span><br>After <span class="hljs-number">4</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.533440</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.604662</span><br>After <span class="hljs-number">5</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.720064</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.217678</span><br>After <span class="hljs-number">6</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.832038</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.078364</span><br>After <span class="hljs-number">7</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.899223</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.028211</span><br>After <span class="hljs-number">8</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.939534</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.010156</span><br>After <span class="hljs-number">9</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.963720</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.003656</span><br>After <span class="hljs-number">10</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.978232</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.001316</span><br>After <span class="hljs-number">11</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.986939</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000474</span><br>After <span class="hljs-number">12</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.992164</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000171</span><br>After <span class="hljs-number">13</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.995298</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000061</span><br>After <span class="hljs-number">14</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.997179</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000022</span><br>After <span class="hljs-number">15</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.998307</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000008</span><br>After <span class="hljs-number">16</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.998984</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000003</span><br>After <span class="hljs-number">17</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.999391</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000001</span><br>After <span class="hljs-number">18</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.999634</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">19</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.999781</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">20</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.999868</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">21</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.999921</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">22</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.999953</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">23</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.999972</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">24</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.999983</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">25</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.999990</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">26</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.999994</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">27</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.999996</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">28</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.999998</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">29</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.999999</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">30</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.999999</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">31</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">1.000000</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">32</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">1.000000</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">33</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">1.000000</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">34</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">1.000000</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">35</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">1.000000</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">36</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">1.000000</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">37</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">1.000000</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">38</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">1.000000</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">39</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">1.000000</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br></code></pre></td></tr></table></figure><h2 id="1-3-张量生成"><a href="#1-3-张量生成" class="headerlink" title="1.3 张量生成"></a>1.3 张量生成</h2><p>张量（Tensor：多维数组 &#x2F;列表  ）        阶 ：张量的维数</p><table><thead><tr><th>维数</th><th>阶</th><th>名</th><th>例</th></tr></thead><tbody><tr><td>0-D</td><td>0</td><td>标量 scalar</td><td>s&#x3D;1</td></tr><tr><td>1-D</td><td>1</td><td>向量 vector</td><td>v&#x3D;[1,2,3]</td></tr><tr><td>2-D</td><td>2</td><td>矩阵 matrix</td><td>m&#x3D;[[1,2],[3,4],[5,6]]</td></tr><tr><td>n-D</td><td>n</td><td>张量 tensor</td><td>t&#x3D;[[[[……]]]] (n个)</td></tr></tbody></table><p>数据类型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">·tf.<span class="hljs-built_in">int</span>   tf.<span class="hljs-built_in">float</span> ...<br>tf.<span class="hljs-built_in">int</span> <span class="hljs-number">32</span>  , tf.<span class="hljs-built_in">float</span> <span class="hljs-number">32</span>  , tf.<span class="hljs-built_in">float</span> <span class="hljs-number">64</span><br>·tf.<span class="hljs-built_in">bool</span><br>tf.constant([true, false])<br>·tf.string<br>tf.constant(<span class="hljs-string">&quot;Hello world!&quot;</span>)<br></code></pre></td></tr></table></figure><p>创建Tensor</p><p><code>tf.constant(张量内容，dtype=数据类型(可选))</code></p><p>创建全为0的张量 <code>tf.zeros(维度)</code>  </p><p>​ 纬度:一维直接写个数；二维[行，列]；多维[n,m,j,k,…..]</p><p>创建全为1的张量 <code>tf.ones(纬度)</code></p><p>创建全为指定值的张量 <code>tf.fill(维度，指定值)</code></p><p>正态分部的随机数，默认值为0,标准差为1</p><p><code>tf.random.normal(纬度，mean=均值，stddev=标准差)</code></p><p>生成截断式正态分布的随机数</p><p><code>tf.random.truncated_normal(纬度，mean=均值，stddev=标准差)</code></p><p>在正态分布中如果随机生成的数据的取值在（$\mu\pm2\sigma$)</p><p>生成均匀分布的随机数</p><p><code>tf.random.uniform(纬度，minval=最小值，maxval=最大值)</code></p><h2 id="1-4-TF2常用函数"><a href="#1-4-TF2常用函数" class="headerlink" title="1.4 TF2常用函数"></a>1.4 TF2常用函数</h2><p>强制tensor转换为该数据类型<br><code>tf.cast (张量名，dtype=数据类型)</code><br>计算张量维度上元素的最小值<br><code>tf.reduce_min (张量名)</code><br>计算张量维度上元素的最大值<br><code>tf.reduce_max (张量名)</code></p><p>理解axis<br>在一个二维张量或数组中，可以通过调整 axis 等于0或1 控制执行维度。<br> axis&#x3D;0代表跨行（经度，down)，而axis&#x3D;1代表跨列（纬度，across)<br> 如果不指定axis，则所有元素参与计算。</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913093248414.png" alt="理解axis"></p><p>计算张量沿着指定维度的平均值<br><code>tf.reduce_mean (张量名，axis=操作轴)</code>  (不指定axis，则对所有元素进行操作)<br>计算张量沿着指定维度的和<br><code>tf.reduce_sum (张量名，axis=操作轴)</code></p><p><code>tf.Variable () </code>将变量标记为“可训练”，被标记的变量会在反向传播<br>中记录梯度信息。神经网络训练中，常用该函数标记待训练参数。<br><code>tf.Variable(初始值)</code><br><code>w = tf.Variable(tf.random.normal([2, 2], mean=0, stddev=1))</code></p><p>TensorFlow中的数学运算<br>对应元素的四则运算：<code>tf.add</code>，<code>tf.subtract</code>，<code>tf.multiply</code>，<code>tf.divide</code>    </p><p> 只有纬度相同的张量才能做四则运算。<br>平方、次方与开方：<code> tf.square</code>，<code>tf.pow</code>，<code>tf.sqrt</code><br>矩阵乘：<code>tf.matmul</code></p><p>切分传入张量的第一维度，生成输入特征&#x2F;标签对，构建数据集<br><code>data = tf.data.Dataset.from_tensor_slices((输入特征, 标签))</code><br>（Numpy和Tensor格式都可用该语句读入数据）</p><p><code>tf.GradientTape</code><br>with结构记录计算过程，gradient求出张量的梯度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> tf.GradientTape( ) <span class="hljs-keyword">as</span> tape:<br>若干个计算过程<br>grad=tape.gradient(函数，对谁求导)<br></code></pre></td></tr></table></figure><p>enumerate是python的内建函数，它可遍历每个元素(如列表、元组<br>或字符串)，组合为：索引 元素，常在for循环中使用。<br><code>enumerate(列表名)</code></p><p>独热编码：在分类问题中，常用独热码做标签，标记类别：1表示是，0表示非。 <code>tf.one_hot (待转换数据, depth=几分类)</code></p><p>当n分类的n个输出 （y0 ，y1, …… yn-1）通过softmax( ) 函数，<br>便符合概率分布了。也就是说，将多个权重占比划分归为1。<br>$$<br>\forall x \ \ P(X &#x3D; x) \in [0,1] 且 \sum_{x}P(X &#x3D; x) &#x3D; 1<br>$$</p><p>assign_sub 赋值操作，更新参数的值并返回。<br>调用assign_sub前，先用 tf.Variable 定义变量 w 为可训练（可自更新）。<br>w.assign_sub (w要自减的内容)</p><p>返回张量沿指定维度最大值的索引<br>tf.argmax (张量名,axis&#x3D;操作轴)     numpy中也有类似函数</p><h2 id="1-5-鸢尾花数据集的读入"><a href="#1-5-鸢尾花数据集的读入" class="headerlink" title="1.5 鸢尾花数据集的读入"></a>1.5 鸢尾花数据集的读入</h2><p>Setosa Iris（狗尾草鸢尾），Versicolour Iris（杂色鸢尾），Virginica Iris（弗吉尼亚鸢尾）</p><p>鸢尾花数据来源：sklearn框架</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913105626307.png" alt="3种鸢尾花"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets <br><span class="hljs-keyword">from</span> pandas <span class="hljs-keyword">import</span> DataFrame<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br>x_data = datasets.load_iris().data  <span class="hljs-comment"># .data返回iris数据集所有输入特征</span><br>y_data = datasets.load_iris().target  <span class="hljs-comment"># .target返回iris数据集所有标签</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;x_data from datasets: \n&quot;</span>, x_data)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;y_data from datasets: \n&quot;</span>, y_data)<br><br>x_data = DataFrame(x_data, columns=[<span class="hljs-string">&#x27;花萼长度&#x27;</span>, <span class="hljs-string">&#x27;花萼宽度&#x27;</span>, <span class="hljs-string">&#x27;花瓣长度&#x27;</span>, <span class="hljs-string">&#x27;花瓣宽度&#x27;</span>])  <span class="hljs-comment"># 为表格增加行索引（左侧）和列标签（上方）</span><br>pd.set_option(<span class="hljs-string">&#x27;display.unicode.east_asian_width&#x27;</span>, <span class="hljs-literal">True</span>)  <span class="hljs-comment"># 设置列名对齐</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;x_data add index: \n&quot;</span>, x_data)<br><br>x_data[<span class="hljs-string">&#x27;类别&#x27;</span>] = y_data  <span class="hljs-comment"># 新加一列，列标签为‘类别’，数据为y_data</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;x_data add a column: \n&quot;</span>, x_data)<br><br><span class="hljs-comment"># 类型维度不确定时，建议用print函数打印出来确认效果</span><br></code></pre></td></tr></table></figure><h2 id="1-8-神经网络实现鸢尾花的分类"><a href="#1-8-神经网络实现鸢尾花的分类" class="headerlink" title="1.8 神经网络实现鸢尾花的分类"></a>1.8 神经网络实现鸢尾花的分类</h2><p>1.准备数据</p><p>​数据集读入</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 从sklearn包datasets 读入数据集：</span><br><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> datasets<br>x_data = datasets.load_iris().data <span class="hljs-comment"># 返回iris数据集所有输入特征</span><br>y_data = datasets.load_iris().target <span class="hljs-comment"># 返回iris数据集所有标签</span><br></code></pre></td></tr></table></figure><p>​数据集乱序</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">np.random.seed(<span class="hljs-number">116</span>) <span class="hljs-comment"># 使用相同的seed，使输入特征/标签一一对应</span><br>np.random.shuffle(x_data)<br>np.random.seed(<span class="hljs-number">116</span>)<br>np.random.shuffle(y_data)<br>tf.random.set_seed(<span class="hljs-number">116</span>)<br></code></pre></td></tr></table></figure><p>​分成用不相见的训练集和测试集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">x_train = x_data[:-<span class="hljs-number">30</span>]<br>y_train = y_data[:-<span class="hljs-number">30</span>]<br>x_test = x_data[-<span class="hljs-number">30</span>:]<br>y_test = y_data[-<span class="hljs-number">30</span>:]<br></code></pre></td></tr></table></figure><p>​配成【输入特征，标签】对，每次喂入一个batch</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(<span class="hljs-number">32</span>)<br>test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(<span class="hljs-number">32</span>)<br></code></pre></td></tr></table></figure><p>2.搭建网络</p><p>​定义神经网络中的所有可训练参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">w1 = tf.Variable(tf.random.truncated_normal([ <span class="hljs-number">4</span>, <span class="hljs-number">3</span> ], stddev=<span class="hljs-number">0.1</span>, seed=<span class="hljs-number">1</span>)) <span class="hljs-comment"># 四种特征，三个结果</span><br>b1 = tf.Variable(tf.random.truncated_normal([ <span class="hljs-number">3</span> ], stddev=<span class="hljs-number">0.1</span>, seed=<span class="hljs-number">1</span>))<br></code></pre></td></tr></table></figure><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913140637316.png" alt="输入层与输出层"></p><p>3.参数优化</p><p>​嵌套循环迭代，with结构更新参数，显示当前loss</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epoch): <span class="hljs-comment">#数据集级别迭代</span><br><span class="hljs-keyword">for</span> step, (x_train, y_train) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_db): <span class="hljs-comment">#batch级别迭代</span><br><span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape: <span class="hljs-comment"># 记录梯度信息</span><br>前向传播过程计算y<br>计算总loss<br>grads = tape.gradient(loss, [ w1, b1 ])<br>w1.assign_sub(lr * grads[<span class="hljs-number">0</span>]) <span class="hljs-comment">#参数自更新</span><br>b1.assign_sub(lr * grads[<span class="hljs-number">1</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Epoch &#123;&#125;, loss: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(epoch, loss_all/<span class="hljs-number">4</span>))<br></code></pre></td></tr></table></figure><p>4.测试效果</p><p>​计算当前参数前向传播后的准确率，显示当前acc</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> x_test, y_test <span class="hljs-keyword">in</span> test_db:<br>y = tf.matmul(h, w) + b <span class="hljs-comment"># y为预测结果</span><br>y = tf.nn.softmax(y)<br><span class="hljs-comment"># y符合概率分布</span><br>pred = tf.argmax(y, axis=<span class="hljs-number">1</span>) <span class="hljs-comment"># 返回y中最大值的索引，即预测的分类</span><br>pred = tf.cast(pred, dtype=y_test.dtype) <span class="hljs-comment">#调整数据类型与标签一致</span><br>correct = tf.cast(tf.equal(pred, y_test), dtype=tf.int32)<br>correct = tf.reduce_sum (correct) <span class="hljs-comment"># 将每个batch的correct数加起来</span><br>total_correct += <span class="hljs-built_in">int</span> (correct) <span class="hljs-comment"># 将所有batch中的correct数加起来</span><br>total_number += x_test.shape [<span class="hljs-number">0</span>]<br>acc = total_correct / total_number<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;test_acc:&quot;</span>, acc)<br></code></pre></td></tr></table></figure><p>5.acc &#x2F; loss 可视化（查看效果）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.title(<span class="hljs-string">&#x27;Acc Curve&#x27;</span>) <span class="hljs-comment"># 图片标题</span><br>plt.xlabel(<span class="hljs-string">&#x27;Epoch&#x27;</span>) <span class="hljs-comment"># x轴名称</span><br>plt.ylabel(<span class="hljs-string">&#x27;Acc&#x27;</span>) <span class="hljs-comment"># y轴名称</span><br>plt.plot(test_acc, label=<span class="hljs-string">&quot;$Accuracy$&quot;</span>) <span class="hljs-comment"># 逐点画出test_acc值并连线</span><br>plt.legend()<br>plt.show()<br></code></pre></td></tr></table></figure><h2 id="2-1-预备知识"><a href="#2-1-预备知识" class="headerlink" title="2.1 预备知识"></a>2.1 预备知识</h2><p>函数：</p><p><code>tf.where()</code>  条件语句真返回A，条件语句假返回B<br><code>tf.where(条件语句，真返回A，假返回B)</code></p><p><code>np.random.RandomState.rand()</code>返回一个[0,1)之间的随机数<br><code>np.random.RandomState.rand(维度) #维度为空，返回标量</code></p><p><code>np.vstack()</code>将两个数组按垂直方向叠加<br><code>np.vstack(数组1，数组2)</code></p><p><code>np.mgrid[ ] </code>返回间隔数值点，可同时返回多组， [起始值 结束值)<br><code>np.mgrid[ 起始值 : 结束值 : 步长 ，起始值 : 结束值 : 步长 , … ]</code></p><p><code> x.ravel( )</code> 将x变为一维数组，“把. 前变量拉直”<br><code>np.c\_[ ] </code>使返回的间隔数值点配对<br><code>np.c\_[ 数组1，数组2， … ]</code></p><h2 id="2-2-复杂度学习率"><a href="#2-2-复杂度学习率" class="headerlink" title="2.2 复杂度学习率"></a>2.2 复杂度学习率</h2><p>NN复杂度：多用NN层数和NN参数的个数表示</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913145516199.png" alt="复杂度"></p><p>空间复杂度：<br>    层数 &#x3D; 隐藏层的层数 + 1个输出层<br>    图为2层NN<br>                总参数 &#x3D; 总w + 总b<br>                图中 3x4+4 + 4x2+2 &#x3D; 26</p><p>时间复杂度：<br>    乘加运算次数<br>    左图 3x4 +  4x2 &#x3D; 20</p><p>学习率：</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913150028635.png" alt="学习率"></p><p>指数衰减学习率：<br>可以先用较大的学习率，快速得到较优解，然后逐步减小学习率，使<br>模型在训练后期稳定。<br><code>指数衰减学习率 = 初始学习率 * 学习率衰减率（ 当前轮数 / 多少轮衰减一次 ）</code></p><h2 id="2-3-激活函数"><a href="#2-3-激活函数" class="headerlink" title="2.3 激活函数"></a>2.3 激活函数</h2><p>优秀的激活函数：<br>• 非线性： 激活函数非线性时，多层神经网络可逼近所有函数<br>• 可微性： 优化器大多用梯度下降更新参数<br>• 单调性： 当激活函数是单调的，能保证单层网络的损失函数是凸函数<br>• 近似恒等性： f(x)≈x当参数初始化为随机小值时，神经网络更稳定</p><p>激活函数输出值的范围：<br>• 激活函数输出为有限值时，基于梯度的优化方法更稳定<br>• 激活函数输出为无限值时，建议调小学习率</p><p>Sigmoid函数：<br>$$<br>f(x) &#x3D; \frac{1}{1 + e ^ {-x}}<br>$$<br><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913151017142.png" alt="Sigmoid函数"></p><p>特点<br>（1）易造成梯度消失<br>（2）输出非0均值，收敛慢<br>（3）幂运算复杂，训练时间长<br>目前Sigmoid函数因计算复杂，已接近弃用。</p><p>Tanh函数：<br>$$<br>f(x) &#x3D; \frac{1-e^{-2x}}{1+e^{-2x}}<br>$$<br><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913151343657.png" alt="Tanh函数"></p><p>特点<br>（1）输出是0均值<br>（2）易造成梯度消失<br>（3）幂运算复杂，训练时间长</p><p>Relu函数：<br>$$<br>f(x) &#x3D; max(x , 0) &#x3D; \begin{cases}0 \quad x&lt;0 \\ x \quad x\geq0 \end{cases}<br>$$<br><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913152152906.png" alt="Relu函数"></p><p>优点：<br>（1） 解决了梯度消失问题 (在正区间)<br>（2） 只需判断输入是否大于0，计算速度快<br>（3） 收敛速度远快于sigmoid和tanh</p><p>缺点：<br>（1） 输出非0均值，收敛慢<br>（2） Dead RelU问题：某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。（神经元死亡）</p><p>Leaky Relu函数：<br>$$<br>f(x) &#x3D; max (ax,x)<br>$$<br><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913152543744.png" alt="Leaky Relu函数"></p><p>理论上来讲，Leaky Relu有Relu的所有优点，外加不会有Dead Relu问题，但是在实际操作当中，并没有完全证明Leaky Relu总是好于Relu。</p><p>对于初学者的建议：<br>首选relu激活函数；<br>学习率设置较小值；<br>输入特征标准化，即让输入特征满足以0为均值，1为标准差的正态分布；<br>初始参数中心化，即让随机生成的参数满足以0为均值,$\sqrt{\frac{2}{当前层输入特征个数}}$为标准差的正态分布。</p><h2 id="2-4-损失函数"><a href="#2-4-损失函数" class="headerlink" title="2.4 损失函数"></a>2.4 损失函数</h2><p>预测值（y）与已知答案（_y）的差距</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913153356947.png" alt="主流的三种计算方法"></p><p>均方误差mse：<br>$$<br>MSE(y_,y)&#x3D;\frac{\sum_{i&#x3D;1}^n (y-y_)^2}{n}<br>$$<br><code>lost_mse = tf.reduce_mean(tf.square(y_-y))</code></p><p>自定义函数：</p><p>可在一定程度上优化实际问题中的预测误差。</p><p>交叉熵CE：</p><p>表明了两个概率分布之间的距离，交叉熵越大，表明两个概率分布越远<br>$$<br>H(y_,y) &#x3D; - \sum y_ \times ln\ y<br>$$<br>交叉熵越小，证明数据距离真实越准确。</p><p>softmax与交叉熵的结合：</p><p>在TensorFlow中提供了函数<code>tf.nn.softmax_cross_entropy_with_logits(y_，y)</code>输出先过softmax函数，再计算y与y_的交叉熵损失函数。</p><h2 id="2-5-缓解过拟合"><a href="#2-5-缓解过拟合" class="headerlink" title="2.5 缓解过拟合"></a>2.5 缓解过拟合</h2><p>欠拟合与过拟合</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913162344502.png" alt="欠拟合与过拟合"></p><table><thead><tr><th>欠拟合的解决方法：<br/>增加输入特征项<br/>增加网络参数<br/>减少正则化参数</th><th>过拟合的解决方法：<br/>数据清洗<br/>增大训练集<br/>采用正则化<br/>增大正则化参数</th></tr></thead></table><p>正则化缓解过拟合：</p><p>正则化在损失函数中引入模型复杂度指标，利用给W加权值，弱化了训练<br>数据的噪声（一般不正则化b）</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913162811491.png" alt="正则化"></p><p>正则化的选择<br>L1正则化大概率会使很多参数变为零，因此该方法可通过稀疏参数，即减少参数的数量，降低复杂度。<br>L2正则化会使参数很接近零但不为零，因此该方法可通过减小参数值的大小降低复杂度。</p><p>使用L2正则化缓解过拟合</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:  <span class="hljs-comment"># 记录梯度信息</span><br><br>           h1 = tf.matmul(x_train, w1) + b1  <span class="hljs-comment"># 记录神经网络乘加运算</span><br>           h1 = tf.nn.relu(h1)<br>           y = tf.matmul(h1, w2) + b2<br><br>           <span class="hljs-comment"># 采用均方误差损失函数mse = mean(sum(y-out)^2)</span><br>           loss_mse = tf.reduce_mean(tf.square(y_train - y))<br>           <span class="hljs-comment"># 添加l2正则化</span><br>           loss_regularization = []<br>           <span class="hljs-comment"># tf.nn.l2_loss(w)=sum(w ** 2) / 2</span><br>           loss_regularization.append(tf.nn.l2_loss(w1))<br>           loss_regularization.append(tf.nn.l2_loss(w2))<br>           <span class="hljs-comment"># 求和</span><br>           <span class="hljs-comment"># 例：x=tf.constant(([1,1,1],[1,1,1]))</span><br>           <span class="hljs-comment">#   tf.reduce_sum(x)</span><br>           <span class="hljs-comment"># &gt;&gt;&gt;6</span><br>           loss_regularization = tf.reduce_sum(loss_regularization)<br>           loss = loss_mse + <span class="hljs-number">0.03</span> * loss_regularization  <span class="hljs-comment"># REGULARIZER = 0.03</span><br><br>       <span class="hljs-comment"># 计算loss对各个参数的梯度</span><br>       variables = [w1, b1, w2, b2]<br>       grads = tape.gradient(loss, variables)<br></code></pre></td></tr></table></figure><p>下表可以看出，L2正则化函数可有效的缓解过饱和现象</p><table><thead><tr><th><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913170454005.png" alt="未填加L2正则化"></th><th><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913170523568.png" alt="加入了L2正则化"></th></tr></thead></table><h2 id="2-6-优化器"><a href="#2-6-优化器" class="headerlink" title="2.6 优化器"></a>2.6 优化器</h2><p>是引导神经网络更新参数的工具。</p><p>神经网络参数优化器：<br>待优化参数𝒘，损失函数loss，学习率lr，每次迭代一个batch，t表示当前batch迭代的总次数：</p><ol><li>计算t时刻损失函数关于当前参数的梯度 $g_t&#x3D;\triangledown loss &#x3D; \frac{\partial loss}{\partial (w_t)} $</li><li>计算t时刻一阶动量 $m_t$ 和二阶动量$V_t$</li><li>计算t时刻下降梯度：$\eta_t &#x3D;\frac{lr·m_t}{\sqrt{V_t}}$</li><li>计算t+1时刻参数：$w_{t+1} &#x3D; w_t - \eta_t &#x3D; w_t - \frac{lr·m_t}{\sqrt{V_t}}$</li></ol><p>一阶动量：与梯度相关的函数<br>二阶动量：与梯度平方相关的函数</p><p>SGD(无momentum)，常用的梯度下降算法：</p><p>$m_t &#x3D; g_t $ $V_t &#x3D; 1$</p><p>$\eta_t &#x3D; \frac{lr·m_t}{\sqrt{V_t}} $</p><p>$w_{t+1} &#x3D; w_t -\eta_t &#x3D; w_t - \frac{lr·m_t}{\sqrt{V_t}} \ &#x3D;w_t - lr·g_t $</p><p>$w_{t+1} &#x3D; w_t -lr \ast \frac{\partial loss}{\partial w_t}  \ 参数更新公式$</p><p>SGDM(含momentum的SGD)，在SGD的基础上增加了一阶动量:</p><p>$m_{t-1}$表示上一时刻的一阶动量。</p><p>$m_t &#x3D; \beta · m_{t-1} + (1-\beta )·g_t $      $V_t&#x3D;1$</p><p>$\eta_t&#x3D;  \frac{lr·m_t}{\sqrt{V_t}} &#x3D; lr· m_t &#x3D;lr·(\beta · m_{t-1}+(1-\beta)·g_t)$</p><p>$w_{t+1}&#x3D;w_t -\eta_t &#x3D; w_t - lr · (\beta · m_{t-1}+(1-\beta)·g_t)$</p><p>Adagrad，在SGD基础上增加二阶动量:</p><p>$m_t&#x3D;g_t$     $V_t&#x3D;\sum_{\tau&#x3D;1}^t g_\tau^2$</p><p>$\eta_t&#x3D;\frac{lr·m_t}{\sqrt{V_t}} &#x3D;\frac{lr·g_t}{\sqrt{\sum_{\tau&#x3D;1}^t g_\tau^2}} $</p><p>$w_{t+1}&#x3D;w_t-\eta_t&#x3D;w_t-\frac{lr·g_t}{\sqrt{\sum_{\tau&#x3D;1}^t g_\tau^2}}$</p><p>RMSProp，SGD基础上增加二阶动量:</p><p>$m_t&#x3D;g_t$      $V_t &#x3D; \beta · V_{t-1} + (1-\beta)·g_t^2 $</p><p>$\eta_t&#x3D;\frac{lr·m_t}{\sqrt{V_t}} &#x3D;\frac{lr·g_t}{\sqrt{ \beta · V_{t-1} + (1-\beta)·g_t^2}} $</p><p>$w_{t+1}&#x3D;w_t-\eta_t&#x3D;w_t-\frac{lr·g_t}{\sqrt{ \beta · V_{t-1} + (1-\beta)·g_t^2}}$</p><p>Adam, 同时结合SGDM一阶动量和RMSProp二阶动量:</p><p>$m_t&#x3D;\beta_1 ·m_{t-1}+(1-\beta_1)·g_t$</p><p>修正一阶动量的偏差：$\widehat{m_t}&#x3D;\frac{m_t}{1-\beta_1^t}$</p><p>$V_t&#x3D;\beta_2 · V_{step-1}+(1-\beta_2)·g_t^2$</p><p>修正二阶动量的偏差：$ \widehat{V_t}&#x3D; \frac{V_t} {1-\beta_2^t} $</p><p>$\eta_t&#x3D;\frac{lr·\widehat{m_t} }{\sqrt{\widehat{V_t} } } &#x3D; lr \cdot \frac{lr{\frac{m_t}{1-\beta_1^t} } }{\sqrt{ {\frac{V_t} {1-\beta_2^t} } } } $</p><p>$w_{t+1}&#x3D;w_t-\eta_t&#x3D;w_t-\frac{lr\cdot{\frac{m_t}{1-\beta_1^t} } }{\sqrt{ {\frac{V_t}{1-\beta_2^t} } } } $</p><h2 id="3-1-搭建网络八股Sequential"><a href="#3-1-搭建网络八股Sequential" class="headerlink" title="3.1 搭建网络八股Sequential"></a>3.1 搭建网络八股Sequential</h2><p>用Tensorflow API：<code>tf.keras</code>搭建网络八股<br>六步法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span>    <span class="hljs-comment">#引入相关模块</span><br>train, test  <span class="hljs-comment">#告知喂入网络的训练集和测试集    特征x_train和标签y_train </span><br>model = tf.keras.models.Sequential <span class="hljs-comment">#搭建网络解构</span><br>model.<span class="hljs-built_in">compile</span> <span class="hljs-comment"># 配置训练方法（选择优化器、损失函数、评测指标）</span><br>model.fit <span class="hljs-comment"># 训练过程，告知train、test，告知batch、迭代次数</span><br>model.summary <span class="hljs-comment"># 打印网络解构、参数统计</span><br></code></pre></td></tr></table></figure><p><code>model = tf.keras.models.Sequential ([ 网络结构 ]) #描述各层网络</code><br>Sequential是容器，给出从输入层到输出层的各层网络解构</p><p>拉直层： tf.keras.layers.Flatten( )</p><p>全连接层： <code>tf.keras.layers.Dense(神经元个数, activation= &quot;激活函数“ ,kernel_regularizer=哪种正则化)</code><br>activation（字符串给出）可选: relu、 softmax、 sigmoid 、 tanh<br>kernel_regularizer可选:tf.keras.regularizers.l1()tf.keras.regularizers.l2()</p><p>卷积层： tf.keras.layers.Conv2D(filters &#x3D; 卷积核个数, kernel_size &#x3D; 卷积核尺寸,<br>strides &#x3D; 卷积步长， padding &#x3D; “ valid” or “same”)</p><p>LSTM层： tf.keras.layers.LSTM()</p><p><code>model.compile(optimizer = 优化器,loss = 损失函数 metrics = [“准确率”] )</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Optimizer可选:</span><br>‘sgd’ <span class="hljs-keyword">or</span> tf.keras.optimizers.SGD (lr=学习率,momentum=动量参数)<br>‘adagrad’ <span class="hljs-keyword">or</span> tf.keras.optimizers.Adagrad (lr=学习率)<br>‘adadelta’ <span class="hljs-keyword">or</span> tf.keras.optimizers.Adadelta (lr=学习率)<br>‘adam’ <span class="hljs-keyword">or</span> tf.keras.optimizers.Adam (lr=学习率, beta_1=<span class="hljs-number">0.9</span>, beta_2=<span class="hljs-number">0.999</span>)<br><span class="hljs-comment">#　loss可选:</span><br>‘mse’ <span class="hljs-keyword">or</span> tf.keras.losses.MeanSquaredError()<br>‘sparse_categorical_crossentropy’ <span class="hljs-keyword">or</span> tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="hljs-literal">False</span>)<br><span class="hljs-comment"># Metrics可选:</span><br>‘accuracy’ ：y_和y都是数值，如y_=[<span class="hljs-number">1</span>] y=[<span class="hljs-number">1</span>]<br>‘categorical_accuracy’ ：y_和y都是独热码(概率分布)，如y_=[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>] y=[<span class="hljs-number">0.256</span>,<span class="hljs-number">0.695</span>,<span class="hljs-number">0.048</span>]<br>‘sparse_categorical_accuracy’ ：y_是数值，y是独热码(概率分布),如y_=[<span class="hljs-number">1</span>] y=[<span class="hljs-number">0.256</span>,<span class="hljs-number">0.695</span>,<span class="hljs-number">0.048</span>]<br></code></pre></td></tr></table></figure><p><code>model.fit (训练集的输入特征, 训练集的标签, batch_size= , epochs= , validation_data=(测试集的输入特征，测试集的标签), validation_split=从训练集划分多少比例给测试集， validation_freq = 多少次epoch测试一次)</code></p><p>使用sequential可以搭建出上层输出就是下层输入的下层网络机构，但无法写出一些带有跳连的非顺序网络结构，这时候可以选择用类Class搭建神经网络解构。</p><h2 id="3-2-搭建网络八股Class"><a href="#3-2-搭建网络八股Class" class="headerlink" title="3.2 搭建网络八股Class"></a>3.2 搭建网络八股Class</h2><p>在六步法中，将第三部的Model改为<code>class MyModel(Model) model=MyModel</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyModel</span>(<span class="hljs-title class_ inherited__">Model</span>): <span class="hljs-comment"># 继承了Tensorflow的model类</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br><span class="hljs-built_in">super</span>(MyModel, self).__init__()<br>定义网络结构块<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">call</span>(<span class="hljs-params">self, x</span>):<br>调用网络结构块，实现前向传播<br><span class="hljs-keyword">return</span> y<br>model = MyModel()<br><br><br><span class="hljs-comment">#__init__( )  定义所需网络结构块</span><br><span class="hljs-comment">#call( )  写出前向传播 实现钱前向传播</span><br><br></code></pre></td></tr></table></figure><h2 id="3-3-MNIST数据集"><a href="#3-3-MNIST数据集" class="headerlink" title="3.3 MNIST数据集"></a>3.3 MNIST数据集</h2><p>手写数字的数据集-上万张</p><p>导入MNIST数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"> mnist = tf.keras.datasets.mnist<br>(x_train, y_train) , (x_test, y_test) = mnist.load_data()<br></code></pre></td></tr></table></figure><p>为输入特征，输入神经网络时，将数据拉伸为一维数组：<br><code>tf.keras.layers.Flatten( )</code></p><h2 id="3-4-FASHION数据集"><a href="#3-4-FASHION数据集" class="headerlink" title="3.4 FASHION数据集"></a>3.4 FASHION数据集</h2><p>提供 6万张 28X28 像素点的衣裤等图片和标签，用于训练。<br>提供 1万张 28X28 像素点的衣裤等图片和标签，用于测试。</p><p>导入FASHION数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">fashion = tf.keras.datasets.fashion_mnist<br>(x_train, y_train),(x_test, y_test) = fashion.load_data()<br></code></pre></td></tr></table></figure><h2 id="4-1-搭建网络八股总览"><a href="#4-1-搭建网络八股总览" class="headerlink" title="4.1 搭建网络八股总览"></a>4.1 搭建网络八股总览</h2><p>① 自制数据集，解决本领域应用<br>② 数据增强，扩充数据集<br>③ 断点续训，存取模型<br>④ 参数提取，把参数存入文本<br>⑤ acc&#x2F;loss可视化，查看训练效果<br>⑥ 应用程序，给图识物</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230916150852673.png" alt="六步法八股总览"></p><h2 id="4-2-自制数据集"><a href="#4-2-自制数据集" class="headerlink" title="4.2 自制数据集"></a>4.2 自制数据集</h2><p>使用Py，目的是将文件夹内的图片读入，返回输入特征、标签。</p><p>标签文件txt</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> os<br><span class="hljs-comment">### import</span><br><br><br>train_path = <span class="hljs-string">&#x27;./mnist_image_label/mnist_train_jpg_60000/&#x27;</span><br>train_txt = <span class="hljs-string">&#x27;./mnist_image_label/mnist_train_jpg_60000.txt&#x27;</span><br>x_train_savepath = <span class="hljs-string">&#x27;./mnist_image_label/mnist_x_train.npy&#x27;</span><br>y_train_savepath = <span class="hljs-string">&#x27;./mnist_image_label/mnist_y_train.npy&#x27;</span><br><span class="hljs-comment">### 训练集</span><br><br><br>test_path = <span class="hljs-string">&#x27;./mnist_image_label/mnist_test_jpg_10000/&#x27;</span><br>test_txt = <span class="hljs-string">&#x27;./mnist_image_label/mnist_test_jpg_10000.txt&#x27;</span><br>x_test_savepath = <span class="hljs-string">&#x27;./mnist_image_label/mnist_x_test.npy&#x27;</span><br>y_test_savepath = <span class="hljs-string">&#x27;./mnist_image_label/mnist_y_test.npy&#x27;</span><br><br><span class="hljs-comment">### 测试集</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">generateds</span>(<span class="hljs-params">path, txt</span>):<br>    f = <span class="hljs-built_in">open</span>(txt, <span class="hljs-string">&#x27;r&#x27;</span>)  <span class="hljs-comment"># 以只读形式打开txt文件</span><br>    contents = f.readlines()  <span class="hljs-comment"># 读取文件中所有行</span><br>    f.close()  <span class="hljs-comment"># 关闭txt文件</span><br>    x, y_ = [], []  <span class="hljs-comment"># 建立空列表</span><br>    <span class="hljs-keyword">for</span> content <span class="hljs-keyword">in</span> contents:  <span class="hljs-comment"># 逐行取出</span><br>        value = content.split()  <span class="hljs-comment"># 以空格分开，图片路径为value[0] , 标签为value[1] , 存入列表</span><br>        img_path = path + value[<span class="hljs-number">0</span>]  <span class="hljs-comment"># 拼出图片路径和文件名</span><br>        img = Image.<span class="hljs-built_in">open</span>(img_path)  <span class="hljs-comment"># 读入图片</span><br>        img = np.array(img.convert(<span class="hljs-string">&#x27;L&#x27;</span>))  <span class="hljs-comment"># 图片变为8位宽灰度值的np.array格式</span><br>        img = img / <span class="hljs-number">255.</span>  <span class="hljs-comment"># 数据归一化 （实现预处理）</span><br>        x.append(img)  <span class="hljs-comment"># 归一化后的数据，贴到列表x</span><br>        y_.append(value[<span class="hljs-number">1</span>])  <span class="hljs-comment"># 标签贴到列表y_</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;loading : &#x27;</span> + content)  <span class="hljs-comment"># 打印状态提示</span><br><br>    x = np.array(x)  <span class="hljs-comment"># 变为np.array格式</span><br>    y_ = np.array(y_)  <span class="hljs-comment"># 变为np.array格式</span><br>    y_ = y_.astype(np.int64)  <span class="hljs-comment"># 变为64位整型</span><br>    <span class="hljs-keyword">return</span> x, y_  <span class="hljs-comment"># 返回输入特征x，返回标签y_</span><br><br><span class="hljs-comment">### generateds函数</span><br><br><br><span class="hljs-keyword">if</span> os.path.exists(x_train_savepath) <span class="hljs-keyword">and</span> os.path.exists(y_train_savepath) <span class="hljs-keyword">and</span> os.path.exists(<br>        x_test_savepath) <span class="hljs-keyword">and</span> os.path.exists(y_test_savepath):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;-------------Load Datasets-----------------&#x27;</span>)<br>    x_train_save = np.load(x_train_savepath)<br>    y_train = np.load(y_train_savepath)<br>    x_test_save = np.load(x_test_savepath)<br>    y_test = np.load(y_test_savepath)<br>    x_train = np.reshape(x_train_save, (<span class="hljs-built_in">len</span>(x_train_save), <span class="hljs-number">28</span>, <span class="hljs-number">28</span>))<br>    x_test = np.reshape(x_test_save, (<span class="hljs-built_in">len</span>(x_test_save), <span class="hljs-number">28</span>, <span class="hljs-number">28</span>))<br><span class="hljs-keyword">else</span>:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;-------------Generate Datasets-----------------&#x27;</span>)<br>    x_train, y_train = generateds(train_path, train_txt)<br>    x_test, y_test = generateds(test_path, test_txt)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;-------------Save Datasets-----------------&#x27;</span>)<br>    x_train_save = np.reshape(x_train, (<span class="hljs-built_in">len</span>(x_train), -<span class="hljs-number">1</span>))<br>    x_test_save = np.reshape(x_test, (<span class="hljs-built_in">len</span>(x_test), -<span class="hljs-number">1</span>))<br>    np.save(x_train_savepath, x_train_save)<br>    np.save(y_train_savepath, y_train)<br>    np.save(x_test_savepath, x_test_save)<br>    np.save(y_test_savepath, y_test)<br><br><span class="hljs-comment">### train test</span><br>    <br>    <br>    <br>model = tf.keras.models.Sequential([<br>    tf.keras.layers.Flatten(),<br>    tf.keras.layers.Dense(<span class="hljs-number">128</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>),<br>    tf.keras.layers.Dense(<span class="hljs-number">10</span>, activation=<span class="hljs-string">&#x27;softmax&#x27;</span>)<br>])<br><br><span class="hljs-comment">### models.sequential</span><br><br>model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;adam&#x27;</span>,<br>              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="hljs-literal">False</span>),<br>              metrics=[<span class="hljs-string">&#x27;sparse_categorical_accuracy&#x27;</span>])<br><br><span class="hljs-comment">### model.compile</span><br><br>model.fit(x_train, y_train, batch_size=<span class="hljs-number">32</span>, epochs=<span class="hljs-number">5</span>, validation_data=(x_test, y_test), validation_freq=<span class="hljs-number">1</span>)<br><br><span class="hljs-comment">### model.fit</span><br><br>model.summary()<br><br><span class="hljs-comment">### model.summary</span><br></code></pre></td></tr></table></figure><p>第一次运行生成了npy格式的数据集。</p><p>第二次会加载数据集，执行训练过程。</p><h2 id="4-3-数据增强"><a href="#4-3-数据增强" class="headerlink" title="4.3 数据增强"></a>4.3 数据增强</h2><p>增大数据量,扩充数据集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">image_gen_train =tf.keras.preprocessing.image.ImageDataGenerator(<br>rescale = 所有数据将乘以该数值<br>rotation_range = 随机旋转角度数范围<br>width_shift_range = 随机宽度偏移量<br>height_shift_range = 随机高度偏移量<br>水平翻转：horizontal_flip = 是否随机水平翻转<br>随机缩放：zoom_range = 随机缩放的范围 [<span class="hljs-number">1</span>-n，<span class="hljs-number">1</span>+n] )<br>image_gen_train.fit(x_train)<br></code></pre></td></tr></table></figure><h2 id="4-4-断点续训"><a href="#4-4-断点续训" class="headerlink" title="4.4 断点续训"></a>4.4 断点续训</h2><p>可以存取模型</p><p>读取模型：<br><code>load_weights(路径文件名）</code></p><p>可以先检查是否存在断点，如有则加载模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">cheakpoint_save_path= <span class="hljs-string">&quot;./cheakpoint/mnist.ckpt&quot;</span><br><span class="hljs-keyword">if</span> os.path.exists(checkpoint_save_path + <span class="hljs-string">&#x27;.index&#x27;</span>):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;--------load the model-------&#x27;</span>)<br>    model.load_weights(checkpoint_save_path)<br></code></pre></td></tr></table></figure><p><code>保存模型： tf.keras.callbacks.ModelCheckpoint(filepath=路径文件名,save_weights_only=True/False,save_best_only=True/False) history = model.fit（ callbacks=[cp_callback] ）</code></p><h2 id="4-5-参数提取"><a href="#4-5-参数提取" class="headerlink" title="4.5 参数提取"></a>4.5 参数提取</h2><p>把参数存入文本</p><p>提取可训练参数<br>model.trainable_variables 返回模型中可训练的参数<br>设置print输出格式<br>np.set_printoptions(threshold&#x3D;超过多少省略显示)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">np.set_printoptions(threshold=np.inf)<br><span class="hljs-comment"># np.inf表示无限大</span><br><br><br><span class="hljs-built_in">print</span>(model.trainable_variables)<br>file = <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;./weights.txt&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>)<br><span class="hljs-keyword">for</span> v <span class="hljs-keyword">in</span> model.trainable_variables:<br>file.write(<span class="hljs-built_in">str</span>(v.name) + <span class="hljs-string">&#x27;\n&#x27;</span>)<br>file.write(<span class="hljs-built_in">str</span>(v.shape) + <span class="hljs-string">&#x27;\n&#x27;</span>)<br>file.write(<span class="hljs-built_in">str</span>(v.numpy()) + <span class="hljs-string">&#x27;\n&#x27;</span>)<br>file.close()<br></code></pre></td></tr></table></figure><h2 id="4-6-acc-loss可视化"><a href="#4-6-acc-loss可视化" class="headerlink" title="4.6 acc&#x2F;loss可视化"></a>4.6 acc&#x2F;loss可视化</h2><p>acc曲线与loss曲线</p><p><code>history=model.fit(训练集数据, 训练集标签, batch_size=, epochs=,validation_split=用作测试数据的比例,validation_data=测试集,validation_freq=测试频率)</code></p><p>只是一段画图程序代码。</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230916164058968.png" alt="添加的画图代码"></p><h2 id="4-7-图片识别"><a href="#4-7-图片识别" class="headerlink" title="4.7 图片识别"></a>4.7 图片识别</h2><p>给图识物</p><p><code>predict（输入特征，batch_size=整数）</code>返回向前传播的计算结果</p><p>复现模型（前向传播）:</p><p><code>model = tf.keras.models.Sequential([ tf.keras.layers.Flatten(), tf.keras.layers.Dense(128, activation=&#39;relu&#39;), tf.keras.layers.Dense(10, activation=&#39;softmax’)])</code></p><p>加载参数:</p><p><code>model.load_weights(model_save_path</code></p><p>预测结果:</p><p><code>result = model.predict(x_predict)</code></p><h2 id="5-1-卷积的计算过程-Convolutional"><a href="#5-1-卷积的计算过程-Convolutional" class="headerlink" title="5.1 卷积的计算过程 Convolutional"></a>5.1 卷积的计算过程 Convolutional</h2><p>全连接 NN 特点：每个神经元与前后相邻层的每一个神经元都有连接关系。（可以实<br>现分类和预测）</p><p>全连接网络参数的个数为：$\sum(前层\times 后层 + 后层)$</p><p>卷积的概念：卷积可以认为是一种有效提取图像特征的方法。一般会用一个正方形的<br>卷积核，按指定步长，在输入特征图上滑动，遍历输入特征图中的每个像素点。每一个步长，<br>卷积核会与输入特征图出现重合区域，重合区域对应元素相乘、求和再加上偏置项得到输出<br>特征的一个像素点。</p><p>对于彩色图像（多通道）来说，卷积核通道数与输入特征一致，套接后在对应位置上进行乘加和操作，如果是彩色图片（RGB）利用三通道卷积核对三通道的彩色特征图做卷积计算。</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230916171743300.png" alt="输出特征尺寸计算"></p><h2 id="5-2-感受野"><a href="#5-2-感受野" class="headerlink" title="5.2 感受野"></a>5.2 感受野</h2><p>感受野（Receptive Field）：卷积神经网络各输出特征图中的每个像素点，在原始输入图片上映射区域的大小。</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230916173848124.png" alt="黄-绿为两次3*3 蓝色为一次5*5"></p><p>通常用两层3*3卷积核替换一层5*5卷积核</p><h2 id="5-3-全零填充-Padding"><a href="#5-3-全零填充-Padding" class="headerlink" title="5.3 全零填充 Padding"></a>5.3 全零填充 Padding</h2><p>将图的四周加上一圈零填充。</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230916174816575.png" alt="padding"></p><p>TF描述全零填充<br>用参数padding &#x3D; ‘SAME’ 或 padding &#x3D; ‘VALID’表示</p><p>SAME：5X5X1  –&gt;  5X5X1      VALID：5X5X1–&gt;3X3X1</p><p>可以让输出特征图和输出特征图的尺寸不变。</p><h2 id="5-4-TF描述卷积计算层"><a href="#5-4-TF描述卷积计算层" class="headerlink" title="5.4 TF描述卷积计算层"></a>5.4 TF描述卷积计算层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">tf.keras.layers.Conv2D (<br>filters = 卷积核个数,<br>kernel_size = 卷积核尺寸, <span class="hljs-comment">#正方形写核长整数，或（核高h，核宽w）</span><br>strides = 滑动步长, <span class="hljs-comment">#横纵向相同写步长整数，或(纵向步长h，横向步长w)，默认1</span><br>padding = “same” <span class="hljs-keyword">or</span> “valid”, <span class="hljs-comment">#使用全零填充是“same”，不使用是“valid”（默认）</span><br>activation = “ relu ” <span class="hljs-keyword">or</span> “ sigmoid ” <span class="hljs-keyword">or</span> “ tanh ” <span class="hljs-keyword">or</span> “ softmax”等 , <span class="hljs-comment">#如有BN此处不写</span><br>input_shape = (高, 宽 , 通道数) <span class="hljs-comment">#输入特征图维度，可省略</span><br>)<br></code></pre></td></tr></table></figure><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240228211041340.png" alt="TF描述卷积层"></p><p>例如可以使用关键字传递参数的方法。</p><h2 id="5-5-批标准化-BN"><a href="#5-5-批标准化-BN" class="headerlink" title="5.5 批标准化 BN"></a>5.5 批标准化 BN</h2><p>标准化：使数据符合0均值，1为标准差的分布。</p><p>批标准化：对一小批数据（batch），做标准化处理 。</p><p>批标准化后，第 k个卷积核的输出特征图（feature map）中第 i 个像素点</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240228211544435.png" alt="BN层位于卷积层之后，激活层之前。"></p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240228211600833.png" alt="批标准化"></p><h2 id="5-6-池化-Pooling"><a href="#5-6-池化-Pooling" class="headerlink" title="5.6 池化 Pooling"></a>5.6 池化 Pooling</h2><p>池化用于减少特征数据量。平均池化和最大池化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">tf.keras.layers.MaxPool2D(<br>pool_size=池化核尺寸，<span class="hljs-comment">#正方形写核长整数，或（核高h，核宽w）</span><br>strides=池化步长，<span class="hljs-comment">#步长整数， 或(纵向步长h，横向步长w)，默认为pool_size</span><br>padding=‘valid’<span class="hljs-keyword">or</span>‘same’ <span class="hljs-comment">#使用全零填充是“same”，不使用是“valid”（默认）</span><br>)<br>tf.keras.layers.AveragePooling2D(<br>pool_size=池化核尺寸，<span class="hljs-comment">#正方形写核长整数，或（核高h，核宽w）</span><br>strides=池化步长，<span class="hljs-comment">#步长整数， 或(纵向步长h，横向步长w)，默认为pool_size</span><br>padding=‘valid’<span class="hljs-keyword">or</span>‘same’ <span class="hljs-comment">#使用全零填充是“same”，不使用是“valid”（默认）</span><br>)<br></code></pre></td></tr></table></figure><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240228211847780.png" alt="池化"></p><h2 id="5-7-舍弃-Dropout"><a href="#5-7-舍弃-Dropout" class="headerlink" title="5.7 舍弃 Dropout"></a>5.7 舍弃 Dropout</h2><p>在神经网络训练时，将一部分神经元按照一定概率从神经网络中暂时舍弃。神经网络使用时，被舍弃的神经元恢复链接。</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240228212108454.png" alt="舍弃"></p><h2 id="5-8-卷积神经网络"><a href="#5-8-卷积神经网络" class="headerlink" title="5.8 卷积神经网络"></a>5.8 卷积神经网络</h2><p>卷积是什么？ 卷积就是特征提取器，就是CBAPD</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240228214733104.png" alt="CBAPD"></p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240228214753084.png" alt="CNN"></p><h2 id="5-9-Cifar10数据集-卷积神经网络搭建示例"><a href="#5-9-Cifar10数据集-卷积神经网络搭建示例" class="headerlink" title="5.9 Cifar10数据集 卷积神经网络搭建示例"></a>5.9 Cifar10数据集 卷积神经网络搭建示例</h2><p>提供 5万张 32*32 像素点的十分类彩色图片和标签，用于训练。<br>提供 1万张 32*32 像素点的十分类彩色图片和标签，用于测试。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#导入cifar10数据集：</span><br>cifar10 = tf.keras.datasets.cifar10<br>(x_train, y_train),(x_test, y_test) = cifar10.load_data()<br><br>plt.imshow(x_train[<span class="hljs-number">0</span>])<br><span class="hljs-comment">#绘制图片</span><br>plt.show()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;x_train[0]:\n&quot;</span> , x_train[<span class="hljs-number">0</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;y_train[0]:&quot;</span>, y_train[<span class="hljs-number">0</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;x_test.shape:&quot;</span>, x_test.shape)<br></code></pre></td></tr></table></figure><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240302132316265.png" alt="卷积神经网络搭建示例"></p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240302132135165.png" alt="搭建示例"></p><h2 id="5-10-LeNet-AlexNet-VGGNet-InceptionNet-ResNet"><a href="#5-10-LeNet-AlexNet-VGGNet-InceptionNet-ResNet" class="headerlink" title="5.10 LeNet AlexNet VGGNet InceptionNet ResNet"></a>5.10 LeNet AlexNet VGGNet InceptionNet ResNet</h2><p>LeNet由Yann LeCun于1998年提出，卷积网络开篇之作。</p><p>AlexNet网络诞生于2012年，当年ImageNet竞赛的冠军，Top5错误率为16.4%。</p><p>VGGNet诞生于2014年，当年ImageNet竞赛的亚军，Top5错误率减小到7.3%。</p><p>InceptionNet诞生于2014年，当年ImageNet竞赛冠军，Top5错误率为6.67%</p><p>ResNet诞生于2015年，当年ImageNet竞赛冠军，Top5错误率为3.57%</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240302153943645.png" alt="经典卷积网络"></p><h2 id="6-1-循环核"><a href="#6-1-循环核" class="headerlink" title="6.1 循环核"></a>6.1 循环核</h2><p>循环核：参数时间共享，循环层提取时间信息。</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240302180849506.png" alt="循环核"></p><h2 id="6-2-循环核按时间步展开"><a href="#6-2-循环核按时间步展开" class="headerlink" title="6.2 循环核按时间步展开"></a>6.2 循环核按时间步展开</h2><p>循环神经网络：借助循环核提取时间特征后，送入全连接网络。</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240302181058519.png" alt="循环神经网络"></p><p>循环计算层：向输出方向生长。</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240302181218194.png" alt="循环计算层"></p><h2 id="6-3-TF描述循环计算层"><a href="#6-3-TF描述循环计算层" class="headerlink" title="6.3 TF描述循环计算层"></a>6.3 TF描述循环计算层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">tf.keras.layers.SimpleRNN(记忆体个数，activation=‘激活函数’ ，<br>return_sequences=是否每个时刻输出ht到下一层)<br>activation=‘激活函数’ （不写，默认使用tanh）<br>return_sequences=<span class="hljs-literal">True</span> 各时间步输出ht<br>return_sequences=<span class="hljs-literal">False</span> 仅最后时间步输出ht（默认）<br>例：SimpleRNN(<span class="hljs-number">3</span>, return_sequences=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>入RNN时， x_train维度：<br>[送入样本数， 循环核时间展开步数， 每个时间步输入特征个数]</p><h2 id="6-4-循环计算过程-字母输入预测"><a href="#6-4-循环计算过程-字母输入预测" class="headerlink" title="6.4 循环计算过程-字母输入预测"></a>6.4 循环计算过程-字母输入预测</h2><p>字母预测：输入a预测出b，输入b预测出c，<br>输入c预测出d，输入d预测出e，输入e预测出a</p><p>用RNN实现输入一个字母，预测下一个字母<br>（One hot 编码）独热码</p><p>用RNN实现输入连续四个字母，预测下一个字母<br>（One hot 编码）</p><p>用RNN实现输入一个字母，预测下一个字母<br>（Embedding 编码）</p><p>用RNN实现输入连续四个字母，预测下一个字母<br>（Embedding 编码）</p><h2 id="6-5-股票预测"><a href="#6-5-股票预测" class="headerlink" title="6.5 股票预测"></a>6.5 股票预测</h2><p>用RNN实现股票预测</p><p>用LSTM实现股票预测</p><p>LSTM 由Hochreiter &amp; Schmidhuber 于1997年提出，通过门控单元改善了RNN长期依赖问题。</p><p>用GRU实现股票预测</p><p>GRU由Cho等人于2014年提出，优化LSTM结构。</p><p>更新于：2024 </p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>工具与软件</category>
      
    </categories>
    
    
    <tags>
      
      <tag>TensorFlow</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2-工具与软件-4-PyTorch</title>
    <link href="/2023/09/08/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-4-PyTorch/"/>
    <url>/2023/09/08/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-4-PyTorch/</url>
    
    <content type="html"><![CDATA[<p>安装CUDA</p><p>UPDATEｔｉｍｅ；</p><p>９．２２　１８：００</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>工具与软件</category>
      
    </categories>
    
    
    <tags>
      
      <tag>PyTorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2-工具与软件-3-数据分析实战</title>
    <link href="/2023/09/08/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-3-%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%98/"/>
    <url>/2023/09/08/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-3-%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%98/</url>
    
    <content type="html"><![CDATA[<blockquote><p>使用Python进行数据分析，对其编程、库，以及⽤于数据分析的⼯具的相关学习与研究。</p></blockquote><h2 id="一-准备工作"><a href="#一-准备工作" class="headerlink" title="一.准备工作"></a>一.准备工作</h2><h2 id="1-重要的Python库"><a href="#1-重要的Python库" class="headerlink" title="1 重要的Python库"></a>1 重要的Python库</h2><h3 id="NumPy"><a href="#NumPy" class="headerlink" title="NumPy"></a>NumPy</h3><p>NumPy（Numerical Python的简称）是Python科学计算的基础包。<a href="https://cnwuyueyu.github.io/2023/09/08/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-1-Numpy/">详见</a>它提供了以下功能（不限于此）：</p><ul><li>快速⾼效的多维数组对象ndarray。</li><li>⽤于对数组执⾏元素级计算以及直接对数组执⾏数学运算的函数。</li><li>⽤于读写硬盘上基于数组的数据集的⼯具。</li><li>线性代数运算、傅⾥叶变换，以及随机数⽣成。-成熟的C API， ⽤于Python插件和原⽣CC++、Fortran代码访问NumPy的数据结构和计算⼯具。</li></ul><p>除了为Python提供快速的数组处理能⼒，NumPy在数据分析⽅⾯还有另外⼀个主要作⽤，即作为在算法和库之间传递数据的容器。对于数值型数据，NumPy数组在存储和处理数据时要⽐内<br>置的Python数据结构⾼效得多。此外，由低级语⾔（⽐如C和Fortran）编写的库可以直接操作NumPy数组中的数据，⽆需进⾏任何数据复制⼯作。因此，许多Python的数值计算⼯具要么使<br>⽤NumPy数组作为主要的数据结构，要么可以与NumPy进⾏⽆缝交互操作。</p><h3 id="pandas"><a href="#pandas" class="headerlink" title="pandas"></a>pandas</h3><p>pandas提供了快速便捷处理结构化数据的⼤量数据结构和函数。⾃从2010年出现以来，它助使Python成为强⼤⽽⾼效的数据分析环境。⽤得最多的pandas对象是DataFrame，它是⼀个⾯向列（column-oriented）的⼆维表结构，另⼀个是Series，⼀个⼀维的标签化数组对象。<a href="https://cnwuyueyu.github.io/2023/09/08/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-2-Pandas/">详见</a></p><p>pandas兼具NumPy⾼性能的数组计算功能以及电⼦表格和关系型数据库（如SQL）灵活的数据处理功能。它提供了复杂精细的索引功能，以便更为便捷地完成重塑、切⽚和切块、聚合以及选取数据⼦集等操作。因为数据操作、准备、清洗是数据分析最重要的技能。</p><p>pandas这个名字源于panel data（⾯板数据，这是多维结构化数据集在计量经济学中的术语）以及Python dataanalysis（Python数据分析）。</p><h3 id="matplotlib"><a href="#matplotlib" class="headerlink" title="matplotlib"></a>matplotlib</h3><p>matplotlib是最流⾏的⽤于绘制图表和其它⼆维数据可视化的Python库。它⾮常适合创建出版物上⽤的图表。虽然还有其它的Python可视化库，matplotlib却是使⽤最⼴泛的，并且它和其它⽣态⼯具配合也⾮常完美。</p><h3 id="IPython和Jupyter"><a href="#IPython和Jupyter" class="headerlink" title="IPython和Jupyter"></a>IPython和Jupyter</h3><p>IPython项⽬起初是Fernando Pérez在2001年的⼀个⽤以加强和Python交互的⼦项⽬。在随后的16年中，它成为了Python数据栈最重要的⼯具之⼀。虽然IPython本身没有提供计算和数据分析的⼯具，它却可以⼤⼤提⾼交互式计算和软件开发的⽣产率。IPython⿎励“执⾏-探索”的⼯作流，区别于其它编程软件的“编辑-编译-运⾏”的⼯作流。它还可以⽅便地访问系统的shell和⽂件系统。因为⼤部分的数据分析代码包括探索、试错和重复，IPython可以使⼯作更快。IPython shell 和Jupyter notebooks特别适合进⾏数据探索和可视化。</p><p>⼤部分Python都要⽤到IPython，包括运⾏、调试和测试代码。</p><h3 id="SciPy"><a href="#SciPy" class="headerlink" title="SciPy"></a>SciPy</h3><p>SciPy是⼀组专⻔解决科学计算中各种标准问题域的包的集合，主要包括下⾯这些包：</p><ul><li>scipy.integrate：数值积分例程和微分⽅程求解器。</li><li>scipy.linalg：扩展了由numpy.linalg提供的线性代数例程和矩阵分解功能。</li><li>scipy.optimize：函数优化器（最⼩化器）以及根查找算法。</li><li>scipy.signal：信号处理⼯具。</li><li>scipy.sparse：稀疏矩阵和稀疏线性系统求解器。</li><li>scipy.special：SPECFUN（这是⼀个实现了许多常⽤数学函数（如伽玛函数）的Fortran库）的包装器。</li><li>scipy.stats：标准连续和离散概率分布（如密度函数、采样器、连续分布函数等）、各种统计检验⽅法，以及更好的描述统计法。</li></ul><p>NumPy和SciPy结合使⽤，便形成了⼀个相当完备和成熟的计算平台，可以处理多种传统的科学计算问题。</p><h3 id="scikit-learn"><a href="#scikit-learn" class="headerlink" title="scikit-learn"></a>scikit-learn</h3><p>2010年诞⽣以来，scikit-learn成为了Python的通⽤机器学习⼯具包。它的⼦模块包括：</p><ul><li>分类：SVM、近邻、随机森林、逻辑回归等等。</li><li>回归：Lasso、岭回归等等。</li><li>聚类：k-均值、谱聚类等等。</li><li>降维：PCA、特征选择、矩阵分解等等。</li><li>选型：⽹格搜索、交叉验证、度量。</li><li>预处理：特征提取、标准化。</li></ul><p>与pandas、statsmodels和IPython⼀起，scikit-learn对于Python成为⾼效数据科学编程语⾔起到了关键作⽤。</p><h3 id="statsmodels"><a href="#statsmodels" class="headerlink" title="statsmodels"></a>statsmodels</h3><p>statsmodels是⼀个统计分析包，与scikit-learn⽐较，statsmodels包含经典统计学和经济计量学的算法。包括如下⼦模块：</p><ul><li>回归模型：线性回归，⼴义线性模型，健壮线性模型，线性混合效应模型等等。</li><li>⽅差分析（ANOVA）。</li><li>时间序列分析：AR，ARMA，ARIMA，VAR和其它模型。</li><li>⾮参数⽅法： 核密度估计，核回归。</li><li>统计模型结果可视化。</li></ul><p>statsmodels更关注与统计推断，提供不确定估计和参数p-值。相反的，scikit-learn注重预测。</p><h1 id="二-Python语法基础"><a href="#二-Python语法基础" class="headerlink" title="二.Python语法基础"></a>二.Python语法基础</h1><h2 id="1-IPython"><a href="#1-IPython" class="headerlink" title="1.IPython"></a>1.IPython</h2><h3 id="快捷键"><a href="#快捷键" class="headerlink" title="快捷键"></a>快捷键</h3><p>有许多键盘快捷键进⾏导航提示（类似Emacs⽂本编辑器或UNIX bash Shell）和交互shell的历史命令。</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-3-%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%98/image.ZUBJB2-16953667317141.png" alt="IPython sheel的快捷键"></p><p>魔术命令</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-3-%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%98/image.LEO7A2.png" alt="IPython魔术命令"></p><p>IPython同时集成了Matplotlib</p><p>三.Python的数据结构、函数和⽂件</p><p>P85</p><p>update time:</p><p>2023-09-22 16:57:38.877910</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>工具与软件</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>2-工具与软件-2-Pandas</title>
    <link href="/2023/09/08/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-2-Pandas/"/>
    <url>/2023/09/08/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-2-Pandas/</url>
    
    <content type="html"><![CDATA[<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>在pycharm中对应的python解释器内安装pandas。</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-2-Pandas/image-20230919175623027.png" alt="pandas的安装"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas<br><br><span class="hljs-built_in">print</span>(pandas.__version__)<br></code></pre></td></tr></table></figure><p>Output:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-number">2.1</span><span class="hljs-number">.0</span><br><br>进程已结束,退出代码<span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><h2 id="Pandas-数据结构-Series"><a href="#Pandas-数据结构-Series" class="headerlink" title="Pandas 数据结构 - Series"></a>Pandas 数据结构 - Series</h2><p>Series 相当于表格中的一个列，函数如下：</p><p><code>pandas.Series( data, index, dtype, name, copy)</code></p><ul><li><strong>data</strong>：一组数据(ndarray 类型)。</li><li><strong>index</strong>：数据索引标签，如果不指定，默认从 0 开始。</li><li><strong>dtype</strong>：数据类型，默认会自己判断。</li><li><strong>name</strong>：设置名称。</li><li><strong>copy</strong>：拷贝数据，默认为 False。</li></ul><p>也可以使用Map来创建Series：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br>sites = &#123;<span class="hljs-number">1</span>: <span class="hljs-string">&quot;Google&quot;</span>, <span class="hljs-number">2</span>: <span class="hljs-string">&quot;Runoob&quot;</span>, <span class="hljs-number">3</span>: <span class="hljs-string">&quot;Wiki&quot;</span>&#125;<br><br>myvar = pd.Series(sites)<br><br><span class="hljs-built_in">print</span>(myvar)<br></code></pre></td></tr></table></figure><p>这样Key就变为了索引值。</p><h2 id="Pandas-数据结构-DataFrame"><a href="#Pandas-数据结构-DataFrame" class="headerlink" title="Pandas 数据结构 - DataFrame"></a>Pandas 数据结构 - DataFrame</h2><p>DataFrame 是一个表格型的数据结构，由一个index组成的第0列和DataFrame组成的n列构成，相当于Series组成的字典（共用一个index）。</p><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-2-Pandas/image-20230919182139779.png" alt="Series与DataFrame"></p><p>函数如下：</p><p><code>pandas.DataFrame( data, index, columns, dtype, copy)</code></p><ul><li><strong>data</strong>：一组数据(ndarray、series, map, lists, dict 等类型)。</li><li><strong>index</strong>：索引值，或者可以称为行标签。</li><li><strong>columns</strong>：列标签，默认为 RangeIndex (0, 1, 2, …, n) 。</li><li><strong>dtype</strong>：数据类型。</li><li><strong>copy</strong>：拷贝数据，默认为 False。</li></ul><p>使用ndarrays创建DataFrame对象:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br>data = &#123;<span class="hljs-string">&#x27;Site&#x27;</span>:[<span class="hljs-string">&#x27;Google&#x27;</span>, <span class="hljs-string">&#x27;Runoob&#x27;</span>, <span class="hljs-string">&#x27;Wiki&#x27;</span>], <span class="hljs-string">&#x27;Age&#x27;</span>:[<span class="hljs-number">10</span>, <span class="hljs-number">12</span>, <span class="hljs-number">13</span>]&#125;<br><br>df = pd.DataFrame(data)<br><br><span class="hljs-built_in">print</span> (df)<br></code></pre></td></tr></table></figure><p>Site Age为列名，0、1、2为行标。</p><p>使用Map创建与Series同理。</p><h2 id="Pandas-CSV-文件"><a href="#Pandas-CSV-文件" class="headerlink" title="Pandas CSV 文件"></a>Pandas CSV 文件</h2><p>Pandas 可以很方便的处理 CSV 文件</p><p><code>df = pd.read_csv(&#39;nba.csv&#39;)</code>读取CSV文件。</p><p><code>df.to_csv(&#39;site.csv&#39;)</code>将DataFrame储存为csv文件。</p><p>数据处理</p><p><strong>head( n )</strong> 方法用于读取前面的 n 行，如果不填参数 n ，默认返回 5 行。</p><p><strong>tail( n )</strong> 方法用于读取尾部的 n 行，如果不填参数 n ，默认返回 5 行，空行各个字段的值返回 <strong>NaN</strong>。</p><p><strong>info()</strong> 方法返回表格的一些基本信息：</p><h2 id="Pandas-JSON-文件"><a href="#Pandas-JSON-文件" class="headerlink" title="Pandas JSON 文件"></a>Pandas JSON 文件</h2><p>Pandas 可以很方便的处理 JSON 数据</p><p><strong>to_string()</strong> 用于返回 <strong>DataFrame</strong>(表格) 类型的数据，我们也可以直接处理 JSON 字符串。</p><p>如果是字符串格式的 JSON 可以直接将Python字典转为DataFrame数据（json对象与Map有相同的格式）</p><p><strong>Json数据的解析</strong></p><p>直接加载一个print一个json文件打印出的并不直观。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">          school_name   <span class="hljs-keyword">class</span>                                           <span class="hljs-title class_">students</span><br><span class="hljs-number">0</span>  ABC primary school  Year <span class="hljs-number">1</span>  &#123;<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-string">&#x27;A001&#x27;</span>, <span class="hljs-string">&#x27;name&#x27;</span>: <span class="hljs-string">&#x27;Tom&#x27;</span>, <span class="hljs-string">&#x27;math&#x27;</span>: <span class="hljs-number">60</span>, <span class="hljs-string">&#x27;phy...</span><br><span class="hljs-string">1  ABC primary school  Year 1  &#123;&#x27;</span><span class="hljs-built_in">id</span><span class="hljs-string">&#x27;: &#x27;</span>A002<span class="hljs-string">&#x27;, &#x27;</span>name<span class="hljs-string">&#x27;: &#x27;</span>James<span class="hljs-string">&#x27;, &#x27;</span>math<span class="hljs-string">&#x27;: 89, &#x27;</span>p...<br><span class="hljs-number">2</span>  ABC primary school  Year <span class="hljs-number">1</span>  &#123;<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-string">&#x27;A003&#x27;</span>, <span class="hljs-string">&#x27;name&#x27;</span>: <span class="hljs-string">&#x27;Jenny&#x27;</span>, <span class="hljs-string">&#x27;math&#x27;</span>: <span class="hljs-number">79</span>, <span class="hljs-string">&#x27;p...</span><br><span class="hljs-string"></span><br></code></pre></td></tr></table></figure><p>用到 <strong>json_normalize()</strong> 方法将内嵌的数据完整的解析出来</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> json<br><br><span class="hljs-comment"># 使用 Python JSON 模块载入数据</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;nested_list.json&#x27;</span>,<span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    data = json.loads(f.read()) <br>    <span class="hljs-comment">#data = json.loads(f.read()) 使用 Python JSON 模块载入数据。</span><br><br><span class="hljs-comment"># 展平数据</span><br>df_nested_list = pd.json_normalize(data, record_path =[<span class="hljs-string">&#x27;students&#x27;</span>])<br><span class="hljs-built_in">print</span>(df_nested_list)<br></code></pre></td></tr></table></figure><p>Output:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">     <span class="hljs-built_in">id</span>   name  math  physics  chemistry<br><span class="hljs-number">0</span>  A001    Tom    <span class="hljs-number">60</span>       <span class="hljs-number">66</span>         <span class="hljs-number">61</span><br><span class="hljs-number">1</span>  A002  James    <span class="hljs-number">89</span>       <span class="hljs-number">76</span>         <span class="hljs-number">51</span><br><span class="hljs-number">2</span>  A003  Jenny    <span class="hljs-number">79</span>       <span class="hljs-number">90</span>         <span class="hljs-number">78</span><br></code></pre></td></tr></table></figure><p><strong>读取一组数据glom</strong></p><p><strong>import</strong> pandas <strong>as</strong> pd<br><strong>from</strong> glom <strong>import</strong> glom   glom模块准许使用“.”来获取内嵌对象的属性</p><p><code>data = df[&#39;students&#39;].apply(**lambda** row: glom(row, &#39;grade.math&#39;))</code></p><p>效果类似于查找。</p><h2 id="Pandas-数据清洗"><a href="#Pandas-数据清洗" class="headerlink" title="Pandas 数据清洗"></a>Pandas 数据清洗</h2><p>很多数据集存在数据缺失、数据格式错误、错误数据或重复数据的情况，如果要使数据分析更加准确，就需要对这些没有用的数据进行处理（清洗）。</p><p>例如数据中的“n&#x2F;a  NA  –  na”，或者空值等。</p><p>清洗空值：dropna（）方法</p><p><code>DataFrame.dropna(axis=0, how=&#39;any&#39;, thresh=None, subset=None, inplace=False)</code></p><ul><li>axis：默认为 <strong>0</strong>，表示逢空值剔除整行，如果设置参数 <strong>axis＝1</strong> 表示逢空值去掉整列。</li><li>how：默认为 <strong>‘any’</strong> 如果一行（或一列）里任何一个数据有出现 NA 就去掉整行，如果设置 <strong>how&#x3D;’all’</strong> 一行（或列）都是 NA 才去掉这整行。</li><li>thresh：设置需要多少非空值的数据才可以保留下来的。</li><li>subset：设置想要检查的列。如果是多个列，可以使用列名的 list 作为参数。</li><li>inplace：如果设置 True，将计算得到的值直接覆盖之前的值并返回 None，修改的是源数据。</li></ul><p>可以先用isnull()判断是否为空。</p><p>可以在read_csv（）方法中指定空数据的数据类型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br>missing_values = [<span class="hljs-string">&quot;n/a&quot;</span>, <span class="hljs-string">&quot;na&quot;</span>, <span class="hljs-string">&quot;--&quot;</span>]<br>df = pd.read_csv(<span class="hljs-string">&#x27;property-data.csv&#x27;</span>, na_values = missing_values)<br></code></pre></td></tr></table></figure><p>dropna会返回一个新的Dataframe不会修改源数据。如果需要修改在inplace设置为True。</p><p>移除指定列有空值的行（移除 ST_NUM 列值为空）<code>df.dropna(subset=[&#39;ST_NUM&#39;], inplace = True)</code></p><p>我们也可以 <strong>fillna()</strong> 方法来替换一些空字段<code>df.fillna(12345, inplace = True)</code></p><p>指定某一个列来替换数据<code>df[&#39;PID&#39;].fillna(12345, inplace = True)</code></p><p>替换空单元格的常用方法是计算列的均值、中值或众数。</p><p>Pandas使用 <strong>mean()<strong>、</strong>median()</strong> 和 <strong>mode()</strong> 方法计算列的均值（所有值加起来的平均值）、中位数值（排序后排在中间的数）和众数（出现频率最高的数）。</p><p>比如mode() 方法计算列的众数并替换空单元格：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br>df = pd.read_csv(<span class="hljs-string">&#x27;property-data.csv&#x27;</span>)<br><br>x = df[<span class="hljs-string">&quot;ST_NUM&quot;</span>].mode()<br><br>df[<span class="hljs-string">&quot;ST_NUM&quot;</span>].fillna(x, inplace = <span class="hljs-literal">True</span>)<br><br><span class="hljs-built_in">print</span>(df.to_string())<br></code></pre></td></tr></table></figure><p><strong>清洗格式错误数据:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br><span class="hljs-comment"># 第三个日期格式错误</span><br>data = &#123;<br>  <span class="hljs-string">&quot;Date&quot;</span>: [<span class="hljs-string">&#x27;2020/12/01&#x27;</span>, <span class="hljs-string">&#x27;2020/12/02&#x27;</span> , <span class="hljs-string">&#x27;20201226&#x27;</span>],<br>  <span class="hljs-string">&quot;duration&quot;</span>: [<span class="hljs-number">50</span>, <span class="hljs-number">40</span>, <span class="hljs-number">45</span>]<br>&#125;<br><br>df = pd.DataFrame(data, index = [<span class="hljs-string">&quot;day1&quot;</span>, <span class="hljs-string">&quot;day2&quot;</span>, <span class="hljs-string">&quot;day3&quot;</span>])<br><br>df[<span class="hljs-string">&#x27;Date&#x27;</span>] = pd.to_datetime(df[<span class="hljs-string">&#x27;Date&#x27;</span>])<br><br><span class="hljs-built_in">print</span>(df.to_string())<br><br><br><span class="hljs-comment">#output：</span><br>           Date  duration<br>day1 <span class="hljs-number">2020</span>-<span class="hljs-number">12</span>-01        <span class="hljs-number">50</span><br>day2 <span class="hljs-number">2020</span>-<span class="hljs-number">12</span>-02        <span class="hljs-number">40</span><br>day3 <span class="hljs-number">2020</span>-<span class="hljs-number">12</span>-<span class="hljs-number">26</span>        <span class="hljs-number">45</span><br></code></pre></td></tr></table></figure><p><strong>清洗错误数据：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br>person = &#123;<br>  <span class="hljs-string">&quot;name&quot;</span>: [<span class="hljs-string">&#x27;Google&#x27;</span>, <span class="hljs-string">&#x27;Runoob&#x27;</span> , <span class="hljs-string">&#x27;Taobao&#x27;</span>],<br>  <span class="hljs-string">&quot;age&quot;</span>: [<span class="hljs-number">50</span>, <span class="hljs-number">40</span>, <span class="hljs-number">12345</span>]    <span class="hljs-comment"># 12345 年龄数据是错误的</span><br>&#125;<br><br>df = pd.DataFrame(person)<br><br>df.loc[<span class="hljs-number">2</span>, <span class="hljs-string">&#x27;age&#x27;</span>] = <span class="hljs-number">30</span> <span class="hljs-comment"># 修改数据  对错误的数据进行替换或移除。也可以使用if来判断if df.loc[x, &quot;age&quot;] &gt; ?: df.drop(x, inplaced = True)</span><br><br><span class="hljs-built_in">print</span>(df.to_string())<br><br><span class="hljs-comment">#output:</span><br>     name  age<br><span class="hljs-number">0</span>  Google   <span class="hljs-number">50</span><br><span class="hljs-number">1</span>  Runoob   <span class="hljs-number">40</span><br><span class="hljs-number">2</span>  Taobao   <span class="hljs-number">30</span><br></code></pre></td></tr></table></figure><p><strong>清洗重复数据:</strong></p><p>如果我们要清洗重复数据，可以使用 <strong>duplicated()</strong> 和 <strong>drop_duplicates()</strong> 方法。</p><p>如果对应的数据是重复的，<strong>duplicated()</strong> 会返回 True，否则返回 False。</p><h2 id="Pandas-常用函数"><a href="#Pandas-常用函数" class="headerlink" title="Pandas 常用函数"></a>Pandas 常用函数</h2><h3 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h3><table><thead><tr><th align="left">函数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">pd.read_csv(filename)</td><td align="left">读取 CSV 文件；</td></tr><tr><td align="left">pd.read_excel(filename)</td><td align="left">读取 Excel 文件；</td></tr><tr><td align="left">pd.read_sql(query, connection_object)</td><td align="left">从 SQL 数据库读取数据；</td></tr><tr><td align="left">pd.read_json(json_string)</td><td align="left">从 JSON 字符串中读取数据；</td></tr><tr><td align="left">pd.read_html(url)</td><td align="left">从 HTML 页面中读取数据。</td></tr></tbody></table><h3 id="查看数据"><a href="#查看数据" class="headerlink" title="查看数据"></a>查看数据</h3><table><thead><tr><th align="left">函数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">df.head(n)</td><td align="left">显示前 n 行数据；</td></tr><tr><td align="left">df.tail(n)</td><td align="left">显示后 n 行数据；</td></tr><tr><td align="left">df.info()</td><td align="left">显示数据的信息，包括列名、数据类型、缺失值等；</td></tr><tr><td align="left">df.describe()</td><td align="left">显示数据的基本统计信息，包括均值、方差、最大值、最小值等；</td></tr><tr><td align="left">df.shape</td><td align="left">显示数据的行数和列数。</td></tr></tbody></table><h3 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h3><table><thead><tr><th align="left">函数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">df.dropna()</td><td align="left">删除包含缺失值的行或列；</td></tr><tr><td align="left">df.fillna(value)</td><td align="left">将缺失值替换为指定的值；</td></tr><tr><td align="left">df.replace(old_value, new_value)</td><td align="left">将指定值替换为新值；</td></tr><tr><td align="left">df.duplicated()</td><td align="left">检查是否有重复的数据；</td></tr><tr><td align="left">df.drop_duplicates()</td><td align="left">删除重复的数据。</td></tr></tbody></table><h3 id="数据选择和切片"><a href="#数据选择和切片" class="headerlink" title="数据选择和切片"></a>数据选择和切片</h3><table><thead><tr><th align="left">函数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">df[column_name]</td><td align="left">选择指定的列；</td></tr><tr><td align="left">df.loc[row_index, column_name]</td><td align="left">通过标签选择数据；</td></tr><tr><td align="left">df.iloc[row_index, column_index]</td><td align="left">通过位置选择数据；</td></tr><tr><td align="left">df.ix[row_index, column_name]</td><td align="left">通过标签或位置选择数据；</td></tr><tr><td align="left">df.filter(items&#x3D;[column_name1, column_name2])</td><td align="left">选择指定的列；</td></tr><tr><td align="left">df.filter(regex&#x3D;’regex’)</td><td align="left">选择列名匹配正则表达式的列；</td></tr><tr><td align="left">df.sample(n)</td><td align="left">随机选择 n 行数据。</td></tr></tbody></table><h3 id="数据排序"><a href="#数据排序" class="headerlink" title="数据排序"></a>数据排序</h3><table><thead><tr><th align="left">函数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">df.sort_values(column_name)</td><td align="left">按照指定列的值排序；</td></tr><tr><td align="left">df.sort_values([column_name1, column_name2], ascending&#x3D;[True, False])</td><td align="left">按照多个列的值排序；</td></tr><tr><td align="left">df.sort_index()</td><td align="left">按照索引排序。</td></tr></tbody></table><h3 id="数据分组和聚合"><a href="#数据分组和聚合" class="headerlink" title="数据分组和聚合"></a>数据分组和聚合</h3><table><thead><tr><th align="left">函数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">df.groupby(column_name)</td><td align="left">按照指定列进行分组；</td></tr><tr><td align="left">df.aggregate(function_name)</td><td align="left">对分组后的数据进行聚合操作；</td></tr><tr><td align="left">df.pivot_table(values, index, columns, aggfunc)</td><td align="left">生成透视表。</td></tr></tbody></table><h3 id="数据合并"><a href="#数据合并" class="headerlink" title="数据合并"></a>数据合并</h3><table><thead><tr><th align="left">函数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">pd.concat([df1, df2])</td><td align="left">将多个数据框按照行或列进行合并；</td></tr><tr><td align="left">pd.merge(df1, df2, on&#x3D;column_name)</td><td align="left">按照指定列将两个数据框进行合并。</td></tr></tbody></table><h3 id="数据选择和过滤"><a href="#数据选择和过滤" class="headerlink" title="数据选择和过滤"></a>数据选择和过滤</h3><table><thead><tr><th align="left">函数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">df.loc[row_indexer, column_indexer]</td><td align="left">按标签选择行和列。</td></tr><tr><td align="left">df.iloc[row_indexer, column_indexer]</td><td align="left">按位置选择行和列。</td></tr><tr><td align="left">df[df[‘column_name’] &gt; value]</td><td align="left">选择列中满足条件的行。</td></tr><tr><td align="left">df.query(‘column_name &gt; value’)</td><td align="left">使用字符串表达式选择列中满足条件的行。</td></tr></tbody></table><h3 id="数据统计和描述"><a href="#数据统计和描述" class="headerlink" title="数据统计和描述"></a>数据统计和描述</h3><table><thead><tr><th align="left">函数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">df.describe()</td><td align="left">计算基本统计信息，如均值、标准差、最小值、最大值等。</td></tr><tr><td align="left">df.mean()</td><td align="left">计算每列的平均值。</td></tr><tr><td align="left">df.median()</td><td align="left">计算每列的中位数。</td></tr><tr><td align="left">df.mode()</td><td align="left">计算每列的众数。</td></tr><tr><td align="left">df.count()</td><td align="left">计算每列非缺失值的数量。</td></tr></tbody></table><p>UPDATE TIME: 星期二 2023年9月19日</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>工具与软件</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Pandas</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2-工具与软件-1-Numpy</title>
    <link href="/2023/09/08/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-1-Numpy/"/>
    <url>/2023/09/08/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-1-Numpy/</url>
    
    <content type="html"><![CDATA[<p>NumPy用于数据分析，提供了大量的维度数组与矩阵运算，NumPy 通常与 SciPy（Scientific Python）和 Matplotlib（绘图库）一起使用。</p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p><code>sudo apt-get install python3-numpy python3-scipy python3-matplotlib ipython ipython-notebook python-pandas python-sympy python-nose</code></p><p>或使用pycharm在import numpy后自动导入。</p><h3 id="N-维数组对象-ndarray"><a href="#N-维数组对象-ndarray" class="headerlink" title="N 维数组对象 ndarray"></a>N 维数组对象 ndarray</h3><p>ndarray 中的每个元素在内存中都有相同存储大小的区域</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">numpy.array(<span class="hljs-built_in">object</span>, dtype = <span class="hljs-literal">None</span>, copy = <span class="hljs-literal">True</span>, order = <span class="hljs-literal">None</span>, subok = <span class="hljs-literal">False</span>, ndmin = <span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><table><thead><tr><th align="left">名称</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">object</td><td align="left">数组或嵌套的数列</td></tr><tr><td align="left">dtype</td><td align="left">数组元素的数据类型，可选</td></tr><tr><td align="left">copy</td><td align="left">对象是否需要复制，可选</td></tr><tr><td align="left">order</td><td align="left">创建数组的样式，C为行方向，F为列方向，A为任意方向（默认）</td></tr><tr><td align="left">subok</td><td align="left">默认返回一个与基类类型一致的数组</td></tr><tr><td align="left">ndmin</td><td align="left">指定生成数组的最小维度</td></tr></tbody></table><h3 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h3><p>int8, int16, int32, int64 四种数据类型可以使用字符串 ‘i1’, ‘i2’,’i4’,’i8’ 代替</p><table><thead><tr><th align="left">名称</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">bool_</td><td align="left">布尔型数据类型（True 或者 False）</td></tr><tr><td align="left">int_</td><td align="left">默认的整数类型（类似于 C 语言中的 long，int32 或 int64）</td></tr><tr><td align="left">intc</td><td align="left">与 C 的 int 类型一样，一般是 int32 或 int 64</td></tr><tr><td align="left">intp</td><td align="left">用于索引的整数类型（类似于 C 的 ssize_t，一般情况下仍然是 int32 或 int64）</td></tr><tr><td align="left">int8</td><td align="left">字节（-128 to 127）</td></tr><tr><td align="left">int16</td><td align="left">整数（-32768 to 32767）</td></tr><tr><td align="left">int32</td><td align="left">整数（-2147483648 to 2147483647）</td></tr><tr><td align="left">int64</td><td align="left">整数（-9223372036854775808 to 9223372036854775807）</td></tr><tr><td align="left">uint8</td><td align="left">无符号整数（0 to 255）</td></tr><tr><td align="left">uint16</td><td align="left">无符号整数（0 to 65535）</td></tr><tr><td align="left">uint32</td><td align="left">无符号整数（0 to 4294967295）</td></tr><tr><td align="left">uint64</td><td align="left">无符号整数（0 to 18446744073709551615）</td></tr><tr><td align="left">float_</td><td align="left">float64 类型的简写</td></tr><tr><td align="left">float16</td><td align="left">半精度浮点数，包括：1 个符号位，5 个指数位，10 个尾数位</td></tr><tr><td align="left">float32</td><td align="left">单精度浮点数，包括：1 个符号位，8 个指数位，23 个尾数位</td></tr><tr><td align="left">float64</td><td align="left">双精度浮点数，包括：1 个符号位，11 个指数位，52 个尾数位</td></tr><tr><td align="left">complex_</td><td align="left">complex128 类型的简写，即 128 位复数</td></tr><tr><td align="left">complex64</td><td align="left">复数，表示双 32 位浮点数（实数部分和虚数部分）</td></tr><tr><td align="left">complex128</td><td align="left">复数，表示双 64 位浮点数（实数部分和虚数部分）</td></tr></tbody></table><p>在创建dtype中（数据类型对象），每个内建类型都有一个唯一定义它的字符代码</p><table><thead><tr><th align="left">字符</th><th align="left">对应类型</th></tr></thead><tbody><tr><td align="left">b</td><td align="left">布尔型</td></tr><tr><td align="left">i</td><td align="left">(有符号) 整型</td></tr><tr><td align="left">u</td><td align="left">无符号整型 integer</td></tr><tr><td align="left">f</td><td align="left">浮点型</td></tr><tr><td align="left">c</td><td align="left">复数浮点型</td></tr><tr><td align="left">m</td><td align="left">timedelta（时间间隔）</td></tr><tr><td align="left">M</td><td align="left">datetime（日期时间）</td></tr><tr><td align="left">O</td><td align="left">(Python) 对象</td></tr><tr><td align="left">S, a</td><td align="left">(byte-)字符串</td></tr><tr><td align="left">U</td><td align="left">Unicode</td></tr><tr><td align="left">V</td><td align="left">原始数据 (void)</td></tr></tbody></table><h3 id="Numpy数组"><a href="#Numpy数组" class="headerlink" title="Numpy数组"></a>Numpy数组</h3><p>维数——秩（rank），维度——轴（axis）</p><table><thead><tr><th align="left">属性</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">ndarray.ndim</td><td align="left">秩，即轴的数量或维度的数量</td></tr><tr><td align="left">ndarray.shape</td><td align="left">数组的维度，对于矩阵，n 行 m 列</td></tr><tr><td align="left">ndarray.size</td><td align="left">数组元素的总个数，相当于 .shape 中 n*m 的值</td></tr><tr><td align="left">ndarray.dtype</td><td align="left">ndarray 对象的元素类型</td></tr><tr><td align="left">ndarray.itemsize</td><td align="left">ndarray 对象中每个元素的大小，以字节为单位</td></tr><tr><td align="left">ndarray.flags</td><td align="left">ndarray 对象的内存信息</td></tr><tr><td align="left">ndarray.real</td><td align="left">ndarray元素的实部</td></tr><tr><td align="left">ndarray.imag</td><td align="left">ndarray 元素的虚部</td></tr><tr><td align="left">ndarray.data</td><td align="left">包含实际数组元素的缓冲区，由于一般通过数组的索引获取元素，所以通常不需要使用这个属性。</td></tr></tbody></table><p>数组的创建</p><p><code>numpy.empty</code> 方法用来创建一个指定形状（shape）、数据类型（dtype）且未初始化的数组：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">numpy.empty(shape, dtype = <span class="hljs-built_in">float</span>, order = <span class="hljs-string">&#x27;C&#x27;</span>)<br></code></pre></td></tr></table></figure><table><thead><tr><th align="left">参数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">shape</td><td align="left">数组形状</td></tr><tr><td align="left">dtype</td><td align="left">数据类型，可选</td></tr><tr><td align="left">order</td><td align="left">有”C”和”F”两个选项,分别代表，行优先和列优先，在计算机内存中的存储元素的顺序。</td></tr></tbody></table><p><code>numpy.zeros</code> 创建指定大小的数组，数组元素以 0 来填充：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">numpy.zeros(shape, dtype = <span class="hljs-built_in">float</span>, order = <span class="hljs-string">&#x27;C&#x27;</span>)<br></code></pre></td></tr></table></figure><table><thead><tr><th align="left">参数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">shape</td><td align="left">数组形状</td></tr><tr><td align="left">dtype</td><td align="left">数据类型，可选 默认为浮点数</td></tr><tr><td align="left">order</td><td align="left">‘C’ 用于 C 的行数组，或者 ‘F’ 用于 FORTRAN 的列数组</td></tr></tbody></table><p><code>numpy.ones</code> 创建指定形状的数组，数组元素以 1 来填充：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">numpy.ones(shape, dtype = <span class="hljs-literal">None</span>, order = <span class="hljs-string">&#x27;C&#x27;</span>)<br></code></pre></td></tr></table></figure><table><thead><tr><th align="left">参数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">shape</td><td align="left">数组形状</td></tr><tr><td align="left">dtype</td><td align="left">数据类型，可选 默认为浮点数</td></tr><tr><td align="left">order</td><td align="left">‘C’ 用于 C 的行数组，或者 ‘F’ 用于 FORTRAN 的列数组</td></tr></tbody></table><p><code>numpy.zeros_like</code>  <code>numpy.ones_like</code> 创建一个模仿数组，以1或者0进行填充。</p><p>numpy.asarray 类似 numpy.array，但 numpy.asarray 参数只有三个，比 numpy.array 少两个。</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs crmsh">numpy.asarray(a, dtype = None, <span class="hljs-keyword">order</span> <span class="hljs-title">= None</span>)<br></code></pre></td></tr></table></figure><p>参数说明：</p><table><thead><tr><th align="left">参数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">a</td><td align="left">任意形式的输入参数，可以是，列表, 列表的元组, 元组, 元组的元组, 元组的列表，多维数组</td></tr><tr><td align="left">dtype</td><td align="left">数据类型，可选</td></tr><tr><td align="left">order</td><td align="left">可选，有”C”和”F”两个选项,分别代表，行优先和列优先，在计算机内存中的存储元素的顺序。</td></tr></tbody></table><p>例:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>x = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]<br>a = np.asarray(x)<br><span class="hljs-built_in">print</span>(x)<br><span class="hljs-built_in">print</span>(a)<br></code></pre></td></tr></table></figure><h6 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]<br>[<span class="hljs-number">1</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span>]<br><span class="hljs-comment">#由此可以看出list和array的区别</span><br></code></pre></td></tr></table></figure><p><code>numpy.frombuffer</code> 用于实现动态数组。</p><p><code>numpy.frombuffer</code> 接受 buffer 输入参数，以流的形式读入转化成 ndarray 对象。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">numpy.frombuffer(buffer, dtype = <span class="hljs-built_in">float</span>, count = -<span class="hljs-number">1</span>, offset = <span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><table><thead><tr><th align="left">参数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">buffer</td><td align="left">可以是任意对象，会以流的形式读入。</td></tr><tr><td align="left">dtype</td><td align="left">返回数组的数据类型，可选</td></tr><tr><td align="left">count</td><td align="left">读取的数据数量，默认为-1，读取所有数据。</td></tr><tr><td align="left">offset</td><td align="left">读取的起始位置，默认为0。</td></tr></tbody></table><p><code>numpy.fromiter</code> 方法从可迭代对象中建立 ndarray 对象，返回一维数组。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">numpy.fromiter(iterable, dtype, count=-<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><table><thead><tr><th align="left">参数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">iterable</td><td align="left">可迭代对象</td></tr><tr><td align="left">dtype</td><td align="left">返回数组的数据类型</td></tr><tr><td align="left">count</td><td align="left">读取的数据数量，默认为-1，读取所有数据</td></tr></tbody></table><p>从数值范围创建数组</p><p>numpy 包中的使用 arange 函数创建数值范围并返回 ndarray 对象，函数格式如下：</p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs arduino">numpy.<span class="hljs-built_in">arange</span>(start, stop, step, dtype)<br></code></pre></td></tr></table></figure><p>根据 start 与 stop 指定的范围以及 step 设定的步长，生成一个 ndarray。</p><p>参数说明：</p><table><thead><tr><th align="left">参数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left"><code>start</code></td><td align="left">起始值，默认为<code>0</code></td></tr><tr><td align="left"><code>stop</code></td><td align="left">终止值（不包含）</td></tr><tr><td align="left"><code>step</code></td><td align="left">步长，默认为<code>1</code></td></tr><tr><td align="left"><code>dtype</code></td><td align="left">返回<code>ndarray</code>的数据类型，如果没有提供，则会使用输入数据的类型。</td></tr></tbody></table><p><code>numpy.linspace</code> 函数用于创建一个一维数组，数组是一个等<strong>差数列构</strong>成的，格式如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">np.linspace(start, stop, <span class="hljs-attribute">num</span>=50, <span class="hljs-attribute">endpoint</span>=<span class="hljs-literal">True</span>, <span class="hljs-attribute">retstep</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">dtype</span>=None)<br></code></pre></td></tr></table></figure><p>参数说明：</p><table><thead><tr><th align="left">参数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left"><code>start</code></td><td align="left">序列的起始值</td></tr><tr><td align="left"><code>stop</code></td><td align="left">序列的终止值，如果<code>endpoint</code>为<code>true</code>，该值包含于数列中</td></tr><tr><td align="left"><code>num</code></td><td align="left">要生成的等步长的样本数量，默认为<code>50</code></td></tr><tr><td align="left"><code>endpoint</code></td><td align="left">该值为 <code>true</code> 时，数列中包含<code>stop</code>值，反之不包含，默认是True。</td></tr><tr><td align="left"><code>retstep</code></td><td align="left">如果为 True 时，生成的数组中会显示间距，反之不显示。</td></tr><tr><td align="left"><code>dtype</code></td><td align="left"><code>ndarray</code> 的数据类型</td></tr></tbody></table><p><code>numpy.logspace</code> 函数用于创建一个于<strong>等比数列</strong>。格式如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">np.logspace(start, stop, <span class="hljs-attribute">num</span>=50, <span class="hljs-attribute">endpoint</span>=<span class="hljs-literal">True</span>, <span class="hljs-attribute">base</span>=10.0, <span class="hljs-attribute">dtype</span>=None)<br></code></pre></td></tr></table></figure><p>base 参数意思是取对数的时候 log 的下标。</p><table><thead><tr><th align="left">参数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left"><code>start</code></td><td align="left">序列的起始值为：base ** start</td></tr><tr><td align="left"><code>stop</code></td><td align="left">序列的终止值为：base ** stop。如果<code>endpoint</code>为<code>true</code>，该值包含于数列中</td></tr><tr><td align="left"><code>num</code></td><td align="left">要生成的等步长的样本数量，默认为<code>50</code></td></tr><tr><td align="left"><code>endpoint</code></td><td align="left">该值为 <code>true</code> 时，数列中中包含<code>stop</code>值，反之不包含，默认是True。</td></tr><tr><td align="left"><code>base</code></td><td align="left">对数 log 的底数。</td></tr><tr><td align="left"><code>dtype</code></td><td align="left"><code>ndarray</code> 的数据类型</td></tr></tbody></table><h3 id="切片和索引"><a href="#切片和索引" class="headerlink" title="切片和索引"></a>切片和索引</h3><p>与list的切片相差不大，使用slice方法或者[:::] (start:finish:step)即可，另外对于多维数组的切分，可使用省略号<code>...</code> </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>a = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>], [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>]])<br><span class="hljs-built_in">print</span>(a[..., <span class="hljs-number">1</span>])  <span class="hljs-comment"># 第2列元素</span><br><span class="hljs-built_in">print</span>(a[<span class="hljs-number">1</span>, ...])  <span class="hljs-comment"># 第2行元素</span><br><span class="hljs-built_in">print</span>(a[..., <span class="hljs-number">1</span>:])  <span class="hljs-comment"># 第2列及剩下的所有元素</span><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">[[<span class="hljs-number">1</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span>]<br> [<span class="hljs-number">3</span> <span class="hljs-number">4</span> <span class="hljs-number">5</span>]<br> [<span class="hljs-number">4</span> <span class="hljs-number">5</span> <span class="hljs-number">6</span>]]<br>---------------------<br>[<span class="hljs-number">2</span> <span class="hljs-number">4</span> <span class="hljs-number">5</span>]<br>---------------------<br>[<span class="hljs-number">3</span> <span class="hljs-number">4</span> <span class="hljs-number">5</span>]<br>---------------------<br>[[<span class="hljs-number">2</span> <span class="hljs-number">3</span>]<br> [<span class="hljs-number">4</span> <span class="hljs-number">5</span>]<br> [<span class="hljs-number">5</span> <span class="hljs-number">6</span>]]<br></code></pre></td></tr></table></figure><p>整数数组索引是指使用一个数组来访问另一个数组的元素。这个数组中的每个元素都是目标数组中某个维度上的索引值。</p><p>以下实例获取了 4X3 数组中的四个角的元素。 行索引是 [0,0] 和 [3,3]，而列索引是 [0,2] 和 [0,2]。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np <br> <br>x = np.array([[  <span class="hljs-number">0</span>,  <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>],[  <span class="hljs-number">3</span>,  <span class="hljs-number">4</span>,  <span class="hljs-number">5</span>],[  <span class="hljs-number">6</span>,  <span class="hljs-number">7</span>,  <span class="hljs-number">8</span>],[  <span class="hljs-number">9</span>,  <span class="hljs-number">10</span>,  <span class="hljs-number">11</span>]])  <br><span class="hljs-built_in">print</span> (<span class="hljs-string">&#x27;我们的数组是：&#x27;</span> )<br><span class="hljs-built_in">print</span> (x)<br><span class="hljs-built_in">print</span> (<span class="hljs-string">&#x27;\n&#x27;</span>)<br>rows = np.array([[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>],[<span class="hljs-number">3</span>,<span class="hljs-number">3</span>]]) <br>cols = np.array([[<span class="hljs-number">0</span>,<span class="hljs-number">2</span>],[<span class="hljs-number">0</span>,<span class="hljs-number">2</span>]]) <br><span class="hljs-comment">#这里的索引是0 0,0 2,3 0,3 2</span><br>y = x[rows,cols]  <br><span class="hljs-built_in">print</span>  (<span class="hljs-string">&#x27;这个数组的四个角元素是：&#x27;</span>)<br><span class="hljs-built_in">print</span> (y)<br><br></code></pre></td></tr></table></figure><h6 id="Output-1"><a href="#Output-1" class="headerlink" title="Output"></a>Output</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">我们的数组是：<br>[[ <span class="hljs-number">0</span>  <span class="hljs-number">1</span>  <span class="hljs-number">2</span>]<br> [ <span class="hljs-number">3</span>  <span class="hljs-number">4</span>  <span class="hljs-number">5</span>]<br> [ <span class="hljs-number">6</span>  <span class="hljs-number">7</span>  <span class="hljs-number">8</span>]<br> [ <span class="hljs-number">9</span> <span class="hljs-number">10</span> <span class="hljs-number">11</span>]]<br><br><br>这个数组的四个角元素是：<br>[[ <span class="hljs-number">0</span>  <span class="hljs-number">2</span>]<br> [ <span class="hljs-number">9</span> <span class="hljs-number">11</span>]]<br></code></pre></td></tr></table></figure><p>关于 np.ix_ 的具体使用：</p><p><code>x[np.ix_([1,5,7,2],[0,3,1,2])]</code> 这句话会输出一个4*4的矩阵，其中的元素分别是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">x[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>] x[<span class="hljs-number">1</span>,<span class="hljs-number">3</span>] x[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>] x[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>]<br>x[<span class="hljs-number">5</span>,<span class="hljs-number">0</span>] x[<span class="hljs-number">5</span>,<span class="hljs-number">3</span>] x[<span class="hljs-number">5</span>,<span class="hljs-number">1</span>] x[<span class="hljs-number">5</span>,<span class="hljs-number">2</span>]<br>x[<span class="hljs-number">7</span>,<span class="hljs-number">0</span>] x[<span class="hljs-number">7</span>,<span class="hljs-number">3</span>] x[<span class="hljs-number">7</span>,<span class="hljs-number">1</span>] x[<span class="hljs-number">7</span>,<span class="hljs-number">2</span>]<br>x[<span class="hljs-number">2</span>,<span class="hljs-number">0</span>] x[<span class="hljs-number">2</span>,<span class="hljs-number">3</span>] x[<span class="hljs-number">2</span>,<span class="hljs-number">1</span>] x[<span class="hljs-number">2</span>,<span class="hljs-number">2</span>]<br></code></pre></td></tr></table></figure><p>相当于：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">y=np.array([[x[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>], x[<span class="hljs-number">1</span>,<span class="hljs-number">3</span>], x[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>], x[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>]],\<br>            [x[<span class="hljs-number">5</span>,<span class="hljs-number">0</span>], x[<span class="hljs-number">5</span>,<span class="hljs-number">3</span>], x[<span class="hljs-number">5</span>,<span class="hljs-number">1</span>],x[<span class="hljs-number">5</span>,<span class="hljs-number">2</span>]],\<br>            [x[<span class="hljs-number">7</span>,<span class="hljs-number">0</span>] ,x[<span class="hljs-number">7</span>,<span class="hljs-number">3</span>], x[<span class="hljs-number">7</span>,<span class="hljs-number">1</span>], x[<span class="hljs-number">7</span>,<span class="hljs-number">2</span>]],\<br>            [x[<span class="hljs-number">2</span>,<span class="hljs-number">0</span>], x[<span class="hljs-number">2</span>,<span class="hljs-number">3</span>], x[<span class="hljs-number">2</span>,<span class="hljs-number">1</span>], x[<span class="hljs-number">2</span>,<span class="hljs-number">2</span>]]])<br></code></pre></td></tr></table></figure><p>就是说，如果 np.xi_ 中输入两个列表，则第一个列表存的是待提取元素的行标，第二个列表存的是待提取元素的列标，第一个列表中的每个元素都会遍历第二个列表中的每个值，构成新矩阵的一行元素。</p><h3 id="广播"><a href="#广播" class="headerlink" title="广播"></a>广播</h3><p>广播(Broadcast)是 numpy 对不同形状(shape)的数组进行数值计算的方式， 对数组的算术运算通常在相应的元素上进行。</p><p>如果两个数组 a 和 b 形状相同，即满足 <strong>a.shape &#x3D;&#x3D; b.shape</strong>，那么 a*b 的结果就是 a 与 b 数组对应位相乘。这要求维数相同，且各维度的长度相同。</p><p>但是两个数组形状不同时，numpy就触发了广播机制。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>a = np.array([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],<br>              [<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>],<br>              [<span class="hljs-number">20</span>, <span class="hljs-number">20</span>, <span class="hljs-number">20</span>],<br>              [<span class="hljs-number">30</span>, <span class="hljs-number">30</span>, <span class="hljs-number">30</span>]])<br>b = np.array([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>])<br><span class="hljs-built_in">print</span>(a + b)<br></code></pre></td></tr></table></figure><p><img src="https://www.runoob.com/wp-content/uploads/2018/10/image0020619.gif" alt="img"></p><h6 id="Output-2"><a href="#Output-2" class="headerlink" title="Output"></a>Output</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">[[ <span class="hljs-number">0</span>  <span class="hljs-number">1</span>  <span class="hljs-number">2</span>]<br> [<span class="hljs-number">10</span> <span class="hljs-number">11</span> <span class="hljs-number">12</span>]<br> [<span class="hljs-number">20</span> <span class="hljs-number">21</span> <span class="hljs-number">22</span>]<br> [<span class="hljs-number">30</span> <span class="hljs-number">31</span> <span class="hljs-number">32</span>]]<br></code></pre></td></tr></table></figure><h3 id="迭代器"><a href="#迭代器" class="headerlink" title="迭代器"></a>迭代器</h3><p>NumPy 迭代器对象<code>numpy.nditer</code>提供了一种灵活访问一个或者多个数组元素的方式。</p><p><code>for x in np.nditer(a, order=&#39;F&#39;):</code>Fortran order，即是列序优先；</p><p><code>for x in np.nditer(a.T, order=&#39;C&#39;):</code>C order，即是行序优先；</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> np.nditer(a, op_flags=[<span class="hljs-string">&#x27;readwrite&#x27;</span>]): <br>    x[...]=<span class="hljs-number">2</span>*x <br></code></pre></td></tr></table></figure><p><strong>x[…]</strong> 是修改原 numpy 元素，x 只是个拷贝。</p><p>order &#x3D; ‘C’，numpy 实例（也就是一个多维数组）本身的存储顺序不会因为转置或 order &#x3D; ‘C’ 或 ‘F’ 而改变。</p><p>只是 numpy 实例中，存储了一个默认的访问顺序的字段。</p><p>numpy.copy 做了特殊处理，它拷贝的时候不是直接把对方的内存复制，而是按照上面 order 指定的顺序逐一拷贝。</p><p><strong>for x in np.nditer(a, order &#x3D; ‘C’)</strong>: 可以在循环中另外指定顺序，如果未指定，则按照上面数组的order顺序访问。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> np.nditer(a, flags = [<span class="hljs-string">&#x27;external_loop&#x27;</span>], order = <span class="hljs-string">&#x27;F&#x27;</span>): <br>    <span class="hljs-built_in">print</span> (x, end=<span class="hljs-string">&quot;, &quot;</span> )<br></code></pre></td></tr></table></figure><p>**flags &#x3D; [‘external_loop’]**，当数组的 order 与在循环中指定的 order 顺序不同时，打印为多个一维数组，当相同时，是整个一个一维数组。</p><h3 id="数组操作"><a href="#数组操作" class="headerlink" title="数组操作"></a>数组操作</h3><p>这一部分基本上一些方法，这里只对方法的函数名和描述给出。</p><p><strong>修改数组形状</strong></p><table><thead><tr><th align="left">函数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left"><code>reshape</code></td><td align="left">不改变数据的条件下修改形状 numpy.reshape(arr, newshape, order&#x3D;’C’)</td></tr><tr><td align="left"><code>flat</code></td><td align="left">数组元素迭代器</td></tr><tr><td align="left"><code>flatten</code></td><td align="left">返回一份数组拷贝，对拷贝所做的修改不会影响原始数组 ndarray.flatten(order&#x3D;’C’)</td></tr><tr><td align="left"><code>ravel</code></td><td align="left">返回展开数组 numpy.ravel(a, order&#x3D;’C’)</td></tr></tbody></table><p><strong>翻转数组</strong></p><table><thead><tr><th align="left">函数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left"><code>transpose</code></td><td align="left">对换数组的维度 numpy.transpose(arr, axes)</td></tr><tr><td align="left"><code>ndarray.T</code></td><td align="left">和 <code>self.transpose()</code> 相同</td></tr><tr><td align="left"><code>rollaxis</code></td><td align="left">向后滚动指定的轴 numpy.rollaxis(arr, axis, start)</td></tr><tr><td align="left"><code>swapaxes</code></td><td align="left">对换数组的两个轴 numpy.swapaxes(arr, axis1, axis2)</td></tr></tbody></table><p><strong>修改数组维度</strong></p><table><thead><tr><th align="left">维度</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left"><code>broadcast</code></td><td align="left">产生模仿广播的对象</td></tr><tr><td align="left"><code>broadcast_to</code></td><td align="left">将数组广播到新形状 numpy.broadcast_to(array, shape, subok)</td></tr><tr><td align="left"><code>expand_dims</code></td><td align="left">扩展数组的形状  numpy.expand_dims(arr, axis)</td></tr><tr><td align="left"><code>squeeze</code></td><td align="left">从数组的形状中删除一维条目 numpy.squeeze(arr, axis)</td></tr></tbody></table><p><strong>连接数组</strong></p><table><thead><tr><th align="left">函数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left"><code>concatenate</code></td><td align="left">连接沿现有轴的数组序列 numpy.concatenate((a1, a2, …), axis)</td></tr><tr><td align="left"><code>stack</code></td><td align="left">沿着新的轴加入一系列数组。 numpy.stack(arrays, axis)</td></tr><tr><td align="left"><code>hstack</code></td><td align="left">水平堆叠序列中的数组（列方向）</td></tr><tr><td align="left"><code>vstack</code></td><td align="left">竖直堆叠序列中的数组（行方向）</td></tr></tbody></table><p><strong>分割数组</strong></p><table><thead><tr><th align="left">函数</th><th align="left">数组及操作</th></tr></thead><tbody><tr><td align="left"><code>split</code></td><td align="left">将一个数组分割为多个子数组 numpy.split(ary, indices_or_sections, axis)</td></tr><tr><td align="left"><code>hsplit</code></td><td align="left">将一个数组水平分割为多个子数组（按列）</td></tr><tr><td align="left"><code>vsplit</code></td><td align="left">将一个数组垂直分割为多个子数组（按行）</td></tr></tbody></table><p><strong>数组元素的添加与删除</strong></p><table><thead><tr><th align="left">函数</th><th align="left">元素及描述</th></tr></thead><tbody><tr><td align="left"><code>resize</code></td><td align="left">返回指定形状的新数组 numpy.resize(arr, shape)</td></tr><tr><td align="left"><code>append</code></td><td align="left">将值添加到数组末尾 numpy.append(arr, values, axis&#x3D;None)</td></tr><tr><td align="left"><code>insert</code></td><td align="left">沿指定轴将值插入到指定下标之前 numpy.insert(arr, obj, values, axis)</td></tr><tr><td align="left"><code>delete</code></td><td align="left">删掉某个轴的子数组，并返回删除后的新数组 Numpy.delete(arr, obj, axis)</td></tr><tr><td align="left"><code>unique</code></td><td align="left">查找数组内的唯一元素 numpy.unique(arr, return_index, return_inverse, return_counts)</td></tr></tbody></table><h3 id="位运算"><a href="#位运算" class="headerlink" title="位运算"></a>位运算</h3><p>Numpy也是内置位运算函数的，我认为这部分了解即可</p><p><a href="https://www.runoob.com/numpy/numpy-binary-operators.html">菜鸟教程-NumPy 位运算</a></p><p>NumPy <strong>“bitwise_”</strong> 开头的函数是位运算函数。</p><p>NumPy 位运算包括以下几个函数：</p><table><thead><tr><th align="left">函数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left"><code>bitwise_and</code></td><td align="left">对数组元素执行位与操作</td></tr><tr><td align="left"><code>bitwise_or</code></td><td align="left">对数组元素执行位或操作</td></tr><tr><td align="left"><code>invert</code></td><td align="left">按位取反</td></tr><tr><td align="left"><code>left_shift</code></td><td align="left">向左移动二进制表示的位</td></tr><tr><td align="left"><code>right_shift</code></td><td align="left">向右移动二进制表示的位</td></tr></tbody></table><p><strong>注：</strong>也可以使用 “&amp;”、 “~”、 “|” 和 “^” 等操作符进行计算。</p><h3 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h3><p>Numpy的字符串函数是基于Python内置库中的标准字符串函数。</p><table><thead><tr><th align="left">函数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left"><code>add()</code></td><td align="left">对两个数组的逐个字符串元素进行连接</td></tr><tr><td align="left"><code>multiply()</code></td><td align="left">返回按元素多重连接后的字符串</td></tr><tr><td align="left"><code>center()</code></td><td align="left">居中字符串</td></tr><tr><td align="left"><code>capitalize()</code></td><td align="left">将字符串第一个字母转换为大写</td></tr><tr><td align="left"><code>title()</code></td><td align="left">将字符串的每个单词的第一个字母转换为大写</td></tr><tr><td align="left"><code>lower()</code></td><td align="left">数组元素转换为小写</td></tr><tr><td align="left"><code>upper()</code></td><td align="left">数组元素转换为大写</td></tr><tr><td align="left"><code>split()</code></td><td align="left">指定分隔符对字符串进行分割，并返回数组列表</td></tr><tr><td align="left"><code>splitlines()</code></td><td align="left">返回元素中的行列表，以换行符分割</td></tr><tr><td align="left"><code>strip()</code></td><td align="left">移除元素开头或者结尾处的特定字符</td></tr><tr><td align="left"><code>join()</code></td><td align="left">通过指定分隔符来连接数组中的元素</td></tr><tr><td align="left"><code>replace()</code></td><td align="left">使用新字符串替换字符串中的所有子字符串</td></tr><tr><td align="left"><code>decode()</code></td><td align="left">数组元素依次调用<code>str.decode</code></td></tr><tr><td align="left"><code>encode()</code></td><td align="left">数组元素依次调用<code>str.encode</code></td></tr></tbody></table><h3 id="数学"><a href="#数学" class="headerlink" title="数学"></a>数学</h3><p>提供了标准的三角函数：**sin()、cos()、tan()**。</p><p><strong>arcsin，arccos，和 arctan</strong> 函数返回给定角度的 sin，cos 和 tan 的反三角函数。</p><p>这些函数的结果可以通过<code>numpy.degrees()</code>函数将弧度转换为角度。</p><p><code>numpy.around()</code> 函数返回指定数字的四舍五入值。<br><code>numpy.around(a,decimals)</code> decimals: 舍入的小数位数。 默认值为0。 如果为负，整数将四舍五入到小数点左侧的位置</p><p><code>numpy.floor()</code> 返回小于或者等于指定表达式的最大整数，即向下取整。</p><p><code>numpy.ceil()</code> 返回大于或者等于指定表达式的最小整数，即向上取整。</p><p>NumPy 算术函数包含简单的加减乘除: <strong>add()<strong>，</strong>subtract()<strong>，</strong>multiply()</strong> 和 **divide()**。</p><p><code>numpy.reciprocal() </code>函数返回参数逐元素的<strong>倒数</strong>。如 <strong>1&#x2F;4</strong> 倒数为 <strong>4&#x2F;1</strong>。</p><p><code>numpy.power() </code>函数将第一个输入数组中的元素作为底数，计算它与第二个输入数组中相应元素的幂。</p><p><code>numpy.mod() </code>计算输入数组中相应元素的相除后的余数。函数<code>numpy.remainder()</code>也产生相同的结果。</p><h3 id="统计学"><a href="#统计学" class="headerlink" title="统计学"></a>统计学</h3><p>NumPy 提供了很多统计函数，用于从数组中查找最小元素，最大元素，百分位标准差和方差等。</p><p>这些统计学函数通常带有较多的传入参数，详见<a href="https://www.runoob.com/numpy/numpy-statistical-functions.html">统计学函数</a></p><p><code>numpy.amin()</code> 用于计算数组中的元素沿指定轴的最小值。</p><p><code>numpy.amax() </code>用于计算数组中的元素沿指定轴的最大值。</p><p><code>numpy.ptp()</code>函数计算数组中元素最大值与最小值的差（最大值 - 最小值）。</p><p><code>numpy.percentile()</code>百分位数是统计中使用的度量，表示小于这个值的观察值的百分比。 </p><p><code>numpy.median() </code>函数用于计算数组 a 中元素的中位数（中值）</p><p><code>numpy.mean()</code> 函数返回数组中元素的算术平均值，如果提供了轴，则沿其计算。</p><p><code>numpy.average() </code>函数根据在另一个数组中给出的各自的权重计算数组中元素的加权平均值。</p><p>标准差是一组数据平均值分散程度的一种度量。标准差是方差的算术平方根。</p><p>标准差公式如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">std = sqrt(mean((x - x.mean())**<span class="hljs-number">2</span>))<br><span class="hljs-comment">#使用例：</span><br><span class="hljs-built_in">print</span> (np.std([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]))<br>&gt;&gt; <span class="hljs-number">1.1180339887498949</span><br></code></pre></td></tr></table></figure><p>统计中的方差（样本方差）是每个样本值与全体样本值的平均数之差的平方值的平均数，</p><p>即 <code>mean((x - x.mean())** 2)</code></p><p>换句话说，标准差是方差的平方根。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#使用例：</span><br><span class="hljs-built_in">print</span> (np.var([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]))<br>&gt;&gt; <span class="hljs-number">1.25</span><br></code></pre></td></tr></table></figure><h3 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h3><table><thead><tr><th align="left">种类</th><th align="left">速度</th><th align="left">最坏情况</th></tr></thead><tbody><tr><td align="left"><code>quicksort</code>（快速排序）</td><td align="left">1</td><td align="left"><code>O(n^2)</code></td></tr><tr><td align="left"><code>mergesort</code>（归并排序）</td><td align="left">2</td><td align="left"><code>O(n*log(n))</code></td></tr><tr><td align="left"><code>heapsort</code>（堆排序）</td><td align="left">3</td><td align="left"><code>O(n*log(n))</code></td></tr></tbody></table><p><code>numpy.sort() </code>函数返回输入数组的排序副本，numpy中还能以字段关键字排序。</p><p>倒序使用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">x = <span class="hljs-built_in">abs</span>(np.sort(-x)) <br></code></pre></td></tr></table></figure><p><code>numpy.argsort() </code>函数返回的是数组值从小到大的索引值。</p><p><code>numpy.lexsort() </code>用于对多个序列进行排序。把它想象成对电子表格进行排序，每一列代表一个序列，排序时优先照顾靠后的列。</p><table><thead><tr><th align="left">函数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left"><code>msort(a)</code></td><td align="left">数组按第一个轴排序，返回排序后的数组副本。np.msort(a) 相等于 np.sort(a, axis&#x3D;0)。</td></tr><tr><td align="left"><code>sort_complex(a)</code></td><td align="left">对复数按照先实部后虚部的顺序进行排序。</td></tr><tr><td align="left"><code>partition(a, kth[, axis, kind, order])</code></td><td align="left">指定一个数，对数组进行分区</td></tr><tr><td align="left"><code>argpartition(a, kth[, axis, kind, order])</code></td><td align="left">可以通过关键字 kind 指定算法沿着指定轴对数组进行分区</td></tr></tbody></table><p><code>numpy.argmax()</code> 和 <code>numpy.argmin()</code>函数分别沿给定轴返回最大和最小元素的索引。</p><p><code>numpy.nonzero() </code>函数返回输入数组中非零元素的索引。</p><p><code>numpy.where() </code>函数返回输入数组中满足给定条件的元素的索引。</p><p><code>numpy.extract()</code>函数根据某个条件从数组中抽取元素，返回满条件的元素。</p><h3 id="字节交换"><a href="#字节交换" class="headerlink" title="字节交换"></a>字节交换</h3><ul><li><strong>大端模式：</strong>指数据的高字节保存在内存的低地址中，而数据的低字节保存在内存的高地址中，这样的存储模式有点儿类似于把数据当作字符串顺序处理：地址由小向大增加，而数据从高位往低位放；这和我们的阅读习惯一致。</li><li><strong>小端模式：</strong>指数据的高字节保存在内存的高地址中，而数据的低字节保存在内存的低地址中，这种存储模式将地址的高低和数据位权有效地结合起来，高地址部分权值高，低地址部分权值低。</li></ul><p><code>numpy.ndarray.byteswap() </code>函数将 ndarray 中每个元素中的字节进行大小端转换。</p><p>(我并不知道这样做这有什么用)</p><h3 id="副本和视图"><a href="#副本和视图" class="headerlink" title="副本和视图"></a>副本和视图</h3><p>和数据库语言的副本、视图类似。</p><p>视图或浅拷贝：<code>ndarray.view() </code>方会创建一个新的数组对象，该方法创建的新数组的维数变化不会改变原始数据的维数。</p><p>副本或深拷贝：<code>ndarray.copy() </code>函数创建一个副本。 对副本数据进行修改，不会影响到原始数据，它们物理内存不在同一位置。</p><h3 id="矩阵（matrix）与线性代数"><a href="#矩阵（matrix）与线性代数" class="headerlink" title="矩阵（matrix）与线性代数"></a>矩阵（matrix）与线性代数</h3><p>一个 m * n 的矩阵</p><p>转置： numpy.transpose 函数来对换数组的维度，还可以使用 <strong>T</strong> 属性。例如有个 m 行 n 列的矩阵，使用 t() 函数就能转换为 n 行 m 列的矩阵。</p><p><code>matlib.empty() </code>函数返回一个新的矩阵。</p><p><code>numpy.matlib.zeros() </code>函数创建一个以 0 填充的矩阵。</p><p><code>numpy.matlib.ones()</code>函数创建一个以 1 填充的矩阵。</p><p><code>numpy.matlib.eye()</code> 函数返回一个矩阵，对角线元素为 1，其他位置为零。</p><p><code>numpy.matlib.identity() </code>函数返回给定大小的单位矩阵。</p><p><code>numpy.matlib.rand() </code>函数创建一个给定大小的矩阵，数据是随机填充的。</p><p><strong>线性代数</strong>函数库 <strong>linalg</strong>，该库包含了线性代数所需的所有功能</p><table><thead><tr><th align="left">函数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left"><code>dot</code></td><td align="left">两个数组的点积，即元素对应相乘。numpy.dot(a, b, out&#x3D;None)</td></tr><tr><td align="left"><code>vdot</code></td><td align="left">两个向量的点积</td></tr><tr><td align="left"><code>inner</code></td><td align="left">两个数组的内积</td></tr><tr><td align="left"><code>matmul</code></td><td align="left">两个数组的矩阵积</td></tr><tr><td align="left"><code>determinant</code></td><td align="left">数组的行列式</td></tr><tr><td align="left"><code>solve</code></td><td align="left">求解线性矩阵方程</td></tr><tr><td align="left"><code>inv</code></td><td align="left">计算矩阵的乘法逆矩阵</td></tr></tbody></table><p><code>numpy.linalg.det() </code>函数计算输入矩阵的行列式。</p><p><code>numpy.linalg.solve() </code>函数给出了矩阵形式的线性方程的解。</p><p><a href="https://www.runoob.com/numpy/numpy-linear-algebra.html">详见</a></p><h3 id="IO"><a href="#IO" class="headerlink" title="IO"></a>IO</h3><p>NumPy 为 ndarray 对象引入了一个简单的文件格式：<strong>npy</strong>。</p><p>npy 文件用于存储重建 ndarray 所需的数据、图形、dtype 和其他信息。</p><p><code>numpy.save() </code>函数将数组保存到以 .npy 为扩展名的文件中。</p><p><code>numpy.savez() </code>函数将多个数组保存到以 npz 为扩展名的文件中。</p><p><code>savetxt()</code> 函数是以简单的文本文件格式存储数据，对应的使用<code> loadtxt()</code> 函数来获取数据。</p><h3 id="Matplotlib"><a href="#Matplotlib" class="headerlink" title="Matplotlib"></a>Matplotlib</h3><p>Matplotlib 是 Python 的绘图库。 它可与 NumPy 一起使用，提供了一种有效的 MatLab 开源替代方案。 它也可以和图形工具包一起使用，如 PyQt 和 wxPython。</p><p><a href="https://search.bilibili.com/all?keyword=Matplotlib&from_source=webtop_search&spm_id_from=333.1007&search_source=5">详见</a></p><p>UPDATE TIME ： </p><p>2023年9月12日星期二</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>工具与软件</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Numpy</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>1-基础部分-2-数学基础</title>
    <link href="/2023/09/08/1-%E5%9F%BA%E7%A1%80%E9%83%A8%E5%88%86-2-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/"/>
    <url>/2023/09/08/1-%E5%9F%BA%E7%A1%80%E9%83%A8%E5%88%86-2-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/</url>
    
    <content type="html"><![CDATA[<h2 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h2><h3 id="最优化问题"><a href="#最优化问题" class="headerlink" title="最优化问题"></a>最优化问题</h3><p>目标函数Error:<br>$$<br>E(\theta)&#x3D;\frac{1}{2} \sum_{i&#x3D;1}^n(y_i-f_\theta(x_i))^2<br>$$<br>i是指第 i 个训练数据.对每个训练数据的误差取平方之后，全部相加，然后乘以0.5 。这么做是为了找到使 E(θ) 的值最小的 θ。这样的问题称为最优化问题。</p><h3 id="最速下降法："><a href="#最速下降法：" class="headerlink" title="最速下降法："></a>最速下降法：</h3><p>$$<br>\theta_0 :&#x3D;\theta_0 - \eta\sum_{i&#x3D;1}^n(f_\theta(x_i)-y_i)<br>$$</p><p>$$<br>\theta_1 :&#x3D;\theta_1 - \eta\sum_{i&#x3D;1}^n(f_\theta(x_i)-y_i) x_i<br>$$</p><h3 id="多项式回归："><a href="#多项式回归：" class="headerlink" title="多项式回归："></a>多项式回归：</h3>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>基础部分</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>1-基础部分-1-Python</title>
    <link href="/2023/09/08/1-%E5%9F%BA%E7%A1%80%E9%83%A8%E5%88%86-1-Python/"/>
    <url>/2023/09/08/1-%E5%9F%BA%E7%A1%80%E9%83%A8%E5%88%86-1-Python/</url>
    
    <content type="html"><![CDATA[<h1>Chapter 1</h1><h2 id="1-1-分解序列"><a href="#1-1-分解序列" class="headerlink" title="1.1 分解序列"></a>1.1 分解序列</h2><p>需要元素数量匹配，除了元组和列表，其余可迭代也可执行。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">data = [<span class="hljs-string">&#x27;ACME&#x27;</span>, <span class="hljs-number">50</span>, <span class="hljs-number">91.9</span>, (<span class="hljs-number">2023</span>, <span class="hljs-number">9</span>, <span class="hljs-number">9</span>)]<br>name, shares, price, data = data<br><br><span class="hljs-built_in">print</span>(name, data)<br><br></code></pre></td></tr></table></figure><h6 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">ACME (<span class="hljs-number">2023</span>, <span class="hljs-number">9</span>, <span class="hljs-number">9</span>)<br></code></pre></td></tr></table></figure><h2 id="1-2-从可迭代对象中分解元素"><a href="#1-2-从可迭代对象中分解元素" class="headerlink" title="1.2 从可迭代对象中分解元素"></a>1.2 从可迭代对象中分解元素</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">avg</span>(<span class="hljs-params">nums</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">sum</span>(nums)/<span class="hljs-built_in">len</span>(nums)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">drop_fst_and_lst</span>(<span class="hljs-params">self</span>):<br>    fst, *mid, lst = self<br>    <span class="hljs-keyword">return</span> avg(mid)<br><br><br>grades = [<span class="hljs-number">100</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">0</span>]<br><span class="hljs-built_in">print</span>(drop_fst_and_lst(grades))<br></code></pre></td></tr></table></figure><h6 id="Output-1"><a href="#Output-1" class="headerlink" title="Output"></a>Output</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-number">10.0</span><br></code></pre></td></tr></table></figure><p>Tips:</p><p>将函数传入值改为self可避免暴露函数内的变量名。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs stylus">line = <span class="hljs-string">&#x27;gting:*:-2:-2:Unprivileged User:/var/empty:/usr/bin/false&#x27;</span><br><br>unmae, *fields, homedir, sh = line<span class="hljs-selector-class">.split</span>(<span class="hljs-string">&#x27;:&#x27;</span>)<br><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(unmae)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(homedir)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(sh)</span></span><br></code></pre></td></tr></table></figure><h6 id="Output-2"><a href="#Output-2" class="headerlink" title="Output"></a>Output</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">gting<br>/var/empty<br>/usr/<span class="hljs-built_in">bin</span>/false<br></code></pre></td></tr></table></figure><h2 id="1-3-双向队列"><a href="#1-3-双向队列" class="headerlink" title="1.3 双向队列"></a>1.3 双向队列</h2><p>初始化：<code>q = deque(maxlen=?)</code></p><p><code>q.append(?)</code> 右侧插入元素</p><p><code>q.appendleft(?)</code> 左侧插入元素</p><p><code>q.pop()</code> 弹出右侧元素</p><p><code>q.popleft()</code> 弹出左侧元素</p><p>如果不指定队列的大小就是一个无限的队列，可在两段进行插入和弹出，并且都是O(1)，而列表是O(n)</p><h2 id="1-4-堆heapq"><a href="#1-4-堆heapq" class="headerlink" title="1.4 堆heapq"></a>1.4 堆<code>heapq</code></h2><p>找到最大或者最小的N个元素。</p><p><code>imort heapq</code></p><p><code>heapq</code>中有两个函数 <code>nlargest()</code> 和 <code>nsmallest()</code> </p><p><code>heapq.nlargest(?, ?list, key)</code> 取出最大的前三项，最小同理。</p><p><code>heapq.heapify(?list)</code>将list排序为小顶堆</p><p><code>heapq.heappop()</code> 获取弹出对顶元素，O(logn)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> heapq<br><br>portfolio = [<br>   &#123;<span class="hljs-string">&#x27;name&#x27;</span>: <span class="hljs-string">&#x27;IBM&#x27;</span>, <span class="hljs-string">&#x27;shares&#x27;</span>: <span class="hljs-number">100</span>, <span class="hljs-string">&#x27;price&#x27;</span>: <span class="hljs-number">91.1</span>&#125;,<br>   &#123;<span class="hljs-string">&#x27;name&#x27;</span>: <span class="hljs-string">&#x27;AAPL&#x27;</span>, <span class="hljs-string">&#x27;shares&#x27;</span>: <span class="hljs-number">50</span>, <span class="hljs-string">&#x27;price&#x27;</span>: <span class="hljs-number">543.22</span>&#125;,<br>   &#123;<span class="hljs-string">&#x27;name&#x27;</span>: <span class="hljs-string">&#x27;FB&#x27;</span>, <span class="hljs-string">&#x27;shares&#x27;</span>: <span class="hljs-number">200</span>, <span class="hljs-string">&#x27;price&#x27;</span>: <span class="hljs-number">21.09</span>&#125;,<br>   &#123;<span class="hljs-string">&#x27;name&#x27;</span>: <span class="hljs-string">&#x27;HPQ&#x27;</span>, <span class="hljs-string">&#x27;shares&#x27;</span>: <span class="hljs-number">35</span>, <span class="hljs-string">&#x27;price&#x27;</span>: <span class="hljs-number">31.75</span>&#125;,<br>   &#123;<span class="hljs-string">&#x27;name&#x27;</span>: <span class="hljs-string">&#x27;YHOO&#x27;</span>, <span class="hljs-string">&#x27;shares&#x27;</span>: <span class="hljs-number">45</span>, <span class="hljs-string">&#x27;price&#x27;</span>: <span class="hljs-number">16.35</span>&#125;,<br>   &#123;<span class="hljs-string">&#x27;name&#x27;</span>: <span class="hljs-string">&#x27;ACME&#x27;</span>, <span class="hljs-string">&#x27;shares&#x27;</span>: <span class="hljs-number">75</span>, <span class="hljs-string">&#x27;price&#x27;</span>: <span class="hljs-number">115.65</span>&#125;<br>]<br><br>cheap = heapq.nsmallest(<span class="hljs-number">3</span>, portfolio, key=<span class="hljs-keyword">lambda</span> s: s[<span class="hljs-string">&#x27;price&#x27;</span>])<br>expensive = heapq.nlargest(<span class="hljs-number">3</span>, portfolio, key=<span class="hljs-keyword">lambda</span> s: s[<span class="hljs-string">&#x27;price&#x27;</span>])<br><br><span class="hljs-built_in">print</span>(cheap)<br><span class="hljs-built_in">print</span>(expensive)<br></code></pre></td></tr></table></figure><h6 id="Output-3"><a href="#Output-3" class="headerlink" title="Output"></a>Output</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">[&#123;<span class="hljs-string">&#x27;name&#x27;</span>: <span class="hljs-string">&#x27;YHOO&#x27;</span>, <span class="hljs-string">&#x27;shares&#x27;</span>: <span class="hljs-number">45</span>, <span class="hljs-string">&#x27;price&#x27;</span>: <span class="hljs-number">16.35</span>&#125;, &#123;<span class="hljs-string">&#x27;name&#x27;</span>: <span class="hljs-string">&#x27;FB&#x27;</span>, <span class="hljs-string">&#x27;shares&#x27;</span>: <span class="hljs-number">200</span>, <span class="hljs-string">&#x27;price&#x27;</span>: <span class="hljs-number">21.09</span>&#125;, &#123;<span class="hljs-string">&#x27;name&#x27;</span>: <span class="hljs-string">&#x27;HPQ&#x27;</span>, <span class="hljs-string">&#x27;shares&#x27;</span>: <span class="hljs-number">35</span>, <span class="hljs-string">&#x27;price&#x27;</span>: <span class="hljs-number">31.75</span>&#125;]<br>[&#123;<span class="hljs-string">&#x27;name&#x27;</span>: <span class="hljs-string">&#x27;AAPL&#x27;</span>, <span class="hljs-string">&#x27;shares&#x27;</span>: <span class="hljs-number">50</span>, <span class="hljs-string">&#x27;price&#x27;</span>: <span class="hljs-number">543.22</span>&#125;, &#123;<span class="hljs-string">&#x27;name&#x27;</span>: <span class="hljs-string">&#x27;ACME&#x27;</span>, <span class="hljs-string">&#x27;shares&#x27;</span>: <span class="hljs-number">75</span>, <span class="hljs-string">&#x27;price&#x27;</span>: <span class="hljs-number">115.65</span>&#125;, &#123;<span class="hljs-string">&#x27;name&#x27;</span>: <span class="hljs-string">&#x27;IBM&#x27;</span>, <span class="hljs-string">&#x27;shares&#x27;</span>: <span class="hljs-number">100</span>, <span class="hljs-string">&#x27;price&#x27;</span>: <span class="hljs-number">91.1</span>&#125;]<br></code></pre></td></tr></table></figure><h2 id="1-5-优先队列"><a href="#1-5-优先队列" class="headerlink" title="1.5 优先队列"></a>1.5 优先队列</h2><p>使用 <code>heap</code>实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># example.py</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># Example of a priority queue</span><br><br><span class="hljs-keyword">import</span> heapq<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">PriorityQueue</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        self._queue = []<br>        self._index = <span class="hljs-number">0</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">push</span>(<span class="hljs-params">self, item, priority</span>):<br>        heapq.heappush(self._queue, (-priority, self._index, item))<br>        self._index += <span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">pop</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> heapq.heappop(self._queue)[-<span class="hljs-number">1</span>]<br><br><br><span class="hljs-comment"># Example use</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Item</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, name</span>):<br>        self.name = name<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__repr__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&#x27;Item(&#123;!r&#125;)&#x27;</span>.<span class="hljs-built_in">format</span>(self.name)<br><br><br>q = PriorityQueue()<br>q.push(Item(<span class="hljs-string">&#x27;foo&#x27;</span>), <span class="hljs-number">1</span>)<br>q.push(Item(<span class="hljs-string">&#x27;bar&#x27;</span>), <span class="hljs-number">5</span>)<br>q.push(Item(<span class="hljs-string">&#x27;spam&#x27;</span>), <span class="hljs-number">4</span>)<br>q.push(Item(<span class="hljs-string">&#x27;grok&#x27;</span>), <span class="hljs-number">1</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Should be bar:&quot;</span>, q.pop())<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Should be spam:&quot;</span>, q.pop())<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Should be foo:&quot;</span>, q.pop())<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Should be grok:&quot;</span>, q.pop())<br></code></pre></td></tr></table></figure><h6 id="Output-4"><a href="#Output-4" class="headerlink" title="Output"></a>Output</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">Should be bar: Item(<span class="hljs-string">&#x27;bar&#x27;</span>)<br>Should be spam: Item(<span class="hljs-string">&#x27;spam&#x27;</span>)<br>Should be foo: Item(<span class="hljs-string">&#x27;foo&#x27;</span>)<br>Should be grok: Item(<span class="hljs-string">&#x27;grok&#x27;</span>)<br></code></pre></td></tr></table></figure><h2 id="1-6-一键多值字典multdct"><a href="#1-6-一键多值字典multdct" class="headerlink" title="1.6 一键多值字典multdct"></a>1.6 一键多值字典<code>multdct</code></h2><p>使用<code>from collections import defaultdict</code></p><p>使用例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">d = defaultdict(<span class="hljs-built_in">list</span>)<br>d[<span class="hljs-string">&#x27;a&#x27;</span>].append(<span class="hljs-number">1</span>)<br>d[<span class="hljs-string">&#x27;a&#x27;</span>].append(<span class="hljs-number">2</span>)<br><br>d2 = defaultdict(<span class="hljs-built_in">set</span>)<br>d2[<span class="hljs-string">&#x27;a&#x27;</span>].add(<span class="hljs-number">1</span>)<br>d2[<span class="hljs-string">&#x27;a&#x27;</span>].add(<span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><h2 id="1-7-有序字典"><a href="#1-7-有序字典" class="headerlink" title="1.7 有序字典"></a>1.7 有序字典</h2><p>使用<code>from collections import OrderedDict</code> 会严格按照字典添加的顺序进行。</p><p>可在JSON编码中控制各字段的顺序。</p><p>1.8 字典中的计算</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>基础部分</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3-机器学习-1-理论</title>
    <link href="/2023/09/08/3-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-1-%E7%90%86%E8%AE%BA/"/>
    <url>/2023/09/08/3-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-1-%E7%90%86%E8%AE%BA/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Linux下的锐捷认证</title>
    <link href="/2023/09/08/Linux%E4%B8%8B%E7%9A%84%E9%94%90%E6%8D%B7%E8%AE%A4%E8%AF%81/"/>
    <url>/2023/09/08/Linux%E4%B8%8B%E7%9A%84%E9%94%90%E6%8D%B7%E8%AE%A4%E8%AF%81/</url>
    
    <content type="html"><![CDATA[<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">ping baidu.com -n 100|foreach -<span class="hljs-string">&quot; -f (Get-Date),<span class="hljs-variable">$_</span>&#125;</span><br><span class="hljs-string">#显示时间的PING指令</span><br></code></pre></td></tr></table></figure><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>第一次使用校园网，才知道校园网原来需要登录认证①，并且限制设备②，在没有办理校内校园流量卡的情况下，原来的流量卡只有100kb&#x2F;s速度（办了校园卡之后，速度也仅有2M&#x2F;s），已经严重影响使用，另外校园全覆盖WiFi，同样需要登录认证，速度大概在2mb&#x2F;s，但是无线传输的弊端就是不稳定，间断性断网已经成为常态。</p><p>①：wifi使用学号+密码网页认证登录，有线网需要锐捷客户端v6.84多运营商版本。</p><p>②：wifi+有线，同一帐号仅能支持两个设备登录，这对于联网终端多的人是无法使用的。</p><p>另外，最重要的是常用的编码环境Linux，官方并没有提供对应的linux的认证客户端，所以这也使linux设备的有线网络连接设下了障碍。</p><p>通过网络上前人的研究了解到，使用mentohust或者其迭代版本minieap可以实现linux系统认证通过锐捷，甚至可以将其交叉编译到软路由上；</p><p>那么，理论可行，实践开始。</p><h2 id="1-mentohust"><a href="#1-mentohust" class="headerlink" title="1.mentohust"></a>1.mentohust</h2><p>首先做提前准备，准备mentohust必须的文件。</p><p>gh源码：<code>gh repo clone hyrathb/mentohust</code></p><p>3个客户端源文件和1个mpf抓包</p><p>W32N55.dll0821x.exeSuConfig.datmentohust.mpf</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo <span class="hljs-built_in">mkdir</span> /etc/mentohust<br>sudo <span class="hljs-built_in">cp</span> ./8021x.exe  /etc/mentohust<br>sudo <span class="hljs-built_in">cp</span> ./W32N55.dll /etc/mentohust<br>sudo <span class="hljs-built_in">cp</span> ./SuConfig.dat /etc/mentohust<br>sudo <span class="hljs-built_in">cp</span> ./mentohust.mpf /etc/mentohust<br></code></pre></td></tr></table></figure><p>mpf文件使用MentoHUSTTool进行获取</p><p>可能因为该项目过于久远，最后没能成功认证，换为minieap</p><h2 id="2-minieap"><a href="#2-minieap" class="headerlink" title="2.minieap"></a>2.minieap</h2><p>其原理和mentohust类似，不过需要手动编译两个文件——minieap和libpcap</p><p><a href="https://github.com/updateing/minieap">Minieap</a></p><p>可能由于多运营商的问题，无法获取运营商的id，这个版本的minieap也没能通过认证，所以让linux连接有线网络以失败告终。</p><h2>更变思路</h2><p>一次偶然的机会，联系上了一位学长，他给出了重要的点拨提示。为什么要仿造锐捷进行认证呢？直接使用路由器仿造电脑不就行了。</p><p>不过我一直怀疑寝室里的网存在环路，导致广播风暴时常发生，只能期待校方有朝一日能够解决这个严重的问题。</p>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2023/09/05/hello-world/"/>
    <url>/2023/09/05/hello-world/</url>
    
    <content type="html"><![CDATA[<h3 id="分类文章-文章标签"><a href="#分类文章-文章标签" class="headerlink" title="分类文章\文章标签"></a>分类文章\文章标签</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-string">并列分类，了解一下：</span><br><span class="hljs-attr">categories:</span><br><span class="hljs-bullet">-</span> [<span class="hljs-string">Linux</span>]<br><span class="hljs-bullet">-</span> [<span class="hljs-string">Tools</span>]<br><br><span class="hljs-string">并列+子分类，再了解一下：</span><br><span class="hljs-attr">categories:</span><br><span class="hljs-bullet">-</span> [<span class="hljs-string">Linux</span>, <span class="hljs-string">Hexo</span>]<br><span class="hljs-bullet">-</span> [<span class="hljs-string">Tools</span>, <span class="hljs-string">PHP</span>]<br><br><span class="hljs-attr">categories:</span><br><span class="hljs-bullet">-</span> <span class="hljs-string">Diary</span><br><span class="hljs-attr">tags:</span><br><span class="hljs-bullet">-</span> <span class="hljs-string">PS3</span><br><span class="hljs-bullet">-</span> <span class="hljs-string">Games</span><br></code></pre></td></tr></table></figure><h3 id="归档文章"><a href="#归档文章" class="headerlink" title="归档文章"></a>归档文章</h3><p>如果只是想让文章在首页隐藏，但仍<strong>需要在归档分类页里展示</strong>，可以在文章开头 <a href="https://hexo.io/zh-cn/docs/front-matter">front-matter (opens new window)</a>中配置 <code>archive: true</code> 属性。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">文章标题</span><br><span class="hljs-attr">index_img:</span> <span class="hljs-string">/img/example.jpg</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2019-10-10 10:00:00</span><br><span class="hljs-attr">archive:</span> <span class="hljs-literal">true</span><br><span class="hljs-meta">---</span><br><span class="hljs-string">以下是文章内容</span><br></code></pre></td></tr></table></figure><h3 id="文章排序"><a href="#文章排序" class="headerlink" title="文章排序"></a>文章排序</h3><p>如果想手动将某些文章固定在首页靠前的位置，可以在安装 <code>hexo-generator-index</code> &gt;&#x3D; 2.0.0 版本的情况下，在文章开头 <a href="https://hexo.io/zh-cn/docs/front-matter">front-matter (opens new window)</a>中配置 <code>sticky</code> 属性：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">文章标题</span><br><span class="hljs-attr">index_img:</span> <span class="hljs-string">/img/example.jpg</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2019-10-10 10:00:00</span><br><span class="hljs-attr">sticky:</span> <span class="hljs-number">100</span><br><span class="hljs-meta">---</span><br><span class="hljs-string">以下是文章内容</span><br></code></pre></td></tr></table></figure><p><code>sticky</code> 数值越大，该文章越靠前，达到类似于置顶的效果，其他未设置的文章依然按默认排序。</p><h3 id="文章在首页的封面图"><a href="#文章在首页的封面图" class="headerlink" title="文章在首页的封面图"></a>文章在首页的封面图</h3><p>对于单篇文章，在文章开头 <a href="https://hexo.io/zh-cn/docs/front-matter">front-matter (opens new window)</a>中配置 <code>index_img</code> 属性。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">文章标题</span><br><span class="hljs-attr">tags:</span> [<span class="hljs-string">Hexo</span>, <span class="hljs-string">Fluid</span>]<br><span class="hljs-attr">index_img:</span> <span class="hljs-string">/img/example.jpg</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2019-10-10 10:00:00</span><br><span class="hljs-meta">---</span><br><span class="hljs-string">以下是文章内容</span><br></code></pre></td></tr></table></figure><p>和 Banner 配置相同，<code>/img/example.jpg</code> 对应的是存放在 <code>/source/img/example.jpg</code> 目录下的图片（目录也可自定义，但必须在 source 目录下）。</p><p>也可以使用外链 Url 的绝对路径。</p><p>如果想统一给文章设置一个默认图片（文章不设置 <code>index_img</code> 则默认使用这张图片），可在<strong>主题配置</strong>中设置：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">post:</span><br>  <span class="hljs-attr">default_index_img:</span> <span class="hljs-string">/images/example.jpeg</span><br></code></pre></td></tr></table></figure><p>当 <code>default_index_img</code> 和 <code>index_img</code> 都为空时，该文章在首页将不显示图片。</p><h3 id="文章页顶部大图"><a href="#文章页顶部大图" class="headerlink" title="文章页顶部大图"></a>文章页顶部大图</h3><p>默认显示<strong>主题配置</strong>中的 <code>post.banner_img</code>，如需要设置单个文章的 Banner，在 <a href="https://hexo.io/zh-cn/docs/front-matter">front-matter (opens new window)</a>中指定 <code>banner_img</code> 属性。</p><p>本地图片存放位置同上。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">文章标题</span><br><span class="hljs-attr">tags:</span> [<span class="hljs-string">Hexo</span>, <span class="hljs-string">Fluid</span>]<br><span class="hljs-attr">index_img:</span> <span class="hljs-string">/img/example.jpg</span><br><span class="hljs-attr">banner_img:</span> <span class="hljs-string">/img/post_banner.jpg</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2019-10-10 10:00:00</span><br><span class="hljs-meta">---</span><br><span class="hljs-string">以下是文章内容</span><br></code></pre></td></tr></table></figure><h3 id="Tag-插件"><a href="#Tag-插件" class="headerlink" title="Tag 插件"></a>Tag 插件</h3><h4 id="便签"><a href="#便签" class="headerlink" title="#便签"></a><a href="https://hexo.fluid-dev.com/docs/guide/#%E4%BE%BF%E7%AD%BE">#</a>便签</h4><p>在 markdown 中加入如下的代码来使用便签：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs markdown">&#123;% note success %&#125;<br>文字 或者 <span class="hljs-code">`markdown`</span> 均可<br>&#123;% endnote %&#125;<br></code></pre></td></tr></table></figure><p>或者使用 HTML 形式：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">p</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;note note-primary&quot;</span>&gt;</span>标签<span class="hljs-tag">&lt;/<span class="hljs-name">p</span>&gt;</span><br></code></pre></td></tr></table></figure><p>可选便签：</p><h4 id=""><a href="#" class="headerlink" title=""></a><img src="/../images/hello-world/image-20240306164525216.png" alt="标签"></h4><h4 id="行内标签"><a href="#行内标签" class="headerlink" title="行内标签"></a>行内标签</h4><p>在 markdown 中加入如下的代码来使用 Label：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs markdown">&#123;% label primary @text %&#125;<br></code></pre></td></tr></table></figure><p>或者使用 HTML 形式：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">span</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;label label-primary&quot;</span>&gt;</span>Label<span class="hljs-tag">&lt;/<span class="hljs-name">span</span>&gt;</span><br></code></pre></td></tr></table></figure><p>可选 Label：</p><p>primary default info success warning danger</p><h4 id="折叠块"><a href="#折叠块" class="headerlink" title="折叠块"></a>折叠块</h4><p>使用折叠块，可以折叠代码、图片、文字等任何内容，你可以在 markdown 中按如下格式：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs markdown">&#123;% fold info @title %&#125;<br>需要折叠的一段内容，支持 markdown<br>&#123;% endfold %&#125;<br></code></pre></td></tr></table></figure><p>info: 和行内标签类似的可选参数 title: 折叠块上的标题</p><h4 id="勾选框"><a href="#勾选框" class="headerlink" title="勾选框"></a>勾选框</h4><p>在 markdown 中加入如下的代码来使用 Checkbox：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs markdown">&#123;% cb text, checked?, incline? %&#125;<br></code></pre></td></tr></table></figure><p>text：显示的文字<br>checked：默认是否已勾选，默认 false<br>incline: 是否内联（可以理解为后面的文字是否换行），默认 false</p><p>示例：</p><div>            <input type="checkbox" disabled >普通示例          </div><div>            <input type="checkbox" disabled checked="checked">默认选中          </div>            <input type="checkbox" disabled >内联示例           后面文字不换行<input type="checkbox" disabled > 也可以只传入一个参数，文字写在后边（这样不支持外联）<h4 id="按钮"><a href="#按钮" class="headerlink" title="按钮"></a>按钮</h4><p>你可以在 markdown 中加入如下的代码来使用 Button：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs markdown">&#123;% btn url, text, title %&#125;<br></code></pre></td></tr></table></figure><p>或者使用 HTML 形式：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">a</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;btn&quot;</span> <span class="hljs-attr">href</span>=<span class="hljs-string">&quot;url&quot;</span> <span class="hljs-attr">title</span>=<span class="hljs-string">&quot;title&quot;</span>&gt;</span>text<span class="hljs-tag">&lt;/<span class="hljs-name">a</span>&gt;</span><br></code></pre></td></tr></table></figure><p>url：跳转链接<br>text：显示的文字<br>title：鼠标悬停时显示的文字（可选）</p><p><a href="javascript:;">text</a></p><h4 id="组图"><a href="#组图" class="headerlink" title="组图"></a>组图</h4><p>如果想把多张图片按一定布局组合显示，你可以在 markdown 中按如下格式：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs markdown">&#123;% gi total n1-n2-... %&#125;<br>  ![](<span class="hljs-link">url</span>)<br>  ![](<span class="hljs-link">url</span>)<br>  ![](<span class="hljs-link">url</span>)<br>  ![](<span class="hljs-link">url</span>)<br>  ![](<span class="hljs-link">url</span>)<br>&#123;% endgi %&#125;<br></code></pre></td></tr></table></figure><p>total：图片总数量，对应中间包含的图片 url 数量<br>n1-n2-…：每行的图片数量，可以省略，默认单行最多 3 张图，求和必须相等于 total，否则按默认样式</p><p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p><h3 id="0-论文-md"><a href="#0-论文-md" class="headerlink" title="0-论文.md"></a>0-论文.md</h3><p>基本格式：</p><p>h2论文标题</p><p>h3背景</p><p>h3实验方法</p><p>h3实验结果</p><p>h3结论</p><p>*h3代码分析</p>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
      <category>Hexo</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
