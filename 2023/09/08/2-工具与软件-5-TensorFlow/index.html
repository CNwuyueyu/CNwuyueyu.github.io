

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=dark>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Wuyueyu">
  <meta name="keywords" content="">
  
    <meta name="description" content="环境的安装首先去conda官网下载  conda  linux系统先使用bash安装 1sudo bash xxx.sh  安装后在pycharm配置conda环境，然后新建AI项目，选择conda，然后在所选择的解释器中安装tensorflow 选择pycharm自动安装（会自动安装其他依赖，十分方便）  所需安装：  conda tensorflow  如果你是N卡，可继续在项目终端中输入 1">
<meta property="og:type" content="article">
<meta property="og:title" content="2-工具与软件-5-TensorFlow">
<meta property="og:url" content="http://example.com/2023/09/08/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/index.html">
<meta property="og:site_name" content="Blog">
<meta property="og:description" content="环境的安装首先去conda官网下载  conda  linux系统先使用bash安装 1sudo bash xxx.sh  安装后在pycharm配置conda环境，然后新建AI项目，选择conda，然后在所选择的解释器中安装tensorflow 选择pycharm自动安装（会自动安装其他依赖，十分方便）  所需安装：  conda tensorflow  如果你是N卡，可继续在项目终端中输入 1">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230912155352950.png">
<meta property="og:image" content="http://example.com/images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230912155649291.png">
<meta property="og:image" content="http://example.com/images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240305095606298.png">
<meta property="og:image" content="http://example.com/images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240305095654806.png">
<meta property="og:image" content="http://example.com/images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913093248414.png">
<meta property="og:image" content="http://example.com/images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913105626307.png">
<meta property="og:image" content="http://example.com/images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913140637316.png">
<meta property="og:image" content="http://example.com/images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913145516199.png">
<meta property="og:image" content="http://example.com/images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913150028635.png">
<meta property="og:image" content="http://example.com/images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913151017142.png">
<meta property="og:image" content="http://example.com/images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913151343657.png">
<meta property="og:image" content="http://example.com/images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913152152906.png">
<meta property="og:image" content="http://example.com/images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913152543744.png">
<meta property="og:image" content="http://example.com/images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913153356947.png">
<meta property="og:image" content="http://example.com/images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913162344502.png">
<meta property="og:image" content="http://example.com/images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913162811491.png">
<meta property="og:image" content="http://example.com/images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913170454005.png">
<meta property="og:image" content="http://example.com/images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913170523568.png">
<meta property="og:image" content="http://example.com/images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230916150852673.png">
<meta property="og:image" content="http://example.com/images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230916164058968.png">
<meta property="og:image" content="http://example.com/images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230916171743300.png">
<meta property="og:image" content="http://example.com/images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230916173848124.png">
<meta property="og:image" content="http://example.com/images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230916174816575.png">
<meta property="og:image" content="http://example.com/images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240228211041340.png">
<meta property="og:image" content="http://example.com/images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240228211544435.png">
<meta property="og:image" content="http://example.com/images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240228211600833.png">
<meta property="og:image" content="http://example.com/images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240228211847780.png">
<meta property="og:image" content="http://example.com/images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240228212108454.png">
<meta property="og:image" content="http://example.com/images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240228214733104.png">
<meta property="og:image" content="http://example.com/images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240228214753084.png">
<meta property="og:image" content="http://example.com/images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240302132316265.png">
<meta property="og:image" content="http://example.com/images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240302132135165.png">
<meta property="og:image" content="http://example.com/images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240302153943645.png">
<meta property="og:image" content="http://example.com/images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240302180849506.png">
<meta property="og:image" content="http://example.com/images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240302181058519.png">
<meta property="og:image" content="http://example.com/images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240302181218194.png">
<meta property="article:published_time" content="2023-09-08T09:29:25.000Z">
<meta property="article:modified_time" content="2024-03-06T08:40:19.592Z">
<meta property="article:author" content="Wuyueyu">
<meta property="article:tag" content="TensorFlow">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230912155352950.png">
  
  
  
  <title>2-工具与软件-5-TensorFlow - Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.5-a","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":"3028c6400adaaa4521a5b772ad242a5e","google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>CV</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/images/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="2-工具与软件-5-TensorFlow"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-09-08 17:29" pubdate>
          上传日期：2023年9月8日
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          21k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          175 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">2-工具与软件-5-TensorFlow</h1>
            
            
              <div class="markdown-body">
                
                <h2 id="环境的安装"><a href="#环境的安装" class="headerlink" title="环境的安装"></a>环境的安装</h2><p>首先去conda官网下载  <a target="_blank" rel="noopener" href="https://repo.anaconda.com/">conda</a></p>
<p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230912155352950.png" srcset="/img/loading.gif" lazyload alt="选择合适的版本"></p>
<p>linux系统先使用bash安装</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo bash xxx.sh<br></code></pre></td></tr></table></figure>

<p>安装后在pycharm配置conda环境，然后新建AI项目，选择conda，然后在所选择的解释器中安装tensorflow</p>
<p>选择pycharm自动安装（会自动安装其他依赖，十分方便）</p>
<p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230912155649291.png" srcset="/img/loading.gif" lazyload alt="tensorflow安装"></p>
<p>所需安装：</p>
<ul>
<li>conda</li>
<li>tensorflow</li>
</ul>
<p>如果你是N卡，可继续在项目终端中输入</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">conda install cudatoolkit<br></code></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">conda install cudnn<br></code></pre></td></tr></table></figure>

<p>安装GUP加速</p>
<h2 id="1-1-人工智能三学派"><a href="#1-1-人工智能三学派" class="headerlink" title="1.1 人工智能三学派"></a>1.1 人工智能三学派</h2><p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240305095606298.png" srcset="/img/loading.gif" lazyload alt="三学派"></p>
<p>行为主义：机器人的摔倒预测</p>
<p>符号主义：用公式描述的人工智能，让PC具有了理性思维</p>
<p>连接主义：仿造人的感性思维</p>
<h2 id="1-2-神经网络的设计过程"><a href="#1-2-神经网络的设计过程" class="headerlink" title="1.2 神经网络的设计过程"></a>1.2 神经网络的设计过程</h2><p>用神经网络实现鸢尾花的分类：<strong>梯度下降</strong></p>
<p>目的：找到一组参数w和b，使得损失函数最小。</p>
<p>梯度：函数对各参数<strong>求偏导</strong>后的向量。 <u>梯度下降的方向是函数减小的方向</u></p>
<p>梯度下降法：沿损失函数梯度下降的方向，寻找损失函数的最小值，得到最优参数的方法</p>
<p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240305095654806.png" srcset="/img/loading.gif" lazyload alt="学习率"></p>
<p>学习率（lr）：设置过小，收敛缓慢；设置过大，无法收敛（找不到最小值）</p>
<p>反向传播：从后向前，逐层求损失函数对每层神经元参数的偏导数，迭代更新所有参数。</p>
<p>损失函数:<br>$$<br>loss &#x3D; （w + 1 )^2<br>$$</p>
<p>$$<br>\frac{\part loss}{\part w} &#x3D; 2w +2<br>$$</p>
<p>代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><br>w = tf.Variable(tf.constant(<span class="hljs-number">5</span>, dtype=tf.float32))<br>lr = <span class="hljs-number">0.2</span>  <span class="hljs-comment"># 学习率</span><br>epoch = <span class="hljs-number">40</span>   <span class="hljs-comment"># 循环迭代数</span><br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epoch):  <span class="hljs-comment"># for epoch 定义顶层循环，表示对数据集循环epoch次，此例数据集数据仅有1个w,初始化时候constant赋值为5，循环40次迭代。</span><br>    <span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:  <span class="hljs-comment"># with结构到grads框起了梯度的计算过程。</span><br>        loss = tf.square(w + <span class="hljs-number">1</span>)<br>    grads = tape.gradient(loss, w)  <span class="hljs-comment"># .gradient函数告知谁对谁求导</span><br><br>    w.assign_sub(lr * grads)  <span class="hljs-comment"># .assign_sub 对变量做自减 即：w -= lr*grads 即 w = w - lr*grads</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;After %s epoch,w is %f,loss is %f&quot;</span> % (epoch, w.numpy(), loss))<br><br><span class="hljs-comment"># lr初始值：0.2   请自改学习率  0.001  0.999 看收敛过程</span><br><span class="hljs-comment"># 最终目的：找到 loss 最小 即 w = -1 的最优参数w</span><br><br></code></pre></td></tr></table></figure>

<p>Output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python">After <span class="hljs-number">0</span> epoch,w <span class="hljs-keyword">is</span> <span class="hljs-number">2.600000</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">36.000000</span><br>After <span class="hljs-number">1</span> epoch,w <span class="hljs-keyword">is</span> <span class="hljs-number">1.160000</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">12.959999</span><br>After <span class="hljs-number">2</span> epoch,w <span class="hljs-keyword">is</span> <span class="hljs-number">0.296000</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">4.665599</span><br>After <span class="hljs-number">3</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.222400</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">1.679616</span><br>After <span class="hljs-number">4</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.533440</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.604662</span><br>After <span class="hljs-number">5</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.720064</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.217678</span><br>After <span class="hljs-number">6</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.832038</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.078364</span><br>After <span class="hljs-number">7</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.899223</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.028211</span><br>After <span class="hljs-number">8</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.939534</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.010156</span><br>After <span class="hljs-number">9</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.963720</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.003656</span><br>After <span class="hljs-number">10</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.978232</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.001316</span><br>After <span class="hljs-number">11</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.986939</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000474</span><br>After <span class="hljs-number">12</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.992164</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000171</span><br>After <span class="hljs-number">13</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.995298</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000061</span><br>After <span class="hljs-number">14</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.997179</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000022</span><br>After <span class="hljs-number">15</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.998307</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000008</span><br>After <span class="hljs-number">16</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.998984</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000003</span><br>After <span class="hljs-number">17</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.999391</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000001</span><br>After <span class="hljs-number">18</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.999634</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">19</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.999781</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">20</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.999868</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">21</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.999921</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">22</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.999953</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">23</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.999972</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">24</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.999983</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">25</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.999990</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">26</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.999994</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">27</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.999996</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">28</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.999998</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">29</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.999999</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">30</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">0.999999</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">31</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">1.000000</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">32</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">1.000000</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">33</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">1.000000</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">34</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">1.000000</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">35</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">1.000000</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">36</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">1.000000</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">37</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">1.000000</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">38</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">1.000000</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br>After <span class="hljs-number">39</span> epoch,w <span class="hljs-keyword">is</span> -<span class="hljs-number">1.000000</span>,loss <span class="hljs-keyword">is</span> <span class="hljs-number">0.000000</span><br></code></pre></td></tr></table></figure>



<h2 id="1-3-张量生成"><a href="#1-3-张量生成" class="headerlink" title="1.3 张量生成"></a>1.3 张量生成</h2><p>张量（Tensor：多维数组 &#x2F;列表  ）        阶 ：张量的维数</p>
<table>
<thead>
<tr>
<th>维数</th>
<th>阶</th>
<th>名</th>
<th>例</th>
</tr>
</thead>
<tbody><tr>
<td>0-D</td>
<td>0</td>
<td>标量 scalar</td>
<td>s&#x3D;1</td>
</tr>
<tr>
<td>1-D</td>
<td>1</td>
<td>向量 vector</td>
<td>v&#x3D;[1,2,3]</td>
</tr>
<tr>
<td>2-D</td>
<td>2</td>
<td>矩阵 matrix</td>
<td>m&#x3D;[[1,2],[3,4],[5,6]]</td>
</tr>
<tr>
<td>n-D</td>
<td>n</td>
<td>张量 tensor</td>
<td>t&#x3D;[[[[……]]]] (n个)</td>
</tr>
</tbody></table>
<p>数据类型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">·tf.<span class="hljs-built_in">int</span>   tf.<span class="hljs-built_in">float</span> ...<br>tf.<span class="hljs-built_in">int</span> <span class="hljs-number">32</span>  , tf.<span class="hljs-built_in">float</span> <span class="hljs-number">32</span>  , tf.<span class="hljs-built_in">float</span> <span class="hljs-number">64</span><br>·tf.<span class="hljs-built_in">bool</span><br>tf.constant([true, false])<br>·tf.string<br>tf.constant(<span class="hljs-string">&quot;Hello world!&quot;</span>)<br></code></pre></td></tr></table></figure>

<p>创建Tensor</p>
<p><code>tf.constant(张量内容，dtype=数据类型(可选))</code></p>
<p>创建全为0的张量 <code>tf.zeros(维度)</code>  </p>
<p>​	 纬度:一维直接写个数；二维[行，列]；多维[n,m,j,k,…..]</p>
<p>创建全为1的张量 <code>tf.ones(纬度)</code></p>
<p>创建全为指定值的张量 <code>tf.fill(维度，指定值)</code></p>
<p>正态分部的随机数，默认值为0,标准差为1</p>
<p><code>tf.random.normal(纬度，mean=均值，stddev=标准差)</code></p>
<p>生成截断式正态分布的随机数</p>
<p><code>tf.random.truncated_normal(纬度，mean=均值，stddev=标准差)</code></p>
<p>在正态分布中如果随机生成的数据的取值在（$\mu\pm2\sigma$)</p>
<p>生成均匀分布的随机数</p>
<p><code>tf.random.uniform(纬度，minval=最小值，maxval=最大值)</code></p>
<h2 id="1-4-TF2常用函数"><a href="#1-4-TF2常用函数" class="headerlink" title="1.4 TF2常用函数"></a>1.4 TF2常用函数</h2><p>强制tensor转换为该数据类型<br><code>tf.cast (张量名，dtype=数据类型)</code><br>计算张量维度上元素的最小值<br><code>tf.reduce_min (张量名)</code><br>计算张量维度上元素的最大值<br><code>tf.reduce_max (张量名)</code></p>
<p>理解axis<br>在一个二维张量或数组中，可以通过调整 axis 等于0或1 控制执行维度。<br> axis&#x3D;0代表跨行（经度，down)，而axis&#x3D;1代表跨列（纬度，across)<br> 如果不指定axis，则所有元素参与计算。</p>
<p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913093248414.png" srcset="/img/loading.gif" lazyload alt="理解axis"></p>
<p>计算张量沿着指定维度的平均值<br><code>tf.reduce_mean (张量名，axis=操作轴)</code>  (不指定axis，则对所有元素进行操作)<br>计算张量沿着指定维度的和<br><code>tf.reduce_sum (张量名，axis=操作轴)</code></p>
<p><code>tf.Variable () </code>将变量标记为“可训练”，被标记的变量会在反向传播<br>中记录梯度信息。神经网络训练中，常用该函数标记待训练参数。<br><code>tf.Variable(初始值)</code><br><code>w = tf.Variable(tf.random.normal([2, 2], mean=0, stddev=1))</code></p>
<p>TensorFlow中的数学运算<br>对应元素的四则运算：<code>tf.add</code>，<code>tf.subtract</code>，<code>tf.multiply</code>，<code>tf.divide</code>    </p>
<p> 只有纬度相同的张量才能做四则运算。<br>平方、次方与开方：<code> tf.square</code>，<code>tf.pow</code>，<code>tf.sqrt</code><br>矩阵乘：<code>tf.matmul</code></p>
<p>切分传入张量的第一维度，生成输入特征&#x2F;标签对，构建数据集<br><code>data = tf.data.Dataset.from_tensor_slices((输入特征, 标签))</code><br>（Numpy和Tensor格式都可用该语句读入数据）</p>
<p><code>tf.GradientTape</code><br>with结构记录计算过程，gradient求出张量的梯度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> tf.GradientTape( ) <span class="hljs-keyword">as</span> tape:<br>	若干个计算过程<br>grad=tape.gradient(函数，对谁求导)<br></code></pre></td></tr></table></figure>

<p>enumerate是python的内建函数，它可遍历每个元素(如列表、元组<br>或字符串)，组合为：索引 元素，常在for循环中使用。<br><code>enumerate(列表名)</code></p>
<p>独热编码：在分类问题中，常用独热码做标签，标记类别：1表示是，0表示非。 <code>tf.one_hot (待转换数据, depth=几分类)</code></p>
<p>当n分类的n个输出 （y0 ，y1, …… yn-1）通过softmax( ) 函数，<br>便符合概率分布了。也就是说，将多个权重占比划分归为1。<br>$$<br>\forall x \ \ P(X &#x3D; x) \in [0,1] 且 \sum_{x}P(X &#x3D; x) &#x3D; 1<br>$$</p>
<p>assign_sub 赋值操作，更新参数的值并返回。<br>调用assign_sub前，先用 tf.Variable 定义变量 w 为可训练（可自更新）。<br>w.assign_sub (w要自减的内容)</p>
<p>返回张量沿指定维度最大值的索引<br>tf.argmax (张量名,axis&#x3D;操作轴)     numpy中也有类似函数</p>
<h2 id="1-5-鸢尾花数据集的读入"><a href="#1-5-鸢尾花数据集的读入" class="headerlink" title="1.5 鸢尾花数据集的读入"></a>1.5 鸢尾花数据集的读入</h2><p>Setosa Iris（狗尾草鸢尾），Versicolour Iris（杂色鸢尾），Virginica Iris（弗吉尼亚鸢尾）</p>
<p>鸢尾花数据来源：sklearn框架</p>
<p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913105626307.png" srcset="/img/loading.gif" lazyload alt="3种鸢尾花"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets <br><span class="hljs-keyword">from</span> pandas <span class="hljs-keyword">import</span> DataFrame<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br>x_data = datasets.load_iris().data  <span class="hljs-comment"># .data返回iris数据集所有输入特征</span><br>y_data = datasets.load_iris().target  <span class="hljs-comment"># .target返回iris数据集所有标签</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;x_data from datasets: \n&quot;</span>, x_data)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;y_data from datasets: \n&quot;</span>, y_data)<br><br>x_data = DataFrame(x_data, columns=[<span class="hljs-string">&#x27;花萼长度&#x27;</span>, <span class="hljs-string">&#x27;花萼宽度&#x27;</span>, <span class="hljs-string">&#x27;花瓣长度&#x27;</span>, <span class="hljs-string">&#x27;花瓣宽度&#x27;</span>])  <span class="hljs-comment"># 为表格增加行索引（左侧）和列标签（上方）</span><br>pd.set_option(<span class="hljs-string">&#x27;display.unicode.east_asian_width&#x27;</span>, <span class="hljs-literal">True</span>)  <span class="hljs-comment"># 设置列名对齐</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;x_data add index: \n&quot;</span>, x_data)<br><br>x_data[<span class="hljs-string">&#x27;类别&#x27;</span>] = y_data  <span class="hljs-comment"># 新加一列，列标签为‘类别’，数据为y_data</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;x_data add a column: \n&quot;</span>, x_data)<br><br><span class="hljs-comment"># 类型维度不确定时，建议用print函数打印出来确认效果</span><br></code></pre></td></tr></table></figure>

<h2 id="1-8-神经网络实现鸢尾花的分类"><a href="#1-8-神经网络实现鸢尾花的分类" class="headerlink" title="1.8 神经网络实现鸢尾花的分类"></a>1.8 神经网络实现鸢尾花的分类</h2><p>1.准备数据</p>
<p>​	数据集读入</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 从sklearn包datasets 读入数据集：</span><br><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> datasets<br>x_data = datasets.load_iris().data <span class="hljs-comment"># 返回iris数据集所有输入特征</span><br>y_data = datasets.load_iris().target <span class="hljs-comment"># 返回iris数据集所有标签</span><br></code></pre></td></tr></table></figure>

<p>​	数据集乱序</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">np.random.seed(<span class="hljs-number">116</span>) <span class="hljs-comment"># 使用相同的seed，使输入特征/标签一一对应</span><br>np.random.shuffle(x_data)<br>np.random.seed(<span class="hljs-number">116</span>)<br>np.random.shuffle(y_data)<br>tf.random.set_seed(<span class="hljs-number">116</span>)<br></code></pre></td></tr></table></figure>

<p>​	分成用不相见的训练集和测试集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">x_train = x_data[:-<span class="hljs-number">30</span>]<br>y_train = y_data[:-<span class="hljs-number">30</span>]<br>x_test = x_data[-<span class="hljs-number">30</span>:]<br>y_test = y_data[-<span class="hljs-number">30</span>:]<br></code></pre></td></tr></table></figure>

<p>​	配成【输入特征，标签】对，每次喂入一个batch</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(<span class="hljs-number">32</span>)<br>test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(<span class="hljs-number">32</span>)<br></code></pre></td></tr></table></figure>

<p>2.搭建网络</p>
<p>​	定义神经网络中的所有可训练参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">w1 = tf.Variable(tf.random.truncated_normal([ <span class="hljs-number">4</span>, <span class="hljs-number">3</span> ], stddev=<span class="hljs-number">0.1</span>, seed=<span class="hljs-number">1</span>)) <span class="hljs-comment"># 四种特征，三个结果</span><br>b1 = tf.Variable(tf.random.truncated_normal([ <span class="hljs-number">3</span> ], stddev=<span class="hljs-number">0.1</span>, seed=<span class="hljs-number">1</span>))<br></code></pre></td></tr></table></figure>

<p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913140637316.png" srcset="/img/loading.gif" lazyload alt="输入层与输出层"></p>
<p>3.参数优化</p>
<p>​	嵌套循环迭代，with结构更新参数，显示当前loss</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epoch): <span class="hljs-comment">#数据集级别迭代</span><br><span class="hljs-keyword">for</span> step, (x_train, y_train) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_db): <span class="hljs-comment">#batch级别迭代</span><br><span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape: <span class="hljs-comment"># 记录梯度信息</span><br>前向传播过程计算y<br>计算总loss<br>grads = tape.gradient(loss, [ w1, b1 ])<br>w1.assign_sub(lr * grads[<span class="hljs-number">0</span>]) <span class="hljs-comment">#参数自更新</span><br>b1.assign_sub(lr * grads[<span class="hljs-number">1</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Epoch &#123;&#125;, loss: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(epoch, loss_all/<span class="hljs-number">4</span>))<br></code></pre></td></tr></table></figure>



<p>4.测试效果</p>
<p>​	计算当前参数前向传播后的准确率，显示当前acc</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> x_test, y_test <span class="hljs-keyword">in</span> test_db:<br>y = tf.matmul(h, w) + b <span class="hljs-comment"># y为预测结果</span><br>y = tf.nn.softmax(y)<br><span class="hljs-comment"># y符合概率分布</span><br>pred = tf.argmax(y, axis=<span class="hljs-number">1</span>) <span class="hljs-comment"># 返回y中最大值的索引，即预测的分类</span><br>pred = tf.cast(pred, dtype=y_test.dtype) <span class="hljs-comment">#调整数据类型与标签一致</span><br>correct = tf.cast(tf.equal(pred, y_test), dtype=tf.int32)<br>correct = tf.reduce_sum (correct) <span class="hljs-comment"># 将每个batch的correct数加起来</span><br>total_correct += <span class="hljs-built_in">int</span> (correct) <span class="hljs-comment"># 将所有batch中的correct数加起来</span><br>total_number += x_test.shape [<span class="hljs-number">0</span>]<br>acc = total_correct / total_number<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;test_acc:&quot;</span>, acc)<br></code></pre></td></tr></table></figure>

<p>5.acc &#x2F; loss 可视化（查看效果）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.title(<span class="hljs-string">&#x27;Acc Curve&#x27;</span>) <span class="hljs-comment"># 图片标题</span><br>plt.xlabel(<span class="hljs-string">&#x27;Epoch&#x27;</span>) <span class="hljs-comment"># x轴名称</span><br>plt.ylabel(<span class="hljs-string">&#x27;Acc&#x27;</span>) <span class="hljs-comment"># y轴名称</span><br>plt.plot(test_acc, label=<span class="hljs-string">&quot;$Accuracy$&quot;</span>) <span class="hljs-comment"># 逐点画出test_acc值并连线</span><br>plt.legend()<br>plt.show()<br></code></pre></td></tr></table></figure>

<h2 id="2-1-预备知识"><a href="#2-1-预备知识" class="headerlink" title="2.1 预备知识"></a>2.1 预备知识</h2><p>函数：</p>
<p><code>tf.where()</code>  条件语句真返回A，条件语句假返回B<br><code>tf.where(条件语句，真返回A，假返回B)</code></p>
<p><code>np.random.RandomState.rand()</code>返回一个[0,1)之间的随机数<br><code>np.random.RandomState.rand(维度) #维度为空，返回标量</code></p>
<p><code>np.vstack()</code>将两个数组按垂直方向叠加<br><code>np.vstack(数组1，数组2)</code></p>
<p><code>np.mgrid[ ] </code>返回间隔数值点，可同时返回多组， [起始值 结束值)<br><code>np.mgrid[ 起始值 : 结束值 : 步长 ，起始值 : 结束值 : 步长 , … ]</code></p>
<p><code> x.ravel( )</code> 将x变为一维数组，“把. 前变量拉直”<br><code>np.c\_[ ] </code>使返回的间隔数值点配对<br><code>np.c\_[ 数组1，数组2， … ]</code></p>
<h2 id="2-2-复杂度学习率"><a href="#2-2-复杂度学习率" class="headerlink" title="2.2 复杂度学习率"></a>2.2 复杂度学习率</h2><p>NN复杂度：多用NN层数和NN参数的个数表示</p>
<p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913145516199.png" srcset="/img/loading.gif" lazyload alt="复杂度"></p>
<p>空间复杂度：<br>    层数 &#x3D; 隐藏层的层数 + 1个输出层<br>    图为2层NN<br>                总参数 &#x3D; 总w + 总b<br>                图中 3x4+4 + 4x2+2 &#x3D; 26</p>
<p>时间复杂度：<br>    乘加运算次数<br>    左图 3x4 +  4x2 &#x3D; 20</p>
<p>学习率：</p>
<p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913150028635.png" srcset="/img/loading.gif" lazyload alt="学习率"></p>
<p>指数衰减学习率：<br>可以先用较大的学习率，快速得到较优解，然后逐步减小学习率，使<br>模型在训练后期稳定。<br><code>指数衰减学习率 = 初始学习率 * 学习率衰减率（ 当前轮数 / 多少轮衰减一次 ）</code></p>
<h2 id="2-3-激活函数"><a href="#2-3-激活函数" class="headerlink" title="2.3 激活函数"></a>2.3 激活函数</h2><p>优秀的激活函数：<br>• 非线性： 激活函数非线性时，多层神经网络可逼近所有函数<br>• 可微性： 优化器大多用梯度下降更新参数<br>• 单调性： 当激活函数是单调的，能保证单层网络的损失函数是凸函数<br>• 近似恒等性： f(x)≈x当参数初始化为随机小值时，神经网络更稳定</p>
<p>激活函数输出值的范围：<br>• 激活函数输出为有限值时，基于梯度的优化方法更稳定<br>• 激活函数输出为无限值时，建议调小学习率</p>
<p>Sigmoid函数：<br>$$<br>f(x) &#x3D; \frac{1}{1 + e ^ {-x}}<br>$$<br><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913151017142.png" srcset="/img/loading.gif" lazyload alt="Sigmoid函数"></p>
<p>特点<br>（1）易造成梯度消失<br>（2）输出非0均值，收敛慢<br>（3）幂运算复杂，训练时间长<br>目前Sigmoid函数因计算复杂，已接近弃用。</p>
<p>Tanh函数：<br>$$<br>f(x) &#x3D; \frac{1-e^{-2x}}{1+e^{-2x}}<br>$$<br><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913151343657.png" srcset="/img/loading.gif" lazyload alt="Tanh函数"></p>
<p>特点<br>（1）输出是0均值<br>（2）易造成梯度消失<br>（3）幂运算复杂，训练时间长</p>
<p>Relu函数：<br>$$<br>f(x) &#x3D; max(x , 0) &#x3D; \begin{cases}0 \quad x&lt;0 \\ x \quad x\geq0 \end{cases}<br>$$<br><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913152152906.png" srcset="/img/loading.gif" lazyload alt="Relu函数"></p>
<p>优点：<br>（1） 解决了梯度消失问题 (在正区间)<br>（2） 只需判断输入是否大于0，计算速度快<br>（3） 收敛速度远快于sigmoid和tanh</p>
<p>缺点：<br>（1） 输出非0均值，收敛慢<br>（2） Dead RelU问题：某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。（神经元死亡）</p>
<p>Leaky Relu函数：<br>$$<br>f(x) &#x3D; max (ax,x)<br>$$<br><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913152543744.png" srcset="/img/loading.gif" lazyload alt="Leaky Relu函数"></p>
<p>理论上来讲，Leaky Relu有Relu的所有优点，外加不会有Dead Relu问题，但是在实际操作当中，并没有完全证明Leaky Relu总是好于Relu。</p>
<p>对于初学者的建议：<br>首选relu激活函数；<br>学习率设置较小值；<br>输入特征标准化，即让输入特征满足以0为均值，1为标准差的正态分布；<br>初始参数中心化，即让随机生成的参数满足以0为均值,$\sqrt{\frac{2}{当前层输入特征个数}}$为标准差的正态分布。</p>
<h2 id="2-4-损失函数"><a href="#2-4-损失函数" class="headerlink" title="2.4 损失函数"></a>2.4 损失函数</h2><p>预测值（y）与已知答案（_y）的差距</p>
<p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913153356947.png" srcset="/img/loading.gif" lazyload alt="主流的三种计算方法"></p>
<p>均方误差mse：<br>$$<br>MSE(y_,y)&#x3D;\frac{\sum_{i&#x3D;1}^n (y-y_)^2}{n}<br>$$<br><code>lost_mse = tf.reduce_mean(tf.square(y_-y))</code></p>
<p>自定义函数：</p>
<p>可在一定程度上优化实际问题中的预测误差。</p>
<p>交叉熵CE：</p>
<p>表明了两个概率分布之间的距离，交叉熵越大，表明两个概率分布越远<br>$$<br>H(y_,y) &#x3D; - \sum y_ \times ln\ y<br>$$<br>交叉熵越小，证明数据距离真实越准确。</p>
<p>softmax与交叉熵的结合：</p>
<p>在TensorFlow中提供了函数<code>tf.nn.softmax_cross_entropy_with_logits(y_，y)</code>输出先过softmax函数，再计算y与y_的交叉熵损失函数。</p>
<h2 id="2-5-缓解过拟合"><a href="#2-5-缓解过拟合" class="headerlink" title="2.5 缓解过拟合"></a>2.5 缓解过拟合</h2><p>欠拟合与过拟合</p>
<p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913162344502.png" srcset="/img/loading.gif" lazyload alt="欠拟合与过拟合"></p>
<table>
<thead>
<tr>
<th>欠拟合的解决方法：<br/>增加输入特征项<br/>增加网络参数<br/>减少正则化参数</th>
<th>过拟合的解决方法：<br/>数据清洗<br/>增大训练集<br/>采用正则化<br/>增大正则化参数</th>
</tr>
</thead>
</table>
<p>正则化缓解过拟合：</p>
<p>正则化在损失函数中引入模型复杂度指标，利用给W加权值，弱化了训练<br>数据的噪声（一般不正则化b）</p>
<p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913162811491.png" srcset="/img/loading.gif" lazyload alt="正则化"></p>
<p>正则化的选择<br>L1正则化大概率会使很多参数变为零，因此该方法可通过稀疏参数，即减少参数的数量，降低复杂度。<br>L2正则化会使参数很接近零但不为零，因此该方法可通过减小参数值的大小降低复杂度。</p>
<p>使用L2正则化缓解过拟合</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:  <span class="hljs-comment"># 记录梯度信息</span><br><br>           h1 = tf.matmul(x_train, w1) + b1  <span class="hljs-comment"># 记录神经网络乘加运算</span><br>           h1 = tf.nn.relu(h1)<br>           y = tf.matmul(h1, w2) + b2<br><br>           <span class="hljs-comment"># 采用均方误差损失函数mse = mean(sum(y-out)^2)</span><br>           loss_mse = tf.reduce_mean(tf.square(y_train - y))<br>           <span class="hljs-comment"># 添加l2正则化</span><br>           loss_regularization = []<br>           <span class="hljs-comment"># tf.nn.l2_loss(w)=sum(w ** 2) / 2</span><br>           loss_regularization.append(tf.nn.l2_loss(w1))<br>           loss_regularization.append(tf.nn.l2_loss(w2))<br>           <span class="hljs-comment"># 求和</span><br>           <span class="hljs-comment"># 例：x=tf.constant(([1,1,1],[1,1,1]))</span><br>           <span class="hljs-comment">#   tf.reduce_sum(x)</span><br>           <span class="hljs-comment"># &gt;&gt;&gt;6</span><br>           loss_regularization = tf.reduce_sum(loss_regularization)<br>           loss = loss_mse + <span class="hljs-number">0.03</span> * loss_regularization  <span class="hljs-comment"># REGULARIZER = 0.03</span><br><br>       <span class="hljs-comment"># 计算loss对各个参数的梯度</span><br>       variables = [w1, b1, w2, b2]<br>       grads = tape.gradient(loss, variables)<br></code></pre></td></tr></table></figure>

<p>下表可以看出，L2正则化函数可有效的缓解过饱和现象</p>
<table>
<thead>
<tr>
<th><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913170454005.png" srcset="/img/loading.gif" lazyload alt="未填加L2正则化"></th>
<th><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230913170523568.png" srcset="/img/loading.gif" lazyload alt="加入了L2正则化"></th>
</tr>
</thead>
</table>
<h2 id="2-6-优化器"><a href="#2-6-优化器" class="headerlink" title="2.6 优化器"></a>2.6 优化器</h2><p>是引导神经网络更新参数的工具。</p>
<p>神经网络参数优化器：<br>待优化参数𝒘，损失函数loss，学习率lr，每次迭代一个batch，t表示当前batch迭代的总次数：</p>
<ol>
<li>计算t时刻损失函数关于当前参数的梯度 $g_t&#x3D;\triangledown loss &#x3D; \frac{\partial loss}{\partial (w_t)} $</li>
<li>计算t时刻一阶动量 $m_t$ 和二阶动量$V_t$</li>
<li>计算t时刻下降梯度：$\eta_t &#x3D;\frac{lr·m_t}{\sqrt{V_t}}$</li>
<li>计算t+1时刻参数：$w_{t+1} &#x3D; w_t - \eta_t &#x3D; w_t - \frac{lr·m_t}{\sqrt{V_t}}$</li>
</ol>
<p>一阶动量：与梯度相关的函数<br>二阶动量：与梯度平方相关的函数</p>
<p>SGD(无momentum)，常用的梯度下降算法：</p>
<p>$m_t &#x3D; g_t $ $V_t &#x3D; 1$</p>
<p>$\eta_t &#x3D; \frac{lr·m_t}{\sqrt{V_t}} $</p>
<p>$w_{t+1} &#x3D; w_t -\eta_t &#x3D; w_t - \frac{lr·m_t}{\sqrt{V_t}} \ &#x3D;w_t - lr·g_t $</p>
<p>$w_{t+1} &#x3D; w_t -lr \ast \frac{\partial loss}{\partial w_t}  \ 参数更新公式$</p>
<p>SGDM(含momentum的SGD)，在SGD的基础上增加了一阶动量:</p>
<p>$m_{t-1}$表示上一时刻的一阶动量。</p>
<p>$m_t &#x3D; \beta · m_{t-1} + (1-\beta )·g_t $      $V_t&#x3D;1$</p>
<p>$\eta_t&#x3D;  \frac{lr·m_t}{\sqrt{V_t}} &#x3D; lr· m_t &#x3D;lr·(\beta · m_{t-1}+(1-\beta)·g_t)$</p>
<p>$w_{t+1}&#x3D;w_t -\eta_t &#x3D; w_t - lr · (\beta · m_{t-1}+(1-\beta)·g_t)$</p>
<p>Adagrad，在SGD基础上增加二阶动量:</p>
<p>$m_t&#x3D;g_t$     $V_t&#x3D;\sum_{\tau&#x3D;1}^t g_\tau^2$</p>
<p>$\eta_t&#x3D;\frac{lr·m_t}{\sqrt{V_t}} &#x3D;\frac{lr·g_t}{\sqrt{\sum_{\tau&#x3D;1}^t g_\tau^2}} $</p>
<p>$w_{t+1}&#x3D;w_t-\eta_t&#x3D;w_t-\frac{lr·g_t}{\sqrt{\sum_{\tau&#x3D;1}^t g_\tau^2}}$</p>
<p>RMSProp，SGD基础上增加二阶动量:</p>
<p>$m_t&#x3D;g_t$      $V_t &#x3D; \beta · V_{t-1} + (1-\beta)·g_t^2 $</p>
<p>$\eta_t&#x3D;\frac{lr·m_t}{\sqrt{V_t}} &#x3D;\frac{lr·g_t}{\sqrt{ \beta · V_{t-1} + (1-\beta)·g_t^2}} $</p>
<p>$w_{t+1}&#x3D;w_t-\eta_t&#x3D;w_t-\frac{lr·g_t}{\sqrt{ \beta · V_{t-1} + (1-\beta)·g_t^2}}$</p>
<p>Adam, 同时结合SGDM一阶动量和RMSProp二阶动量:</p>
<p>$m_t&#x3D;\beta_1 ·m_{t-1}+(1-\beta_1)·g_t$</p>
<p>修正一阶动量的偏差：$\widehat{m_t}&#x3D;\frac{m_t}{1-\beta_1^t}$</p>
<p>$V_t&#x3D;\beta_2 · V_{step-1}+(1-\beta_2)·g_t^2$</p>
<p>修正二阶动量的偏差：$ \widehat{V_t}&#x3D; \frac{V_t} {1-\beta_2^t} $</p>
<p>$\eta_t&#x3D;\frac{lr·\widehat{m_t} }{\sqrt{\widehat{V_t} } } &#x3D; lr \cdot \frac{lr{\frac{m_t}{1-\beta_1^t} } }{\sqrt{ {\frac{V_t} {1-\beta_2^t} } } } $</p>
<p>$w_{t+1}&#x3D;w_t-\eta_t&#x3D;w_t-\frac{lr\cdot{\frac{m_t}{1-\beta_1^t} } }{\sqrt{ {\frac{V_t}{1-\beta_2^t} } } } $</p>
<h2 id="3-1-搭建网络八股Sequential"><a href="#3-1-搭建网络八股Sequential" class="headerlink" title="3.1 搭建网络八股Sequential"></a>3.1 搭建网络八股Sequential</h2><p>用Tensorflow API：<code>tf.keras</code>搭建网络八股<br>六步法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span>    <span class="hljs-comment">#引入相关模块</span><br>train, test  <span class="hljs-comment">#告知喂入网络的训练集和测试集    特征x_train和标签y_train </span><br>model = tf.keras.models.Sequential <span class="hljs-comment">#搭建网络解构</span><br>model.<span class="hljs-built_in">compile</span> <span class="hljs-comment"># 配置训练方法（选择优化器、损失函数、评测指标）</span><br>model.fit <span class="hljs-comment"># 训练过程，告知train、test，告知batch、迭代次数</span><br>model.summary <span class="hljs-comment"># 打印网络解构、参数统计</span><br></code></pre></td></tr></table></figure>

<p><code>model = tf.keras.models.Sequential ([ 网络结构 ]) #描述各层网络</code><br>Sequential是容器，给出从输入层到输出层的各层网络解构</p>
<p>拉直层： tf.keras.layers.Flatten( )</p>
<p>全连接层： <code>tf.keras.layers.Dense(神经元个数, activation= &quot;激活函数“ ,kernel_regularizer=哪种正则化)</code><br>activation（字符串给出）可选: relu、 softmax、 sigmoid 、 tanh<br>kernel_regularizer可选:tf.keras.regularizers.l1()tf.keras.regularizers.l2()</p>
<p>卷积层： tf.keras.layers.Conv2D(filters &#x3D; 卷积核个数, kernel_size &#x3D; 卷积核尺寸,<br>strides &#x3D; 卷积步长， padding &#x3D; “ valid” or “same”)</p>
<p>LSTM层： tf.keras.layers.LSTM()</p>
<p><code>model.compile(optimizer = 优化器,loss = 损失函数 metrics = [“准确率”] )</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Optimizer可选:</span><br>‘sgd’ <span class="hljs-keyword">or</span> tf.keras.optimizers.SGD (lr=学习率,momentum=动量参数)<br>‘adagrad’ <span class="hljs-keyword">or</span> tf.keras.optimizers.Adagrad (lr=学习率)<br>‘adadelta’ <span class="hljs-keyword">or</span> tf.keras.optimizers.Adadelta (lr=学习率)<br>‘adam’ <span class="hljs-keyword">or</span> tf.keras.optimizers.Adam (lr=学习率, beta_1=<span class="hljs-number">0.9</span>, beta_2=<span class="hljs-number">0.999</span>)<br><span class="hljs-comment">#　loss可选:</span><br>‘mse’ <span class="hljs-keyword">or</span> tf.keras.losses.MeanSquaredError()<br>‘sparse_categorical_crossentropy’ <span class="hljs-keyword">or</span> tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="hljs-literal">False</span>)<br><span class="hljs-comment"># Metrics可选:</span><br>‘accuracy’ ：y_和y都是数值，如y_=[<span class="hljs-number">1</span>] y=[<span class="hljs-number">1</span>]<br>‘categorical_accuracy’ ：y_和y都是独热码(概率分布)，如y_=[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>] y=[<span class="hljs-number">0.256</span>,<span class="hljs-number">0.695</span>,<span class="hljs-number">0.048</span>]<br>‘sparse_categorical_accuracy’ ：y_是数值，y是独热码(概率分布),如y_=[<span class="hljs-number">1</span>] y=[<span class="hljs-number">0.256</span>,<span class="hljs-number">0.695</span>,<span class="hljs-number">0.048</span>]<br></code></pre></td></tr></table></figure>

<p><code>model.fit (训练集的输入特征, 训练集的标签, batch_size= , epochs= , validation_data=(测试集的输入特征，测试集的标签), validation_split=从训练集划分多少比例给测试集， validation_freq = 多少次epoch测试一次)</code></p>
<p>使用sequential可以搭建出上层输出就是下层输入的下层网络机构，但无法写出一些带有跳连的非顺序网络结构，这时候可以选择用类Class搭建神经网络解构。</p>
<h2 id="3-2-搭建网络八股Class"><a href="#3-2-搭建网络八股Class" class="headerlink" title="3.2 搭建网络八股Class"></a>3.2 搭建网络八股Class</h2><p>在六步法中，将第三部的Model改为<code>class MyModel(Model) model=MyModel</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyModel</span>(<span class="hljs-title class_ inherited__">Model</span>): <span class="hljs-comment"># 继承了Tensorflow的model类</span><br>	<span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>		<span class="hljs-built_in">super</span>(MyModel, self).__init__()<br>		定义网络结构块<br>	<span class="hljs-keyword">def</span> <span class="hljs-title function_">call</span>(<span class="hljs-params">self, x</span>):<br>		调用网络结构块，实现前向传播<br>		<span class="hljs-keyword">return</span> y<br>model = MyModel()<br><br><br><span class="hljs-comment">#__init__( )  定义所需网络结构块</span><br><span class="hljs-comment">#call( )  写出前向传播 实现钱前向传播</span><br><br></code></pre></td></tr></table></figure>

<h2 id="3-3-MNIST数据集"><a href="#3-3-MNIST数据集" class="headerlink" title="3.3 MNIST数据集"></a>3.3 MNIST数据集</h2><p>手写数字的数据集-上万张</p>
<p>导入MNIST数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"> mnist = tf.keras.datasets.mnist<br>(x_train, y_train) , (x_test, y_test) = mnist.load_data()<br></code></pre></td></tr></table></figure>

<p>为输入特征，输入神经网络时，将数据拉伸为一维数组：<br><code>tf.keras.layers.Flatten( )</code></p>
<h2 id="3-4-FASHION数据集"><a href="#3-4-FASHION数据集" class="headerlink" title="3.4 FASHION数据集"></a>3.4 FASHION数据集</h2><p>提供 6万张 28X28 像素点的衣裤等图片和标签，用于训练。<br>提供 1万张 28X28 像素点的衣裤等图片和标签，用于测试。</p>
<p>导入FASHION数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">fashion = tf.keras.datasets.fashion_mnist<br>(x_train, y_train),(x_test, y_test) = fashion.load_data()<br></code></pre></td></tr></table></figure>

<h2 id="4-1-搭建网络八股总览"><a href="#4-1-搭建网络八股总览" class="headerlink" title="4.1 搭建网络八股总览"></a>4.1 搭建网络八股总览</h2><p>① 自制数据集，解决本领域应用<br>② 数据增强，扩充数据集<br>③ 断点续训，存取模型<br>④ 参数提取，把参数存入文本<br>⑤ acc&#x2F;loss可视化，查看训练效果<br>⑥ 应用程序，给图识物</p>
<p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230916150852673.png" srcset="/img/loading.gif" lazyload alt="六步法八股总览"></p>
<h2 id="4-2-自制数据集"><a href="#4-2-自制数据集" class="headerlink" title="4.2 自制数据集"></a>4.2 自制数据集</h2><p>使用Py，目的是将文件夹内的图片读入，返回输入特征、标签。</p>
<p>标签文件txt</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> os<br><span class="hljs-comment">### import</span><br><br><br>train_path = <span class="hljs-string">&#x27;./mnist_image_label/mnist_train_jpg_60000/&#x27;</span><br>train_txt = <span class="hljs-string">&#x27;./mnist_image_label/mnist_train_jpg_60000.txt&#x27;</span><br>x_train_savepath = <span class="hljs-string">&#x27;./mnist_image_label/mnist_x_train.npy&#x27;</span><br>y_train_savepath = <span class="hljs-string">&#x27;./mnist_image_label/mnist_y_train.npy&#x27;</span><br><span class="hljs-comment">### 训练集</span><br><br><br>test_path = <span class="hljs-string">&#x27;./mnist_image_label/mnist_test_jpg_10000/&#x27;</span><br>test_txt = <span class="hljs-string">&#x27;./mnist_image_label/mnist_test_jpg_10000.txt&#x27;</span><br>x_test_savepath = <span class="hljs-string">&#x27;./mnist_image_label/mnist_x_test.npy&#x27;</span><br>y_test_savepath = <span class="hljs-string">&#x27;./mnist_image_label/mnist_y_test.npy&#x27;</span><br><br><span class="hljs-comment">### 测试集</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">generateds</span>(<span class="hljs-params">path, txt</span>):<br>    f = <span class="hljs-built_in">open</span>(txt, <span class="hljs-string">&#x27;r&#x27;</span>)  <span class="hljs-comment"># 以只读形式打开txt文件</span><br>    contents = f.readlines()  <span class="hljs-comment"># 读取文件中所有行</span><br>    f.close()  <span class="hljs-comment"># 关闭txt文件</span><br>    x, y_ = [], []  <span class="hljs-comment"># 建立空列表</span><br>    <span class="hljs-keyword">for</span> content <span class="hljs-keyword">in</span> contents:  <span class="hljs-comment"># 逐行取出</span><br>        value = content.split()  <span class="hljs-comment"># 以空格分开，图片路径为value[0] , 标签为value[1] , 存入列表</span><br>        img_path = path + value[<span class="hljs-number">0</span>]  <span class="hljs-comment"># 拼出图片路径和文件名</span><br>        img = Image.<span class="hljs-built_in">open</span>(img_path)  <span class="hljs-comment"># 读入图片</span><br>        img = np.array(img.convert(<span class="hljs-string">&#x27;L&#x27;</span>))  <span class="hljs-comment"># 图片变为8位宽灰度值的np.array格式</span><br>        img = img / <span class="hljs-number">255.</span>  <span class="hljs-comment"># 数据归一化 （实现预处理）</span><br>        x.append(img)  <span class="hljs-comment"># 归一化后的数据，贴到列表x</span><br>        y_.append(value[<span class="hljs-number">1</span>])  <span class="hljs-comment"># 标签贴到列表y_</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;loading : &#x27;</span> + content)  <span class="hljs-comment"># 打印状态提示</span><br><br>    x = np.array(x)  <span class="hljs-comment"># 变为np.array格式</span><br>    y_ = np.array(y_)  <span class="hljs-comment"># 变为np.array格式</span><br>    y_ = y_.astype(np.int64)  <span class="hljs-comment"># 变为64位整型</span><br>    <span class="hljs-keyword">return</span> x, y_  <span class="hljs-comment"># 返回输入特征x，返回标签y_</span><br><br><span class="hljs-comment">### generateds函数</span><br><br><br><span class="hljs-keyword">if</span> os.path.exists(x_train_savepath) <span class="hljs-keyword">and</span> os.path.exists(y_train_savepath) <span class="hljs-keyword">and</span> os.path.exists(<br>        x_test_savepath) <span class="hljs-keyword">and</span> os.path.exists(y_test_savepath):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;-------------Load Datasets-----------------&#x27;</span>)<br>    x_train_save = np.load(x_train_savepath)<br>    y_train = np.load(y_train_savepath)<br>    x_test_save = np.load(x_test_savepath)<br>    y_test = np.load(y_test_savepath)<br>    x_train = np.reshape(x_train_save, (<span class="hljs-built_in">len</span>(x_train_save), <span class="hljs-number">28</span>, <span class="hljs-number">28</span>))<br>    x_test = np.reshape(x_test_save, (<span class="hljs-built_in">len</span>(x_test_save), <span class="hljs-number">28</span>, <span class="hljs-number">28</span>))<br><span class="hljs-keyword">else</span>:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;-------------Generate Datasets-----------------&#x27;</span>)<br>    x_train, y_train = generateds(train_path, train_txt)<br>    x_test, y_test = generateds(test_path, test_txt)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;-------------Save Datasets-----------------&#x27;</span>)<br>    x_train_save = np.reshape(x_train, (<span class="hljs-built_in">len</span>(x_train), -<span class="hljs-number">1</span>))<br>    x_test_save = np.reshape(x_test, (<span class="hljs-built_in">len</span>(x_test), -<span class="hljs-number">1</span>))<br>    np.save(x_train_savepath, x_train_save)<br>    np.save(y_train_savepath, y_train)<br>    np.save(x_test_savepath, x_test_save)<br>    np.save(y_test_savepath, y_test)<br><br><span class="hljs-comment">### train test</span><br>    <br>    <br>    <br>model = tf.keras.models.Sequential([<br>    tf.keras.layers.Flatten(),<br>    tf.keras.layers.Dense(<span class="hljs-number">128</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>),<br>    tf.keras.layers.Dense(<span class="hljs-number">10</span>, activation=<span class="hljs-string">&#x27;softmax&#x27;</span>)<br>])<br><br><span class="hljs-comment">### models.sequential</span><br><br>model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;adam&#x27;</span>,<br>              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="hljs-literal">False</span>),<br>              metrics=[<span class="hljs-string">&#x27;sparse_categorical_accuracy&#x27;</span>])<br><br><span class="hljs-comment">### model.compile</span><br><br>model.fit(x_train, y_train, batch_size=<span class="hljs-number">32</span>, epochs=<span class="hljs-number">5</span>, validation_data=(x_test, y_test), validation_freq=<span class="hljs-number">1</span>)<br><br><span class="hljs-comment">### model.fit</span><br><br>model.summary()<br><br><span class="hljs-comment">### model.summary</span><br></code></pre></td></tr></table></figure>

<p>第一次运行生成了npy格式的数据集。</p>
<p>第二次会加载数据集，执行训练过程。</p>
<h2 id="4-3-数据增强"><a href="#4-3-数据增强" class="headerlink" title="4.3 数据增强"></a>4.3 数据增强</h2><p>增大数据量,扩充数据集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">image_gen_train =tf.keras.preprocessing.image.ImageDataGenerator(<br>	rescale = 所有数据将乘以该数值<br>	rotation_range = 随机旋转角度数范围<br>	width_shift_range = 随机宽度偏移量<br>	height_shift_range = 随机高度偏移量<br>	水平翻转：horizontal_flip = 是否随机水平翻转<br>	随机缩放：zoom_range = 随机缩放的范围 [<span class="hljs-number">1</span>-n，<span class="hljs-number">1</span>+n] )<br>image_gen_train.fit(x_train)<br></code></pre></td></tr></table></figure>

<h2 id="4-4-断点续训"><a href="#4-4-断点续训" class="headerlink" title="4.4 断点续训"></a>4.4 断点续训</h2><p>可以存取模型</p>
<p>读取模型：<br><code>load_weights(路径文件名）</code></p>
<p>可以先检查是否存在断点，如有则加载模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">cheakpoint_save_path= <span class="hljs-string">&quot;./cheakpoint/mnist.ckpt&quot;</span><br><span class="hljs-keyword">if</span> os.path.exists(checkpoint_save_path + <span class="hljs-string">&#x27;.index&#x27;</span>):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;--------load the model-------&#x27;</span>)<br>    model.load_weights(checkpoint_save_path)<br></code></pre></td></tr></table></figure>

<p><code>保存模型： tf.keras.callbacks.ModelCheckpoint(filepath=路径文件名,save_weights_only=True/False,save_best_only=True/False) history = model.fit（ callbacks=[cp_callback] ）</code></p>
<h2 id="4-5-参数提取"><a href="#4-5-参数提取" class="headerlink" title="4.5 参数提取"></a>4.5 参数提取</h2><p>把参数存入文本</p>
<p>提取可训练参数<br>model.trainable_variables 返回模型中可训练的参数<br>设置print输出格式<br>np.set_printoptions(threshold&#x3D;超过多少省略显示)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">np.set_printoptions(threshold=np.inf)<br><span class="hljs-comment"># np.inf表示无限大</span><br><br><br><span class="hljs-built_in">print</span>(model.trainable_variables)<br>file = <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;./weights.txt&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>)<br><span class="hljs-keyword">for</span> v <span class="hljs-keyword">in</span> model.trainable_variables:<br>file.write(<span class="hljs-built_in">str</span>(v.name) + <span class="hljs-string">&#x27;\n&#x27;</span>)<br>file.write(<span class="hljs-built_in">str</span>(v.shape) + <span class="hljs-string">&#x27;\n&#x27;</span>)<br>file.write(<span class="hljs-built_in">str</span>(v.numpy()) + <span class="hljs-string">&#x27;\n&#x27;</span>)<br>file.close()<br></code></pre></td></tr></table></figure>

<h2 id="4-6-acc-loss可视化"><a href="#4-6-acc-loss可视化" class="headerlink" title="4.6 acc&#x2F;loss可视化"></a>4.6 acc&#x2F;loss可视化</h2><p>acc曲线与loss曲线</p>
<p><code>history=model.fit(训练集数据, 训练集标签, batch_size=, epochs=,validation_split=用作测试数据的比例,validation_data=测试集,validation_freq=测试频率)</code></p>
<p>只是一段画图程序代码。</p>
<p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230916164058968.png" srcset="/img/loading.gif" lazyload alt="添加的画图代码"></p>
<h2 id="4-7-图片识别"><a href="#4-7-图片识别" class="headerlink" title="4.7 图片识别"></a>4.7 图片识别</h2><p>给图识物</p>
<p><code>predict（输入特征，batch_size=整数）</code>返回向前传播的计算结果</p>
<p>复现模型（前向传播）:</p>
<p><code>model = tf.keras.models.Sequential([ tf.keras.layers.Flatten(), tf.keras.layers.Dense(128, activation=&#39;relu&#39;), tf.keras.layers.Dense(10, activation=&#39;softmax’)])</code></p>
<p>加载参数:</p>
<p><code>model.load_weights(model_save_path</code></p>
<p>预测结果:</p>
<p><code>result = model.predict(x_predict)</code></p>
<h2 id="5-1-卷积的计算过程-Convolutional"><a href="#5-1-卷积的计算过程-Convolutional" class="headerlink" title="5.1 卷积的计算过程 Convolutional"></a>5.1 卷积的计算过程 Convolutional</h2><p>全连接 NN 特点：每个神经元与前后相邻层的每一个神经元都有连接关系。（可以实<br>现分类和预测）</p>
<p>全连接网络参数的个数为：$\sum(前层\times 后层 + 后层)$</p>
<p>卷积的概念：卷积可以认为是一种有效提取图像特征的方法。一般会用一个正方形的<br>卷积核，按指定步长，在输入特征图上滑动，遍历输入特征图中的每个像素点。每一个步长，<br>卷积核会与输入特征图出现重合区域，重合区域对应元素相乘、求和再加上偏置项得到输出<br>特征的一个像素点。</p>
<p>对于彩色图像（多通道）来说，卷积核通道数与输入特征一致，套接后在对应位置上进行乘加和操作，如果是彩色图片（RGB）利用三通道卷积核对三通道的彩色特征图做卷积计算。</p>
<p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230916171743300.png" srcset="/img/loading.gif" lazyload alt="输出特征尺寸计算"></p>
<h2 id="5-2-感受野"><a href="#5-2-感受野" class="headerlink" title="5.2 感受野"></a>5.2 感受野</h2><p>感受野（Receptive Field）：卷积神经网络各输出特征图中的每个像素点，在原始输入图片上映射区域的大小。</p>
<p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230916173848124.png" srcset="/img/loading.gif" lazyload alt="黄-绿为两次3*3 蓝色为一次5*5"></p>
<p>通常用两层3*3卷积核替换一层5*5卷积核</p>
<h2 id="5-3-全零填充-Padding"><a href="#5-3-全零填充-Padding" class="headerlink" title="5.3 全零填充 Padding"></a>5.3 全零填充 Padding</h2><p>将图的四周加上一圈零填充。</p>
<p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20230916174816575.png" srcset="/img/loading.gif" lazyload alt="padding"></p>
<p>TF描述全零填充<br>用参数padding &#x3D; ‘SAME’ 或 padding &#x3D; ‘VALID’表示</p>
<p>SAME：5X5X1  –&gt;  5X5X1      VALID：5X5X1–&gt;3X3X1</p>
<p>可以让输出特征图和输出特征图的尺寸不变。</p>
<h2 id="5-4-TF描述卷积计算层"><a href="#5-4-TF描述卷积计算层" class="headerlink" title="5.4 TF描述卷积计算层"></a>5.4 TF描述卷积计算层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">tf.keras.layers.Conv2D (<br>	filters = 卷积核个数,<br>	kernel_size = 卷积核尺寸, <span class="hljs-comment">#正方形写核长整数，或（核高h，核宽w）</span><br>	strides = 滑动步长, <span class="hljs-comment">#横纵向相同写步长整数，或(纵向步长h，横向步长w)，默认1</span><br>	padding = “same” <span class="hljs-keyword">or</span> “valid”, <span class="hljs-comment">#使用全零填充是“same”，不使用是“valid”（默认）</span><br>	activation = “ relu ” <span class="hljs-keyword">or</span> “ sigmoid ” <span class="hljs-keyword">or</span> “ tanh ” <span class="hljs-keyword">or</span> “ softmax”等 , <span class="hljs-comment">#如有BN此处不写</span><br>	input_shape = (高, 宽 , 通道数) <span class="hljs-comment">#输入特征图维度，可省略</span><br>)<br></code></pre></td></tr></table></figure>

<p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240228211041340.png" srcset="/img/loading.gif" lazyload alt="TF描述卷积层"></p>
<p>例如可以使用关键字传递参数的方法。</p>
<h2 id="5-5-批标准化-BN"><a href="#5-5-批标准化-BN" class="headerlink" title="5.5 批标准化 BN"></a>5.5 批标准化 BN</h2><p>标准化：使数据符合0均值，1为标准差的分布。</p>
<p>批标准化：对一小批数据（batch），做标准化处理 。</p>
<p>批标准化后，第 k个卷积核的输出特征图（feature map）中第 i 个像素点</p>
<p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240228211544435.png" srcset="/img/loading.gif" lazyload alt="BN层位于卷积层之后，激活层之前。"></p>
<p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240228211600833.png" srcset="/img/loading.gif" lazyload alt="批标准化"></p>
<h2 id="5-6-池化-Pooling"><a href="#5-6-池化-Pooling" class="headerlink" title="5.6 池化 Pooling"></a>5.6 池化 Pooling</h2><p>池化用于减少特征数据量。平均池化和最大池化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">tf.keras.layers.MaxPool2D(<br>	pool_size=池化核尺寸，<span class="hljs-comment">#正方形写核长整数，或（核高h，核宽w）</span><br>	strides=池化步长，<span class="hljs-comment">#步长整数， 或(纵向步长h，横向步长w)，默认为pool_size</span><br>	padding=‘valid’<span class="hljs-keyword">or</span>‘same’ <span class="hljs-comment">#使用全零填充是“same”，不使用是“valid”（默认）</span><br>)<br>tf.keras.layers.AveragePooling2D(<br>	pool_size=池化核尺寸，<span class="hljs-comment">#正方形写核长整数，或（核高h，核宽w）</span><br>	strides=池化步长，<span class="hljs-comment">#步长整数， 或(纵向步长h，横向步长w)，默认为pool_size</span><br>	padding=‘valid’<span class="hljs-keyword">or</span>‘same’ <span class="hljs-comment">#使用全零填充是“same”，不使用是“valid”（默认）</span><br>)<br></code></pre></td></tr></table></figure>

<p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240228211847780.png" srcset="/img/loading.gif" lazyload alt="池化"></p>
<h2 id="5-7-舍弃-Dropout"><a href="#5-7-舍弃-Dropout" class="headerlink" title="5.7 舍弃 Dropout"></a>5.7 舍弃 Dropout</h2><p>在神经网络训练时，将一部分神经元按照一定概率从神经网络中暂时舍弃。神经网络使用时，被舍弃的神经元恢复链接。</p>
<p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240228212108454.png" srcset="/img/loading.gif" lazyload alt="舍弃"></p>
<h2 id="5-8-卷积神经网络"><a href="#5-8-卷积神经网络" class="headerlink" title="5.8 卷积神经网络"></a>5.8 卷积神经网络</h2><p>卷积是什么？ 卷积就是特征提取器，就是CBAPD</p>
<p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240228214733104.png" srcset="/img/loading.gif" lazyload alt="CBAPD"></p>
<p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240228214753084.png" srcset="/img/loading.gif" lazyload alt="CNN"></p>
<h2 id="5-9-Cifar10数据集-卷积神经网络搭建示例"><a href="#5-9-Cifar10数据集-卷积神经网络搭建示例" class="headerlink" title="5.9 Cifar10数据集 卷积神经网络搭建示例"></a>5.9 Cifar10数据集 卷积神经网络搭建示例</h2><p>提供 5万张 32*32 像素点的十分类彩色图片和标签，用于训练。<br>提供 1万张 32*32 像素点的十分类彩色图片和标签，用于测试。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#导入cifar10数据集：</span><br>cifar10 = tf.keras.datasets.cifar10<br>(x_train, y_train),(x_test, y_test) = cifar10.load_data()<br><br>plt.imshow(x_train[<span class="hljs-number">0</span>])<br><span class="hljs-comment">#绘制图片</span><br>plt.show()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;x_train[0]:\n&quot;</span> , x_train[<span class="hljs-number">0</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;y_train[0]:&quot;</span>, y_train[<span class="hljs-number">0</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;x_test.shape:&quot;</span>, x_test.shape)<br></code></pre></td></tr></table></figure>

<p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240302132316265.png" srcset="/img/loading.gif" lazyload alt="卷积神经网络搭建示例"></p>
<p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240302132135165.png" srcset="/img/loading.gif" lazyload alt="搭建示例"></p>
<h2 id="5-10-LeNet-AlexNet-VGGNet-InceptionNet-ResNet"><a href="#5-10-LeNet-AlexNet-VGGNet-InceptionNet-ResNet" class="headerlink" title="5.10 LeNet AlexNet VGGNet InceptionNet ResNet"></a>5.10 LeNet AlexNet VGGNet InceptionNet ResNet</h2><p>LeNet由Yann LeCun于1998年提出，卷积网络开篇之作。</p>
<p>AlexNet网络诞生于2012年，当年ImageNet竞赛的冠军，Top5错误率为16.4%。</p>
<p>VGGNet诞生于2014年，当年ImageNet竞赛的亚军，Top5错误率减小到7.3%。</p>
<p>InceptionNet诞生于2014年，当年ImageNet竞赛冠军，Top5错误率为6.67%</p>
<p>ResNet诞生于2015年，当年ImageNet竞赛冠军，Top5错误率为3.57%</p>
<p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240302153943645.png" srcset="/img/loading.gif" lazyload alt="经典卷积网络"></p>
<h2 id="6-1-循环核"><a href="#6-1-循环核" class="headerlink" title="6.1 循环核"></a>6.1 循环核</h2><p>循环核：参数时间共享，循环层提取时间信息。</p>
<p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240302180849506.png" srcset="/img/loading.gif" lazyload alt="循环核"></p>
<h2 id="6-2-循环核按时间步展开"><a href="#6-2-循环核按时间步展开" class="headerlink" title="6.2 循环核按时间步展开"></a>6.2 循环核按时间步展开</h2><p>循环神经网络：借助循环核提取时间特征后，送入全连接网络。</p>
<p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240302181058519.png" srcset="/img/loading.gif" lazyload alt="循环神经网络"></p>
<p>循环计算层：向输出方向生长。</p>
<p><img src="/../images/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-5-TensorFlow/image-20240302181218194.png" srcset="/img/loading.gif" lazyload alt="循环计算层"></p>
<h2 id="6-3-TF描述循环计算层"><a href="#6-3-TF描述循环计算层" class="headerlink" title="6.3 TF描述循环计算层"></a>6.3 TF描述循环计算层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">tf.keras.layers.SimpleRNN(记忆体个数，activation=‘激活函数’ ，<br>return_sequences=是否每个时刻输出ht到下一层)<br>activation=‘激活函数’ （不写，默认使用tanh）<br>return_sequences=<span class="hljs-literal">True</span> 各时间步输出ht<br>return_sequences=<span class="hljs-literal">False</span> 仅最后时间步输出ht（默认）<br>例：SimpleRNN(<span class="hljs-number">3</span>, return_sequences=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure>

<p>入RNN时， x_train维度：<br>[送入样本数， 循环核时间展开步数， 每个时间步输入特征个数]</p>
<h2 id="6-4-循环计算过程-字母输入预测"><a href="#6-4-循环计算过程-字母输入预测" class="headerlink" title="6.4 循环计算过程-字母输入预测"></a>6.4 循环计算过程-字母输入预测</h2><p>字母预测：输入a预测出b，输入b预测出c，<br>输入c预测出d，输入d预测出e，输入e预测出a</p>
<p>用RNN实现输入一个字母，预测下一个字母<br>（One hot 编码）独热码</p>
<p>用RNN实现输入连续四个字母，预测下一个字母<br>（One hot 编码）</p>
<p>用RNN实现输入一个字母，预测下一个字母<br>（Embedding 编码）</p>
<p>用RNN实现输入连续四个字母，预测下一个字母<br>（Embedding 编码）</p>
<h2 id="6-5-股票预测"><a href="#6-5-股票预测" class="headerlink" title="6.5 股票预测"></a>6.5 股票预测</h2><p>用RNN实现股票预测</p>
<p>用LSTM实现股票预测</p>
<p>LSTM 由Hochreiter &amp; Schmidhuber 于1997年提出，通过门控单元改善了RNN长期依赖问题。</p>
<p>用GRU实现股票预测</p>
<p>GRU由Cho等人于2014年提出，优化LSTM结构。</p>
<p>更新于：2024 </p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="category-chain-item">机器学习</a>
  
  
    <span>></span>
    
  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6/" class="category-chain-item">工具与软件</a>
  
  

  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/TensorFlow/" class="print-no-link">#TensorFlow</a>
      
    </div>
  
</div>


              

              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2023/09/12/markdown%E4%B8%AD%E7%9A%84%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/" title="markdown中的数学公式">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">markdown中的数学公式</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/09/08/2-%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%BD%AF%E4%BB%B6-4-PyTorch/" title="2-工具与软件-4-PyTorch">
                        <span class="hidden-mobile">2-工具与软件-4-PyTorch</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
    <div id="giscus" class="giscus"></div>
    <script type="text/javascript">
      Fluid.utils.loadComments('#giscus', function() {
        var options = {"repo":"CNwuyueyu/CNwuyueyu.github.io","repo-id":"R_kgDOKPjK3A","category":"Announcements","category-id":"DIC_kwDOKPjK3M4CZLEu","theme-light":"light","theme-dark":"dark","mapping":"pathname","reactions-enabled":1,"emit-metadata":0,"input-position":"bottom","lang":"zh-CN"};
        var attributes = {};
        for (let option in options) {
          if (!option.startsWith('theme-')) {
            var key = option.startsWith('data-') ? option : 'data-' + option;
            attributes[key] = options[option];
          }
        }
        var light = 'light';
        var dark = 'dark';
        window.GiscusThemeLight = light;
        window.GiscusThemeDark = dark;
        attributes['data-theme'] = document.documentElement.getAttribute('data-user-color-scheme') === 'dark' ? dark : light;
        for (let attribute in attributes) {
          var value = attributes[attribute];
          if (value === undefined || value === null || value === '') {
            delete attributes[attribute];
          }
        }
        var s = document.createElement('script');
        s.setAttribute('src', 'https://giscus.app/client.js');
        s.setAttribute('crossorigin', 'anonymous');
        for (let attribute in attributes) {
          s.setAttribute(attribute, attributes[attribute]);
        }
        var ss = document.getElementsByTagName('script');
        var e = ss.length > 0 ? ss[ss.length - 1] : document.head || document.documentElement;
        e.parentNode.insertBefore(s, e.nextSibling);
      });
    </script>
    <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>由Hexo框架支持</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
